{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4150ad40",
   "metadata": {},
   "source": [
    "# ETL_Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e018b5",
   "metadata": {},
   "source": [
    "__!!!!!IMP!!!!: On the day when the timezone changes, we must implement a new logic. We utilize the current UTC time, but it's crucial not to mix data from before and after the time change. Otherwise, all data will advance by one hour. If you use the old UTC time, the data will be delayed by one hour.__\n",
    "\n",
    "   _Portugal > Winter: \"UTC +0\" || Summers: \"UTC +1\" (+01:00 Paris now)_\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7cecb8",
   "metadata": {},
   "source": [
    "__Run the bellow 2 groups ( Libraries +USERNAME and DOWNLOAD FOLDER)__\n",
    "\n",
    " 1 group will import the libraries, if some of it will not work, please open a new code cell and write: Example ( pip install shutill) and run it, this will install the library\n",
    " \n",
    " 2 group will read you username and the folder where you have the downloads, you can have another then download, just ajust the variable \" folder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5930b6d",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d46b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import re\n",
    "import win32com.client\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import win32com.client as win32\n",
    "import subprocess\n",
    "import warnings\n",
    "import win32com.client as w3c\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444afe17",
   "metadata": {},
   "source": [
    "# USERNAME and DOWNLOAD FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12ec8d",
   "metadata": {},
   "source": [
    "__!!!CHANGE THE  USER NAME, AND DOWNLOAD PATH IF NEEDED:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c9e21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your username:bechara.9\n",
      "Folder where is your downloads:C:\\\\Users\\\\bechara.9\\\\Downloads\n"
     ]
    }
   ],
   "source": [
    "#THE CODE SHOULD SHOW YOUR NAME:\n",
    "username = os.getlogin()\n",
    "print('Your username:'+ username)\n",
    "#IF YOU DONT USE YOUR DOWNLOAD FOLDER CHANGE THE BELOW STRING TO YOUR FOLDER:\n",
    "folder=\"Downloads\"\n",
    "downloads_folder_path = fr\"C:\\\\Users\\\\{username}\\\\{folder}\"\n",
    "print('Folder where is your downloads:' + downloads_folder_path)\n",
    "#Clean python cache\n",
    "gen_py_path = fr\"C:\\Users\\\\{username}\\\\AppData\\Local\\Temp\\gen_py\"\n",
    "shutil.rmtree(gen_py_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc8b32",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e828331",
   "metadata": {},
   "source": [
    "# ( !!!MONDAY // WEDNESDAY!!!) Random Sampling ( TTR1, TTR2, LIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0187049f",
   "metadata": {},
   "source": [
    "__Link:__ \n",
    "\n",
    "https://datapower-va.bytelemon.com/bi/visit/7361807889996120069\n",
    "\n",
    "\n",
    " __Moderation Date:__ Extract last 4 weeks of data ( Monday to Sunday)  // __Press \"Querry\"__\n",
    "\n",
    " __Tab:__ Ops Record\n",
    "\n",
    " __Table:__ OPS Record SQA\n",
    "\n",
    "__Number of rows:__ 1000.000\n",
    "\n",
    "__Format:__ UTF-8 encoded CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6accbe68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied to: Ops_Record - 2024 W34.csv\n",
      "File copied to: Ops_Record - 2024 W33.csv\n",
      "File copied to: Ops_Record - 2024 W32.csv\n",
      "File copied to: Ops_Record - 2024 W31.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Destination Emea:\n",
    "destination = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\New_Random Sampling_Detailed\"\n",
    "\n",
    "# List files in the downloads folder\n",
    "files = os.listdir(downloads_folder_path)\n",
    "week_file_count = {}\n",
    "\n",
    "for file in files:\n",
    "    if file.startswith(\"_OPS Record SQA -\"):  \n",
    "        source_path = os.path.join(downloads_folder_path, file)\n",
    "        df = pd.read_csv(source_path, delimiter=\",\")  \n",
    "        \n",
    "        # Extract unique date\n",
    "        unique_date = pd.to_datetime(df[\"Date\"]).dt.date.unique()\n",
    "        \n",
    "        # Extract the date from the first row\n",
    "        date_str = df[\"Date\"].iloc[0]\n",
    "        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "        week = date.strftime(\"2024 W%U\")\n",
    "        \n",
    "        next_week_date = date + timedelta(days=8)\n",
    "        week = next_week_date.strftime(\"2024 W%U\")\n",
    "        \n",
    "        if week in week_file_count:\n",
    "            week_file_count[week].append((file, unique_date))\n",
    "        else:\n",
    "            week_file_count[week] = [(file, unique_date)]\n",
    "\n",
    "\n",
    "\n",
    "for week, files_list in week_file_count.items():\n",
    "    for index, (file, unique_date) in enumerate(files_list, 1):\n",
    "        source_path = os.path.join(downloads_folder_path, file)\n",
    "        if len(files_list) == 1:\n",
    "            new_name = f\"Ops_Record - {week}.csv\"\n",
    "        else:\n",
    "            new_name = f\"Ops_Record - {week}_part{index}.csv\"\n",
    "\n",
    "        destination_path = os.path.join(destination, new_name)\n",
    "        shutil.copy(source_path, destination_path)\n",
    "        print(f\"File copied to: {new_name}\")\n",
    "\n",
    "#This line will remove the files from your download files:\n",
    "[os.remove(os.path.join(downloads_folder_path, filename)) for filename in os.listdir(downloads_folder_path)\n",
    " if filename.startswith(\"_OPS Record SQA -\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1a65f",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a14fe",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd61d51",
   "metadata": {},
   "source": [
    "https://byteworks-va.bytelemon.com/v2/workhour/correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cd71c6",
   "metadata": {},
   "source": [
    "# # Logic of the GA and UNO:\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4856d38a",
   "metadata": {},
   "source": [
    "__In each process, we have Glidepaths files from TP QAs and raw data from the client. Then we utilize a spider script to bring specifc columns from the Glidepath, base on the task_id + Queue ID into the client's dataset.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb07e72",
   "metadata": {},
   "source": [
    "#    ( !!!MONDAY // WEDNESDAY!!!) GA GLIEPATHS Bot is doing this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e5938",
   "metadata": {},
   "source": [
    "----------TTR1 Glidepaths---------\n",
    "\n",
    "\n",
    "__PT TTR1 Glidepath & Action Tracker:__\n",
    "https://teleperformance.larksuite.com/sheets/shtusjLubYGiKkRYwhqySE39Hgg?sheet=XhmUPq\n",
    "\n",
    "__FI TTR1 Glidepath & Action Tracker:__\n",
    "https://teleperformance.larksuite.com/sheets/shtusIbCsBcNIGhr9EsqGg4FHte?sheet=PRHk1L\n",
    "\n",
    "__UA TTR1 Glidepath & Action Tracker:__\n",
    "https://teleperformance.larksuite.com/sheets/shtusJDpWNWcJc7esxbFcKHqowe?sheet=ZzgRZL\n",
    "\n",
    "__HB TTR1 Glidepath & Action Tracker:__\n",
    "https://teleperformance.larksuite.com/sheets/shtusiIjVTZVpOHItQzXmSvYr0g?sheet=ILdzs7\n",
    "\n",
    "\n",
    "----------TTR2 Glidepaths---------\n",
    "\n",
    "__2024-PT TTR2 Glidepath & Action Tracker:__\n",
    "https://teleperformance.larksuite.com/sheets/MciUsGYmHh3iOFtJHSxum0uFswb?sheet=ECWE7s\n",
    "\n",
    "__UA TTR2 Glidepath & Action tracker:__\n",
    "https://teleperformance.larksuite.com/sheets/Ufkqsy2WuhHIrHtdzfmuGlEWsEf?sheet=thvKHO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ea90f",
   "metadata": {},
   "source": [
    "__Step 1:__ Run the code below. It will simultaneously open all the glidepaths, you just need to wait for the pages to load. In case it doesn't work, you use the links above.\n",
    "\n",
    "__GA:IMP REMOVE FILTERS before download__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4489ab",
   "metadata": {},
   "source": [
    "__Step 2:__ The below code will read the files that you downloaded and move it to destination folder.\n",
    "\n",
    "__IMP:__ Verify the output, it will display the names of the files that have been moved. The code will execute the move operation if at least one file is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0621397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved file: C:\\Users\\\\bechara.9\\\\Downloads\\PT TTR1 Glidepath & Action Tracker - GA Review (100).csv \n",
      "Moved file: C:\\Users\\\\bechara.9\\\\Downloads\\FI TTR1 Glidepath & Action Tracker - GA Review (100).csv \n",
      "Moved file: C:\\Users\\\\bechara.9\\\\Downloads\\UA TTR1 Glidepath & Action Tracker - GA Review (100).csv \n",
      "Moved file: C:\\Users\\\\bechara.9\\\\Downloads\\HB TTR1 Glidepath & Action Tracker - GA Review (100).csv \n",
      "Moved file: C:\\Users\\\\bechara.9\\\\Downloads\\TTR2 - GA - PT Masterfile 2024 - GA Review (100).csv \n",
      "Moved file: C:\\Users\\\\bechara.9\\\\Downloads\\UA TTR2 Glidepath & Action Tracker - GA Review (100).csv \n",
      "Files moved with success.\n"
     ]
    }
   ],
   "source": [
    "#Ps: If the file is renamed with a different name, you can simply update the code to read the new filename from your folder and add a new destination line with the name that the file should have.\n",
    "#This ensures that we won't need to modify the GA script.\n",
    "#THIS CODE WILL READ THE DOWNLOADED FILES:\n",
    "GP_PT_TTR1=fr\"C:\\Users\\\\{username}\\\\{folder}\\PT TTR1 Glidepath & Action Tracker - GA Review (100).csv\"\n",
    "GP_FI_TTR1=fr\"C:\\Users\\\\{username}\\\\{folder}\\FI TTR1 Glidepath & Action Tracker - GA Review (100).csv\"\n",
    "GP_UA_TTR1=fr\"C:\\Users\\\\{username}\\\\{folder}\\UA TTR1 Glidepath & Action Tracker - GA Review (100).csv\"\n",
    "GP_HB_TTR1=fr\"C:\\Users\\\\{username}\\\\{folder}\\HB TTR1 Glidepath & Action Tracker - GA Review (100).csv\"\n",
    "GP_PT_TTR2=fr\"C:\\Users\\\\{username}\\\\{folder}\\TTR2 - GA - PT Masterfile 2024 - GA Review (100).csv\"\n",
    "GP_UA_TTR2=fr\"C:\\Users\\\\{username}\\\\{folder}\\UA TTR2 Glidepath & Action Tracker - GA Review (100).csv\"\n",
    "\n",
    "#DESTINATION FOLDER:\n",
    "GP_TTR1_EMEA=r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\TTR1 GA Glidepath\"\n",
    "GP_TTR2_EMEA=r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\TTR2 GA Glidepath\"\n",
    "\n",
    "\n",
    "#THIS WILL MOVE ALL THE ABOVE FILES, IF THE FILE IS NOT DOWNLOAED THE CODE WILL NOT RUN:\n",
    "file_paths = [\n",
    "    \n",
    "     #(File dowloaded,destination)\n",
    "    (GP_PT_TTR1, GP_TTR1_EMEA),\n",
    "    (GP_FI_TTR1, GP_TTR1_EMEA),\n",
    "    (GP_UA_TTR1, GP_TTR1_EMEA),\n",
    "    (GP_HB_TTR1, GP_TTR1_EMEA),\n",
    "    (GP_PT_TTR2, GP_TTR2_EMEA),\n",
    "    (GP_UA_TTR2, GP_TTR2_EMEA)\n",
    "]\n",
    "\n",
    "try:\n",
    "    for source, destination in file_paths:\n",
    "        try:\n",
    "            shutil.copy(source, destination)\n",
    "            print(f\"Moved file: {source} \")\n",
    "        except FileNotFoundError:\n",
    "            continue  # If file is missing, continue to the next one\n",
    "    print(\"Files moved with success.\")\n",
    "except Exception as e:\n",
    "    print(\"Fails\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e00e7",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fadbd3",
   "metadata": {},
   "source": [
    "#  (!!!MONDAY // WEDNESDAY!!!) GA DataPower // Client Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925797c4",
   "metadata": {},
   "source": [
    "### <del>__OLD DataPower GA Link__:https://datapower-va.bytelemon.com/bi/visit/7171442951608877061?immersive=1\n",
    "\n",
    "__DataPower GA Link__:https://datapower-va.bytelemon.com/bi/visit/7325384900452941829?immersive=1\n",
    "\n",
    "__TAB:__ GA Performance\n",
    "\n",
    "__SHEET:__ GA 1.0\n",
    "\n",
    "__FILTERS TOP:__  _Site:_ \" TP_LIS \" /// PRESS: \"QUERRY\" to applay the filters. ´\n",
    "\n",
    "__Table:__ Last Table >> \"Case Picker - Simulation\"\n",
    "\n",
    "__Filter on Table:__  Batch\n",
    "\n",
    "__Extract:__ Last 2 weeks data available. \n",
    "\n",
    "__Format:__ CSV.  // 1000000 of rows\n",
    "    \n",
    "__IMP:__ Sometimes we have 2 files per week ( example- Week 06 2024 we had (2024-02-05 and 2024-02-06)\n",
    "    Each  need just contain 1 date values, but the GA script will verify that for us \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80ceddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week File Count Dictionary:\n",
      "{'2024 W34': [('_Case Picker - Simulation - 2024-08-21 08-36-31.csv', '2024-08-19')], '2024 W33': [('_Case Picker - Simulation - 2024-08-21 08-36-52.csv', '2024-08-12')], '2024 W32': [('_Case Picker - Simulation - 2024-08-21 08-40-02.csv', '2024-08-05')]}\n",
      "--------------------------------------------\n",
      "File copied to: _Case Picker - Simulation  - 2024 W34.csv   // Unique date: 2024-08-19\n",
      "File copied to: _Case Picker - Simulation  - 2024 W33.csv   // Unique date: 2024-08-12\n",
      "File copied to: _Case Picker - Simulation  - 2024 W32.csv   // Unique date: 2024-08-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Destination directory (final folder)\n",
    "destination = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\GA Sampling_new\\Datapower\"\n",
    "\n",
    "# List files in the downloads folder\n",
    "files = os.listdir(downloads_folder_path)\n",
    "week_file_count = {}\n",
    "\n",
    "# Função para transformar a string para o formato \"2024-06-17\"\n",
    "def transform_date(batch):\n",
    "    match = re.search(r'(\\d{4})-W\\d{2}\\((\\d{2})/(\\d{2})~\\d{2}/\\d{2}\\)', batch)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        day = match.group(3)\n",
    "        return f\"{year}-{month}-{day}\"\n",
    "    else:\n",
    "        return batch\n",
    "    \n",
    "for file in files:\n",
    "    if file.startswith(\"_Case Picker\"):\n",
    "        source_path = os.path.join(downloads_folder_path, file)\n",
    "        df = pd.read_csv(source_path, delimiter=\",\")\n",
    "         # Verificar se a coluna existe antes de renomear\n",
    "        df.rename(columns={'Batch (Date)': 'Batch'}, inplace=True)\n",
    "        # Apply the transformation to the Batch column\n",
    "        df[\"Batch\"] = df[\"Batch\"].apply(transform_date)\n",
    "        df[\"Batch\"] = df[\"Batch\"].astype(str)\n",
    "        # Extract the date from the first row\n",
    "        date_str = df[\"Batch\"].iloc[0]\n",
    "        date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "         # Calcular a semana ISO\n",
    "        year, week, _ = date.isocalendar()\n",
    "        week_str = f\"{year} W{week:02d}\"\n",
    "        \n",
    "        df[\"Batch\"] = date.strftime(\"%Y-%m-%d\")\n",
    "        unique_date = date_str \n",
    "\n",
    "        if week in week_file_count:\n",
    "            week_file_count[week_str].append((file, unique_date))\n",
    "        else:\n",
    "            week_file_count[week_str] = [(file, unique_date)]\n",
    "        # Salvar o DataFrame com o mesmo nome do arquivo original\n",
    "        output_path = os.path.join(downloads_folder_path, file)\n",
    "        df.to_csv(output_path, index=False)\n",
    "       \n",
    "\n",
    "\n",
    "print(\"Week File Count Dictionary:\")\n",
    "print(week_file_count)\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "for week, files_list in week_file_count.items():\n",
    "    for index, (file, unique_date) in enumerate(files_list, 1):\n",
    "        source_path = os.path.join(downloads_folder_path, file)\n",
    "        if len(files_list) == 1:\n",
    "            new_name = f\"_Case Picker - Simulation  - {week}.csv\"\n",
    "        else:\n",
    "            new_name = f\"_Case Picker - Simulation  - {week}_part{index}.csv\"\n",
    "\n",
    "        destination_path = os.path.join(destination, new_name)\n",
    "\n",
    "        shutil.copy(source_path, destination_path)\n",
    "        print(f\"File copied to: {new_name}   // Unique date: {unique_date}\")\n",
    "\n",
    "#This line will remove the files from your download files:\n",
    "[os.remove(os.path.join(downloads_folder_path, filename)) for filename in os.listdir(downloads_folder_path)\n",
    " if filename.startswith(\"_Case Picker\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8da7164",
   "metadata": {},
   "source": [
    "# (SCRIPT):GA SCRIPT\n",
    "__The next script will run th GA script based on the inputs we give:__\n",
    "\n",
    "__!!!IMP: CHANGE THE INPUT FOR THE CORRECT WEEK NUMBER!!!!__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "114b3ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: CompletedProcess(args=['python', '\\\\\\\\emea.tpg.ads\\\\portugal\\\\Departments\\\\ITDEV\\\\PowerBI\\\\accounting\\\\business analysts\\\\01 - ba\\\\02 - projects\\\\tiktok\\\\10. Python scripts\\\\NEW ETL\\\\Aux Scripts\\\\GA Sampling\\\\GA Sampling_new.py'], returncode=0)\n",
      "Result 2: CompletedProcess(args=['python', '\\\\\\\\emea.tpg.ads\\\\portugal\\\\Departments\\\\ITDEV\\\\PowerBI\\\\accounting\\\\business analysts\\\\01 - ba\\\\02 - projects\\\\tiktok\\\\10. Python scripts\\\\NEW ETL\\\\Aux Scripts\\\\GA Sampling\\\\GA Sampling_new.py'], returncode=0)\n",
      "Result 3: CompletedProcess(args=['python', '\\\\\\\\emea.tpg.ads\\\\portugal\\\\Departments\\\\ITDEV\\\\PowerBI\\\\accounting\\\\business analysts\\\\01 - ba\\\\02 - projects\\\\tiktok\\\\10. Python scripts\\\\NEW ETL\\\\Aux Scripts\\\\GA Sampling\\\\GA Sampling_new.py'], returncode=0)\n"
     ]
    }
   ],
   "source": [
    "Script_path = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\10. Python scripts\\NEW ETL\\Aux Scripts\\GA Sampling\\GA Sampling_new.py\"\n",
    "\n",
    "#run the second input\n",
    "\n",
    "\n",
    "Input1 = '2024 W34'\n",
    "Result1 = subprocess.run(['python', Script_path], input=Input1, text=True)\n",
    "\n",
    "\n",
    "Input2 = '2024 W33'\n",
    "Result2 = subprocess.run(['python', Script_path], input=Input2, text=True)\n",
    "\n",
    "Input3 = '2024 W32'\n",
    "Result3 = subprocess.run(['python', Script_path], input=Input3, text=True)\n",
    "\n",
    "\n",
    "print(\"Result 1:\", Result1)\n",
    "print(\"Result 2:\", Result2)\n",
    "print(\"Result 3:\", Result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c266f",
   "metadata": {},
   "source": [
    "__RCA VALIDATION:__\n",
    "\n",
    "The below code will verify the RCAs mapped on out DIM_RCA an them compare with the RCA mapped on the final GA file, \n",
    "If the data has wrong RCA the code will trigger an email to the QAs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a19df88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: QA_Case Picker - Simulation  - 2024 W32.csv\n",
      "Reading file: QA_Case Picker - Simulation  - 2024 W33.csv\n",
      "Reading file: QA_Case Picker - Simulation  - 2024 W34.csv\n",
      "Reading file: QA_Case Picker - Simulation  - 2024 W31.csv\n",
      "----------------------------------------------\n",
      "All the RCAs are correct\n"
     ]
    }
   ],
   "source": [
    "#lista_emails=['pedro.esteves.rivera@teleperformance.com']\n",
    "lista_emails=['divelly.rivera@teleperformance.com','dario.rocco@teleperformance.com','soraia.amarelinho@teleperformance.com',\n",
    "      'alexia.dejesus@teleperformance.com','domenico.frisone@pt.teleperformance.com','vanessa.rubio@teleperformance.com','natalia.shkodenko@teleperformance.com',]\n",
    "#----------STD_DIM-------------\n",
    "#Location of STD_DIM:\n",
    "DIM_RCA_path= r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\STD_DIMS_TTOK new.xlsx'\n",
    "#Reading the STD_DIM and the sheet \"DIM_RCA\", and pushing just the column \"RCA_ID\":\n",
    "DIM_RCA=pd.read_excel(DIM_RCA_path,sheet_name='DIM_RCA')[['RCA_ID']]\n",
    "#Filtring the column for the data that start with \" Golden Assessment\":\n",
    "DIM_RCA=DIM_RCA[DIM_RCA.RCA_ID.str.startswith('Golden Assessment/')]\n",
    "#Trim the all values:\n",
    "DIM_RCA['RCA_ID']= DIM_RCA['RCA_ID'].str.strip()\n",
    "#Lower case all the values\n",
    "DIM_RCA['RCA_ID']= DIM_RCA['RCA_ID'].str.lower()\n",
    "\n",
    "#------End of manipulation of the dataframe STD_DIM----------\n",
    "\n",
    "#Creation of a function that will read just the last 4 the final data of GA on folder GA_samplin_new, folder Final_version:\n",
    "def get_df1(folder_path):\n",
    "    df1 = pd.DataFrame()\n",
    "    os.chdir(folder_path)\n",
    "    filesGA = sorted((f for f in os.listdir() if f.endswith('.csv')), key=os.path.getmtime, reverse=True)[:4]\n",
    "    for file in filesGA:\n",
    "        print(\"Reading file:\", file)  # Print the name of the file being read\n",
    "        aux = pd.read_csv(file, error_bad_lines=False)\n",
    "        df1 = df1.append(aux)\n",
    "    return df1\n",
    "#Read location of the Final version of GA:\n",
    "folder_path =  r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\GA Sampling_new\\Final_version\"\n",
    "#Creating a dataframe of GA data:\n",
    "df1 = get_df1(folder_path)\n",
    "#Column creation \"RCA_ID\" with a concat of a string with 3 other columns with \"/\" as separation:\n",
    "df1['RCA_ID'] = 'Golden Assessment' + '/' + df1['Agree/Not agree'].astype(str) + '/' + df1['1st RCA'] + '/' + df1['2nd RCA']\n",
    "#Change the \"Batch\" column to datetime and with a specific date format:\n",
    "df1['Batch'] =  pd.to_datetime(df1['Batch'], format='%Y/%m/%d')\n",
    "#Column creation with the week number, base on the batch number:\n",
    "df1['Week'] = df1['Batch'].dt.isocalendar().week\n",
    "#Column change type to string:\n",
    "df1['RCA_ID'] = df1['RCA_ID'].astype(str)\n",
    "#Trim the values of the columns:\n",
    "df1['RCA_ID']= df1['RCA_ID'].str.strip()\n",
    "#Lower case the values of the colum:\n",
    "df1['RCA_ID']= df1['RCA_ID'].str.lower()\n",
    "df1['1st RCA']= df1['1st RCA'].str.lower()\n",
    "df1['2nd RCA']= df1['2nd RCA'].str.lower()\n",
    "#Filter out the nan values from the column:\n",
    "df1= df1.loc[df1['RCA_ID'] != 'nan']\n",
    "#Creating a dataframe with a specific columns:\n",
    "df1=df1[['Batch','Week','Simulation Queue Round','Market','Agree/Not agree','1st RCA','2nd RCA','RCA_ID']]\n",
    "# Extract the column you want to compare\n",
    "column_to_compare = 'RCA_ID'\n",
    "# Check if values from df2's column_to_compare are in df1\n",
    "df1['RCA Validation'] = df1[column_to_compare].isin(DIM_RCA[column_to_compare]).map({True: 'Values present', False: 'Wrong RCA'})\n",
    "def validate_rca(row):\n",
    "    if row['1st RCA'] != 'correct case' and pd.isnull(row['2nd RCA']):\n",
    "        return 'Wrong RCA'\n",
    "    else:\n",
    "        return row['RCA Validation']\n",
    "df1['RCA Validation'] = df1.apply(validate_rca, axis=1)\n",
    "#New data set will be generated only with the values that are \"Missing\"\n",
    "missing_data= df1.loc[df1['RCA Validation'] == 'Wrong RCA']\n",
    "missing_data = missing_data.drop_duplicates(keep='first', inplace=False)\n",
    "#Creating a dataframe with a specific columns:\n",
    "missing_data=missing_data[['Batch','Week','Simulation Queue Round','Market','Agree/Not agree','1st RCA','2nd RCA','RCA Validation']]\n",
    "\n",
    "print('----------------------------------------------')\n",
    "#_____________________________________________________________\n",
    "\n",
    "#If the variable \"missing_data\" will be empty the file will not be save in excel on downloads folder:\n",
    "if not missing_data.empty:\n",
    "    # Define the path for the output Excel file\n",
    "    missing_data.to_excel(fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\GA1.0_RCA_validation.xlsx\",index=False)\n",
    "    \n",
    "#_________Trigger an e-mail if the a Missing_data will be generated__________   \n",
    "\n",
    "    # Create an Outlook instance\n",
    "    ol = win32com.client.Dispatch(\"outlook.application\")\n",
    "    olmailitem = 0x0  # Mail item\n",
    "    newmail = ol.CreateItem(olmailitem)\n",
    "    newmail.Subject = 'GA 1.0 RCA with mistakes'\n",
    "    newmail.To = ';'.join(lista_emails)  # Replace with the recipient's email address\n",
    "    newmail.CC = \"Data-Analytics-TikTok-Lisbon@pt.teleperformance.com\"  # Replace with the CC email address if needed\n",
    "    newmail.Body = '''\\\n",
    "Hello Team,\n",
    "\n",
    "Could you please correct the RCA of GA 1.0 that are incorrect on the attached file.\n",
    "\n",
    "Please let us know when all the errors will be correct, so we can extract all the data again.\n",
    "\n",
    "\n",
    "Thank you.\n",
    "\n",
    "'''\n",
    "    # Attach the Excel file to the email\n",
    "    attach1 = fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\GA1.0_RCA_validation.xlsx\"\n",
    "    attach1 = newmail.Attachments.Add(attach1)\n",
    "    # To display the mail before sending it\n",
    "    # newmail.Display()\n",
    "\n",
    "    # Send the email\n",
    "    newmail.Send()\n",
    "    \n",
    "    print(\"Incorrect RCA found, the email will be send.\")\n",
    "else:\n",
    "    print(\"All the RCAs are correct\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971afea4",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7449165",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc8307",
   "metadata": {},
   "source": [
    "# (!!!MONDAY // WED!!!)   UNO Project GA- Glidepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7ec54",
   "metadata": {},
   "source": [
    "__Download glidepath file for the LOB(ttr1 and ttr2):__\n",
    "\n",
    "\n",
    "__TTR1 IT // TAB: GA Review(100) GA 2.0__  https://teleperformance.larksuite.com/sheets/GR5zsTOhYhlcCetrj2auysbRsff?sheet=tux0nD\n",
    "\n",
    "__TTR2 IT // TAB: GA Review(100) GA 2.0__  https://teleperformance.larksuite.com/sheets/HUnHsthM3hFsPvt90U2u2S4zsvg?sheet=d55kza\n",
    "\n",
    "__TTR1 FR Link:TAB:Sample Cases__    https://teleperformance.larksuite.com/sheets/shtusRvfqaTzYkEzWeBeeCXNl4c?sheet=sbfUJT\n",
    "\n",
    "__TTR1 FR Creator Link:TAB:Sample Cases__ https://teleperformance.larksuite.com/sheets/shtus1Juf67YEF9ZXlx0jjVSJ2d?sheet=HnwcUG\n",
    "\n",
    "__TTR1 NL Link:TAB:Sample Cases__ https://teleperformance.larksuite.com/sheets/shtus7tLBP6YaA73FmeLO7aGnTh?sheet=Kh9ZTH\n",
    "\n",
    "__TTR1 ES Link:TAB:Sample Cases__ https://teleperformance.larksuite.com/sheets/shtusbL94YqOzSPResz49UNwHYM?sheet=scRTg8\n",
    "\n",
    "__TTR2 FR Link:TAB:Sample Cases__ https://teleperformance.larksuite.com/sheets/WeuXsloKmh7g1ati1dbuyyVOs2g?sheet=bUgE81\n",
    "\n",
    "__SPS IT__ https://teleperformance.larksuite.com/sheets/BMmGs7rfghlgehtMwHMuMTkMsCf?sheet=EfOtFR\n",
    "\n",
    "__SPS FR__  https://teleperformance.larksuite.com/sheets/BMmGs7rfghlgehtMwHMuMTkMsCf?sheet=RAS00L\n",
    "\n",
    "__SPS NL__ https://teleperformance.larksuite.com/sheets/BMmGs7rfghlgehtMwHMuMTkMsCf?sheet=f910b9\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "__Step 1 :__ Run the below code to move the Glidepath to EMEA folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aede838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moveed with success.\n"
     ]
    }
   ],
   "source": [
    "#Glidepath location:\n",
    "UNO_GA_IT_TTR2= fr\"C:\\Users\\\\{username}\\\\{folder}\\TTR2 - UNO Project - Masterfile - GA Review (100) GA 2.0.csv\"\n",
    "UNO_GA_IT_TTR1= fr\"C:\\Users\\\\{username}\\\\{folder}\\TTR1 - UNO Project - Masterfile - GA Review (100) GA 2.0.csv\"\n",
    "UNO_GA_TTR1FR= fr\"C:\\Users\\\\{username}\\\\{folder}\\FR TTR1 Glidepath & Action Tracker - GA Review (100) GA 2.0.csv\"\n",
    "UNO_GA_TTR1NL = fr\"C:\\Users\\\\{username}\\\\{folder}\\NL TTR1 Glidepath & Action Tracker - GA Review (100) GA 2.0.csv\"\n",
    "UNO_GA_CreatorFR= fr\"C:\\Users\\\\{username}\\\\{folder}\\Glidepath Creator TTR1 FR - GA Review (100) GA 2.0.csv\"\n",
    "UNO_GA_TTR1ES= fr\"C:\\Users\\\\{username}\\\\{folder}\\ES TTR1 Glidepath & Action Tracker - GA Review (100) GA 2.0.csv\"\n",
    "UNO_GA_TTR1= fr\"C:\\Users\\\\{username}\\\\{folder}\\NL TTR1 Glidepath & Action Tracker - GA Review (100) GA 2.0.csv\"\n",
    "Uno_GA_TTR2 = fr\"C:\\Users\\\\{username}\\\\{folder}\\TTR2 - GA - FR Masterfile  - GA Review (100) GA 2.0.csv\"\n",
    "Uno_GA_SPS_IT =fr\"C:\\Users\\\\{username}\\\\{folder}\\SPS GA Masterfile - IT GA Review (100) GA 2.0.csv\"\n",
    "Uno_GA_SPS_FR = fr\"C:\\Users\\\\{username}\\\\{folder}\\SPS GA Masterfile - FR GA Review (100) GA 2.0.csv\"\n",
    "Uno_GA_SPS_NL = fr\"C:\\Users\\\\{username}\\\\{folder}\\SPS GA Masterfile - NL GA Review (100) GA 2.0.csv\"\n",
    "#File destination:\n",
    "UNO_Glidepath_location=r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\UNO Project\\Glidepath files\"\n",
    "\n",
    "#This code will mode the file to the destination folder:\n",
    "try:\n",
    "    # Move the files from EMEA folder to DROPS folder \n",
    "\n",
    "    shutil.copy(UNO_GA_IT_TTR2, UNO_Glidepath_location)\n",
    "    shutil.copy(UNO_GA_IT_TTR1, UNO_Glidepath_location)\n",
    "    shutil.copy(UNO_GA_TTR1NL,UNO_Glidepath_location)\n",
    "    shutil.copy(UNO_GA_TTR1FR, UNO_Glidepath_location)\n",
    "    shutil.copy(UNO_GA_CreatorFR, UNO_Glidepath_location)\n",
    "    shutil.copy(UNO_GA_TTR1ES, UNO_Glidepath_location)\n",
    "    shutil.copy(UNO_GA_TTR1, UNO_Glidepath_location)\n",
    "    shutil.copy(Uno_GA_TTR2, UNO_Glidepath_location)\n",
    "    shutil.copy(Uno_GA_SPS_IT, UNO_Glidepath_location)\n",
    "    shutil.copy(Uno_GA_SPS_FR, UNO_Glidepath_location)\n",
    "    shutil.copy(Uno_GA_SPS_NL, UNO_Glidepath_location)\n",
    "\n",
    "    \n",
    "    print(\"Files moveed with success.\")\n",
    "except Exception as e:\n",
    "    print(\"Fails\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85347539",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71d0dc",
   "metadata": {},
   "source": [
    "#  (!!!MONDAY // WED!!!)UNO Project(GA SAMPLING)- Client Data // RUN THE UNO SCRIPT: CODE AFTER TTR1 CLIENT DATA \n",
    "\n",
    "\n",
    "\n",
    "__LINK:__ https://datapower-va.bytelemon.com/bi/visit/7325384900452941829?immersive=1\n",
    "__TAB:__ GA Performance\n",
    "\n",
    "__SHEET:__ GA 2.0\n",
    "\n",
    "__FILTERS TOP:__  \n",
    "\n",
    "         __Site:__ \" TP_LIS \" /// PRESS: \"QUERRY\" to applay the filters.\n",
    "\n",
    "         __Batch Date__ ( files from saturday to  Friday) This how the client separates the weeks data. \n",
    "\n",
    "__Table:__ penultimate Table >> \"Case Picker - Simulation\"\n",
    "\n",
    "__Extract:__ Last 2 weeks data available. \n",
    "\n",
    "__Format:__ CSV.  // 1000000 of rows\n",
    "\n",
    "Download client files, LAST 2 WEEKS DATA.\n",
    "\n",
    "__Run the bellow code:__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a91d00",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb13a9",
   "metadata": {},
   "source": [
    "#Client data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d567cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week File Count Dictionary:\n",
      "{'2024 W31': [('_Case Picker - Simulation - 2024-08-21 08-56-25.csv', '2024-08-03')]}\n",
      "--------------------------------------------------------------------------\n",
      "File copied to: _Case Picker - Simulation  - 2024 W31.csv   // Unique date: 2024-08-03\n",
      "-----------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS CODE WILL ADAPT THE FILE NAME AND SEND THE FILE TO THE FOLDER DESTINATION:\n",
    "destination = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\UNO Project\\UNO_GA\"\n",
    "\n",
    "\n",
    "# List files in the downloads folder\n",
    "files = os.listdir(downloads_folder_path)\n",
    "week_file_count = {}\n",
    "def transform_date(batch):\n",
    "    match = re.search(r'(\\d{4})-W\\d{2}\\((\\d{2})/(\\d{2})~\\d{2}/\\d{2}\\)', batch)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        day = match.group(3)\n",
    "        return f\"{year}-{month}-{day}\"\n",
    "    else:\n",
    "        return batch\n",
    "for file in files:\n",
    "    if file.startswith(\"_Case Picker\"):  \n",
    "        source_path = os.path.join(downloads_folder_path, file)\n",
    "        df = pd.read_csv(source_path, delimiter=\",\")\n",
    "        df.rename(columns={'Batch(Date)': 'Batch'}, inplace=True)\n",
    "        # Apply the transformation to the Batch column\n",
    "        df[\"Batch\"] = df[\"Batch\"].apply(transform_date)\n",
    "        df[\"Batch\"] = df[\"Batch\"].astype(str)\n",
    "        # Extract the date from the first row\n",
    "        date_str = df[\"Batch\"].iloc[0]\n",
    "        date = datetime.strptime(date_str, \"%Y-%m-%d\")  \n",
    "        \n",
    "         # Calcular a semana ISO\n",
    "        year, week, _ = date.isocalendar()\n",
    "        week_str = f\"{year} W{week:02d}\"\n",
    "        \n",
    "        df[\"Batch\"] = date.strftime(\"%Y-%m-%d\")\n",
    "        unique_date = date_str \n",
    "        \n",
    "        if week in week_file_count:\n",
    "            week_file_count[week_str].append((file, unique_date))\n",
    "        else:\n",
    "            week_file_count[week_str] = [(file, unique_date)]\n",
    "        # Salvar o DataFrame com o mesmo nome do arquivo original\n",
    "        output_path = os.path.join(downloads_folder_path, file)\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Week File Count Dictionary:\")\n",
    "print(week_file_count)\n",
    "print(\"--------------------------------------------------------------------------\")\n",
    "\n",
    "for week, files_list in week_file_count.items():\n",
    "    for index, (file, unique_date) in enumerate(files_list, 1):\n",
    "        source_path = os.path.join(downloads_folder_path, file)\n",
    "        if len(files_list) == 1:\n",
    "            new_name = f\"_Case Picker - Simulation  - {week}.csv\"\n",
    "        else:\n",
    "            new_name = f\"_Case Picker - Simulation  - {week}_part{index}.csv\"\n",
    "\n",
    "        destination_path = os.path.join(destination, new_name)\n",
    "        shutil.copy(source_path, destination_path)\n",
    "        print(f\"File copied to: {new_name}   // Unique date: {unique_date}\")\n",
    "\n",
    "\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "#This line will remove the files from your download files:\n",
    "[os.remove(os.path.join(downloads_folder_path, filename)) for filename in os.listdir(downloads_folder_path)\n",
    " if filename.startswith(\"_Case Picker\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121b4ce",
   "metadata": {},
   "source": [
    "\n",
    "# (SCRIPT): UNO SCRIPT\n",
    "__The next script will run th UNO script base on the inputs we give:__\n",
    "\n",
    "__!!!IMP: CHANGE THE INPUT TO THE CORRECT WEEK NUMBER, THE SAME AS THE WEEKS IN THE ABOVE CODE. !!!!__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c702e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: CompletedProcess(args=['python', '\\\\\\\\emea.tpg.ads\\\\portugal\\\\Departments\\\\ITDEV\\\\PowerBI\\\\accounting\\\\business analysts\\\\01 - ba\\\\02 - projects\\\\tiktok\\\\10. Python scripts\\\\NEW ETL\\\\Aux Scripts\\\\GA Sampling\\\\GA Sampling_new.py'], returncode=0)\n",
      "Result 2: CompletedProcess(args=['python', '\\\\\\\\emea.tpg.ads\\\\portugal\\\\Departments\\\\ITDEV\\\\PowerBI\\\\accounting\\\\business analysts\\\\01 - ba\\\\02 - projects\\\\tiktok\\\\10. Python scripts\\\\NEW ETL\\\\Aux Scripts\\\\GA Sampling\\\\GA Sampling_new.py'], returncode=0)\n",
      "Result 3: CompletedProcess(args=['python', '\\\\\\\\emea.tpg.ads\\\\portugal\\\\Departments\\\\ITDEV\\\\PowerBI\\\\accounting\\\\business analysts\\\\01 - ba\\\\02 - projects\\\\tiktok\\\\10. Python scripts\\\\NEW ETL\\\\Aux Scripts\\\\GA Sampling\\\\GA Sampling_new.py'], returncode=0)\n"
     ]
    }
   ],
   "source": [
    "Script_path = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\10. Python scripts\\NEW ETL\\Aux Scripts\\UNO Sampling\\UNO Sampling_new.py\"\n",
    "\n",
    "\n",
    "#run the second input\n",
    "\n",
    "\n",
    "Input4 = '2024 W31'\n",
    "Result4 = subprocess.run(['python', Script_path], input=Input4, text=True)\n",
    "\n",
    "Input5 = '2024 W32'\n",
    "Result5 = subprocess.run(['python', Script_path], input=Input5, text=True)\n",
    "\n",
    "Input8 = '2024 W33'\n",
    "Result8 = subprocess.run(['python', Script_path], input=Input8, text=True)\n",
    "\n",
    "print(\"Result 1:\", Result1)\n",
    "print(\"Result 2:\", Result2)\n",
    "print(\"Result 3:\", Result3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bf3e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: UNO_Moderator Error Case Picker_2024 W33.csv\n",
      "Reading file: UNO_Moderator Error Case Picker_2024 W32.csv\n",
      "Reading file: UNO_Moderator Error Case Picker_2024 W31.csv\n",
      "Reading file: UNO_Moderator Error Case Picker_2024 W30.csv\n",
      "----------------------------------------------\n",
      "Incorrect RCA found, the email will be send.\n"
     ]
    }
   ],
   "source": [
    "#lista_emails=['pedro.esteves@teleperformance.com']\n",
    "lista_emails=['divelly.rivera@teleperformance.com','dario.rocco@teleperformance.com','soraia.amarelinho@teleperformance.com',\n",
    "      'alexia.dejesus@teleperformance.com','domenico.frisone@pt.teleperformance.com','vanessa.rubio@teleperformance.com','natalia.shkodenko@teleperformance.com',]\n",
    "#----------STD_DIM-------------\n",
    "#Location of STD_DIM:\n",
    "DIM_RCA_path= r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\STD_DIMS_TTOK new.xlsx'\n",
    "#Reading the STD_DIM and the sheet \"DIM_RCA\", and pushing just the column \"RCA_ID\":\n",
    "DIM_RCA=pd.read_excel(DIM_RCA_path,sheet_name='DIM_RCA')[['RCA_ID']]\n",
    "#Filtring the column for the data that start with \" Golden Assessment\":\n",
    "DIM_RCA=DIM_RCA[DIM_RCA.RCA_ID.str.startswith('Golden Assessment/')]\n",
    "#Trim the all values:\n",
    "DIM_RCA['RCA_ID']= DIM_RCA['RCA_ID'].str.strip()\n",
    "#Lower case all the values\n",
    "DIM_RCA['RCA_ID']= DIM_RCA['RCA_ID'].str.lower()\n",
    "\n",
    "#------End of manipulation of the dataframe STD_DIM----------\n",
    "\n",
    "#Creation of a function that will read just the last 4 the final data of GA on folder GA_samplin_new, folder Final_version:\n",
    "def get_df1(folder_path):\n",
    "    df1 = pd.DataFrame()\n",
    "    os.chdir(folder_path)\n",
    "    filesGA = sorted((f for f in os.listdir() if f.endswith('.csv')), key=os.path.getmtime, reverse=True)[:4]\n",
    "    for file in filesGA:\n",
    "        print(\"Reading file:\", file)  # Print the name of the file being read\n",
    "        aux = pd.read_csv(file, error_bad_lines=False)\n",
    "        df1 = df1.append(aux)\n",
    "    return df1\n",
    "#Read location of the Final version of GA:\n",
    "folder_path =  r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\UNO Project\\Final_version\"\n",
    "#Creating a dataframe of GA data:\n",
    "df1 = get_df1(folder_path)\n",
    "#Column creation \"RCA_ID\" with a concat of a string with 3 other columns with \"/\" as separation:\n",
    "df1['RCA_ID'] = 'Golden Assessment' + '/' + df1['Agree/Not agree'].astype(str) + '/' + df1['1st RCA'].astype(str) + '/' + df1['2nd RCA']\n",
    "\n",
    "#Read the DIM_LOB\n",
    "lob=pd.read_excel(r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\STD_DIMS_TTOK new.xlsx',sheet_name='DIM_LOB')\n",
    "lob['QUEUE_ID_L7']=lob['QUEUE_ID_L7'].astype(str)\n",
    "df1[\"Queue ID_GA\"]=df1[\"Queue ID_GA\"].astype(str)\n",
    "dim_lob=lob.rename(columns ={\"QUEUE_ID_L7\":\"Queue ID_GA\"})\n",
    "dim_lob[\"Queue ID_GA\"]=dim_lob[\"Queue ID_GA\"].str.strip()   \n",
    "\n",
    "df1 = pd.merge(df1, dim_lob[['Queue ID_GA', 'QUEUE_NAME_L8']].drop_duplicates(subset=['Queue ID_GA']), on='Queue ID_GA', how='left')\n",
    "\n",
    "\n",
    "\n",
    "df1['RCA_ID'] = df1['RCA_ID'].astype(str)\n",
    "#Trim the values of the columns:\n",
    "df1['RCA_ID']= df1['RCA_ID'].str.strip()\n",
    "#Lower case the values of the colum:\n",
    "df1['RCA_ID']= df1['RCA_ID'].str.lower()\n",
    "df1['1st RCA']= df1['1st RCA'].str.lower()\n",
    "df1['2nd RCA']= df1['2nd RCA'].str.lower()\n",
    "#Filter out the nan values from the column:\n",
    "df1= df1.loc[df1['RCA_ID'] != 'nan']\n",
    "\n",
    "#df1.to_csv(r\"C:\\Users\\esteves.155\\Downloads\\rca_test.csv\")\n",
    "#Creating a dataframe with a specific columns:\n",
    "df1=df1[['Batch','Market Name_GA','QUEUE_NAME_L8','Agree/Not agree','1st RCA','2nd RCA','RCA_ID']]\n",
    "# Extract the column you want to compare\n",
    "column_to_compare = 'RCA_ID'\n",
    "# Check if values from df2's column_to_compare are in df1\n",
    "df1['RCA Validation'] = df1[column_to_compare].isin(DIM_RCA[column_to_compare]).map({True: 'Values present', False: 'Wrong RCA'})\n",
    "def validate_rca(row):\n",
    "    if row['1st RCA'] != 'correct case' and pd.isnull(row['2nd RCA']):\n",
    "        return 'Wrong RCA'\n",
    "    else:\n",
    "        return row['RCA Validation']\n",
    "df1['RCA Validation'] = df1.apply(validate_rca, axis=1)\n",
    "#New data set will be generated only with the values that are \"Missing\"\n",
    "missing_data= df1.loc[df1['RCA Validation'] == 'Wrong RCA']\n",
    "missing_data = missing_data.drop_duplicates(keep='first', inplace=False)\n",
    "#Creating a dataframe with a specific columns:\n",
    "missing_data=missing_data[['Batch','Market Name_GA','QUEUE_NAME_L8','Agree/Not agree','1st RCA','2nd RCA','RCA Validation']]\n",
    "\n",
    "\n",
    "print('----------------------------------------------')\n",
    "#_____________________________________________________________\n",
    "\n",
    "#If the variable \"missing_data\" will be empty the file will not be save in excel on downloads folder:\n",
    "if not missing_data.empty:\n",
    "    # Define the path for the output Excel file\n",
    "    missing_data.to_excel(fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\GA2.0_RCA_validation.xlsx\",index=False)\n",
    "    \n",
    "#_________Trigger an e-mail if the a Missing_data will be generated__________   \n",
    "\n",
    "    # Create an Outlook instance\n",
    "    ol = win32com.client.Dispatch(\"outlook.application\")\n",
    "    olmailitem = 0x0  # Mail item\n",
    "    newmail = ol.CreateItem(olmailitem)\n",
    "    newmail.Subject = 'GA 2.0 UNO RCA with mistakes'\n",
    "    newmail.To = ';'.join(lista_emails)  # Replace with the recipient's email address\n",
    "    newmail.CC = 'Data-Analytics-TikTok-Lisbon@pt.teleperformance.com'  # Replace with the CC email address if needed\n",
    "    newmail.Body = '''\\\n",
    "Hello Team,\n",
    "\n",
    "Could you please correct the RCA of GA2.0 that are incorrect on the attached file.\n",
    "\n",
    "Please let us know when all the errors will be correct, so we can extract all the data again.\n",
    "\n",
    "\n",
    "Thank you.\n",
    "\n",
    "'''\n",
    "    # Attach the Excel file to the email\n",
    "    attach1 = fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\GA2.0_RCA_validation.xlsx\"\n",
    "    attach1 = newmail.Attachments.Add(attach1)\n",
    "    # To display the mail before sending it\n",
    "    # newmail.Display()\n",
    "\n",
    "    # Send the email\n",
    "    newmail.Send()\n",
    "    \n",
    "    print(\"Incorrect RCA found, the email will be send.\")\n",
    "else:\n",
    "    print(\"All the RCAs are correct\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcd18f",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5878bb",
   "metadata": {},
   "source": [
    "#  (!!!MONDAY) CGVR Quality\n",
    "\n",
    "__Link__:https://datapower-va.bytelemon.com/bi/visit/7325384900452941829?immersive=1\n",
    "\n",
    "__TAB:__ TCOR/CVGR\n",
    "\n",
    "__SHEET:__ CGVR\n",
    "\n",
    "__FILTERS TOP:__  _Site:_ \" TP_LIS \" // Moderation Date: from sunday to saturday\n",
    "\n",
    "__Table:__ CGVR Case Picker\n",
    "\n",
    "__Extract:__ Last 2 weeks data available (last and previous). \n",
    "\n",
    "__Format:__ CSV.  // 1000000 of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9320601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied to: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\CCHA\\CGVR\\_CGVR Case Picker  - 2024 W33.csv\n",
      "File copied to: C:\\Users\\\\bechara.9\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\CCHA\\CGVR\\_CGVR Case Picker  - 2024 W33.csv\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "# Destinos para os arquivos\n",
    "destination1 = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\CCHA\\CGVR\"\n",
    "destination2 = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\CCHA\\CGVR\"\n",
    "\n",
    "def extract_week_from_mod_resolve_time(mod_resolve_time):\n",
    "    match = re.search(r'(\\d{4}-W\\d{2})\\(\\d{2}/\\d{2}~\\d{2}/\\d{2}\\)', mod_resolve_time)\n",
    "    if match:\n",
    "        # Retirar o hífen e substituir por espaço\n",
    "        return match.group(1).replace('-', ' ')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Caminho para a pasta de downloads tradicional\n",
    "downloads_folder_path = os.path.join(os.path.expanduser('~'), 'Downloads')\n",
    "files = os.listdir(downloads_folder_path)\n",
    "\n",
    "# Listar arquivos que começam com \"_CGVR Case Picker\"\n",
    "for file in files:\n",
    "    if file.startswith(\"_CGVR Case Picker\"):  \n",
    "        source_path = os.path.join(downloads_folder_path, file)\n",
    "        df = pd.read_csv(source_path, delimiter=\",\")\n",
    "\n",
    "        # Extrair a semana da primeira linha da coluna \"Mod. Resolve Time\"\n",
    "        week_str = None\n",
    "        for mod_resolve_time in df[\"Mod. Resolve Time\"]:\n",
    "            week_str = extract_week_from_mod_resolve_time(mod_resolve_time)\n",
    "            if week_str:\n",
    "                break\n",
    "\n",
    "        if week_str:\n",
    "            # Formatar o nome do novo arquivo corretamente\n",
    "            new_name = f\"_CGVR Case Picker  - {week_str}.csv\"\n",
    "            destination_path1 = os.path.join(destination1, new_name)\n",
    "            destination_path2 = os.path.join(destination2, new_name)\n",
    "            \n",
    "            # Substituir arquivo existente, se já estiver presente (Destino 1)\n",
    "            if os.path.exists(destination_path1):\n",
    "                os.remove(destination_path1)\n",
    "            shutil.copy2(source_path, destination_path1)\n",
    "            print(f\"File copied to: {destination_path1}\")\n",
    "            \n",
    "            # Substituir arquivo existente, se já estiver presente (Destino 2)\n",
    "            if os.path.exists(destination_path2):\n",
    "                os.remove(destination_path2)\n",
    "            shutil.copy2(source_path, destination_path2)\n",
    "            print(f\"File copied to: {destination_path2}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Could not extract week from: {file}\")\n",
    "\n",
    "print(\"Process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2fd6c0",
   "metadata": {},
   "source": [
    "#  (!!!MONDAY) TCOR Quality\n",
    "\n",
    "__Link__:https://datapower-va.bytelemon.com/bi/visit/7325384900452941829?immersive=1\n",
    "\n",
    "__TAB:__ TCOR/CVGR\n",
    "\n",
    "__SHEET:__ TCOR\n",
    "\n",
    "__FILTERS TOP:__  _Site:_ \" TP_LIS \" // Moderation Date: from sunday to saturday\n",
    "\n",
    "__Table:__ TCOR Quality Case Picker\n",
    "\n",
    "__Extract:__ Last 2 weeks data available (last and previous). \n",
    "\n",
    "__Format:__ CSV.  // 1000000 of rows \n",
    "\n",
    "__Filename:__  manually rename file to = _TCOR Quality Case Picker  - AAAA W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "373447f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied to: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\CCHA\\TCOR\\_TCOR Quality Case Picker  - 2024 W33.csv\n",
      "File copied to: C:\\Users\\\\bechara.9\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\CCHA\\TCOR\\_TCOR Quality Case Picker  - 2024 W33.csv\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "destination1 = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\CCHA\\TCOR\"\n",
    "destination2 = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\CCHA\\TCOR\"\n",
    "\n",
    "# Caminho para a pasta de downloads tradicional\n",
    "downloads_folder_path = os.path.join(os.path.expanduser('~'), 'Downloads')\n",
    "files = os.listdir(downloads_folder_path)\n",
    "\n",
    "# List files that start with \"_TCOR Quality Case Picker\"\n",
    "for file in files:\n",
    "    if file.startswith(\"_TCOR Quality Case Picke\"):  \n",
    "        source_path = os.path.join(downloads_folder_path, file)\n",
    "        \n",
    "        # Caminho para o primeiro destino\n",
    "        destination_path1 = os.path.join(destination1, file)\n",
    "        \n",
    "        # Substituir arquivo existente, se já estiver presente (Primeiro destino)\n",
    "        if os.path.exists(destination_path1):\n",
    "            os.remove(destination_path1)\n",
    "        \n",
    "        # Copiar o arquivo para o primeiro destino\n",
    "        shutil.copy2(source_path, destination_path1)\n",
    "        print(f\"File copied to: {destination_path1}\")\n",
    "        \n",
    "        # Caminho para o segundo destino\n",
    "        destination_path2 = os.path.join(destination2, file)\n",
    "        \n",
    "        # Substituir arquivo existente, se já estiver presente (Segundo destino)\n",
    "        if os.path.exists(destination_path2):\n",
    "            os.remove(destination_path2)\n",
    "        \n",
    "        # Copiar o arquivo para o segundo destino\n",
    "        shutil.copy2(source_path, destination_path2)\n",
    "        print(f\"File copied to: {destination_path2}\")\n",
    "\n",
    "print(\"Process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d67fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
