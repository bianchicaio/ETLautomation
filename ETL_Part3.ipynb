{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4150ad40",
   "metadata": {},
   "source": [
    "# ETL_Part3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e018b5",
   "metadata": {},
   "source": [
    "__!!!!!IMP!!!!: On the day when the timezone changes, we must implement a new logic. We utilize the current UTC time, but it's crucial not to mix data from before and after the time change. Otherwise, all data will advance by one hour. If you use the old UTC time, the data will be delayed by one hour.__\n",
    "\n",
    "   _Portugal > Winter: \"UTC +0\" || Summers: \"UTC +1\" (+01:00 Paris now)_\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c109bf0",
   "metadata": {},
   "source": [
    "__Run the bellow 2 groups ( Libraries +USERNAME and DOWNLOAD FOLDER)__\n",
    "\n",
    " 1 group will import the libraries, if some of it will not work, please open a new code cell and write: Example ( pip install shutill) and run it, this will install the library\n",
    " \n",
    " 2 group will read you username and the folder where you have the downloads, you can have another then download, just ajust the variable \" folder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5930b6d",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d846f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import re\n",
    "import win32com.client\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import win32com.client as win32\n",
    "import subprocess\n",
    "import warnings\n",
    "import win32com.client as w3c\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc8b32",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55766696",
   "metadata": {},
   "source": [
    "# USERNAME and DOWNLOAD FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12ec8d",
   "metadata": {},
   "source": [
    "__!!!CHANGE THE  USER NAME, AND DOWNLOAD PATH IF NEEDED:__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4c9e21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your username:bechara.9\n",
      "Folder where is your downloads:C:\\\\Users\\\\bechara.9\\\\Downloads\n",
      "---------------------------------------------\n",
      "Empty DataFrame\n",
      "Columns: [Filename, Date, Status, Message]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#THE CODE SHOULD SHOW YOUR NAME:\n",
    "username = os.getlogin()\n",
    "print('Your username:'+ username)\n",
    "#IF YOU DONT USE YOUR DOWNLOAD FOLDER CHANGE THE BELOW STRING TO YOUR FOLDER:\n",
    "folder=\"Downloads\"\n",
    "downloads_folder_path = fr\"C:\\\\Users\\\\{username}\\\\{folder}\"\n",
    "print('Folder where is your downloads:' + downloads_folder_path)\n",
    "#Clean python cache\n",
    "gen_py_path = fr\"C:\\Users\\\\{username}\\\\AppData\\Local\\Temp\\gen_py\"\n",
    "shutil.rmtree(gen_py_path, ignore_errors=True)\n",
    "\n",
    "date_today= datetime.today().strftime('%d-%m-%Y')\n",
    "\n",
    "\n",
    "log_df = pd.DataFrame(columns=['Filename', 'Date', 'Status', 'Message'])\n",
    "log_df.drop(log_df.index, inplace=True)\n",
    "print(\"---------------------------------------------\")\n",
    "print(log_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18869f",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802a59c",
   "metadata": {},
   "source": [
    "# __________________________________MI DATA___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de623bc2",
   "metadata": {},
   "source": [
    "## Ads Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48226fe4",
   "metadata": {},
   "source": [
    "Link:  https://datapower-va.bytelemon.com/bi/visit/6953507931698642950\n",
    "\n",
    "- Dashboard: Ads & Account Review >>> Ads Review Raw Data\n",
    "\n",
    "\n",
    "- You need to extract the current week ( files de Monday to Sunday)+ previous week data, on the fowlloing logic:\n",
    "\n",
    "  - __Extract 2 files per week:__ 1º file with the filter (__AFFECT EXPERIENCIE : VALID TASK__) and other with  (__AFFECT EXPERIENCIE :INVALID TASK__)-- on the name file for the invalid, rename it by adding in the end the letter (v) \"!! small letter !!\"\n",
    "  \n",
    "  \n",
    "- Filter to use:\n",
    "  - __AUDITOR SITE:__  LIS_TP\n",
    "  - __TASK FLAG:__ unselect both piscos on the bottom \"Select all\" +\"Exclude\"\n",
    "  - __MANUAL OPERATION FLAG :__ 1  (Normally is all the time on 1)\n",
    "  - __AFFECT EXPERIENCIE :__ valid task   or  invalid task \n",
    "\n",
    "!!Affter the download of all the data, run the below code and the code will merge together the files from the same week. !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee3cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\u001b[1;31mAds_Review\u001b[0m', the most recent file is: '\u001b[1;31mAdsReview_RawData-02092024_08092024.csv\u001b[0m'\n"
     ]
    }
   ],
   "source": [
    "mi_ads_Ads_Review={'Ads_Review':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Ads_Efficiency\"}\n",
    "def format_text(text, color_code, bold=False):\n",
    "    return f\"\\033[1;{color_code}m{text}\\033[0m\" if bold else f\"\\033[{color_code}m{text}\\033[0m\"\n",
    "\n",
    "for short_name, folder_path in mi_ads_Ads_Review.items():\n",
    "    if os.path.isdir(folder_path) and (files := os.listdir(folder_path)):\n",
    "        date_pattern = re.compile(r'_(\\d{8})_(\\d{8})')\n",
    "\n",
    "        max_date_file = max(\n",
    "            (file for file in files if (date_matches := date_pattern.search(file)) or (date_matches := re.search(r'(\\d{8})_(\\d{8})', file))),\n",
    "            key=lambda file: datetime.strptime(date_matches.groups()[1], '%d%m%Y') if date_matches else datetime.min,\n",
    "            default=None)\n",
    "\n",
    "        if max_date_file:\n",
    "            max_date_format = format_text(max_date_file, 31, bold=True)\n",
    "            short_name_format = format_text(short_name, 31, bold=True)\n",
    "            print(f\"'{short_name_format}', the most recent file is: '{max_date_format}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c459f47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_Ads Review Raw Data - last_week_valid.csv', '_Ads Review Raw Data - week_before_last_valid.csv']\n",
      "['_Ads Review Raw Data - last_week_invalid_v.csv', '_Ads Review Raw Data - week_before_last_invalid_v.csv']\n"
     ]
    }
   ],
   "source": [
    "#fOLDER DESTINATION:\n",
    "Ads_Review_destination_datadrops = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\MI_Ads\"\n",
    "Ads_Review_destination_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Ads_Efficiency\"\n",
    "#Sharepoitn\n",
    "ADS_Review_onedrive = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Ads_Efficiency\"\n",
    "#lOOK FOR THE FILE DOWNLOADED BASE ON ACONDITION:\n",
    "Ads_Review_files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                    filename.startswith(\"_Ads Review Raw Data\") and not filename.endswith(\"v.csv\")]\n",
    "Ads_Error_files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                   filename.startswith(\"_Ads Review Raw Data\") and filename.endswith(\"v.csv\")]\n",
    "print(Ads_Review_files)\n",
    "print(Ads_Error_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882f59ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Filename      Date   Status  \\\n",
      "0  AdsReview_RawData-02092024_08092024.csv  02092024  Success   \n",
      "1  AdsReview_RawData-26082024_01092024.csv  26082024  Success   \n",
      "\n",
      "                   Message  \n",
      "0  File saved successfully  \n",
      "1  File saved successfully  \n"
     ]
    }
   ],
   "source": [
    "# Creation of empty dictionaries to store DataFrames\n",
    "ads_review_dict = {}\n",
    "ads_error_dict = {}\n",
    "\n",
    "# For loop to read the files and create columns \"Affect Experience\"\n",
    "for o in Ads_Review_files:\n",
    "        p = pd.read_csv(downloads_folder_path + '\\\\' + o)\n",
    "        p['Affect Experience'] = 'Valid Task'\n",
    "        date = pd.to_datetime(p['operate_date'].min()).strftime('%d%m%Y')\n",
    "        ads_review_dict.setdefault(date, []).append(p)\n",
    "    \n",
    "for q in Ads_Error_files:\n",
    "        r = pd.read_csv(downloads_folder_path + '\\\\' + q)\n",
    "        r['Affect Experience'] = 'Invalid Task'\n",
    "        date = pd.to_datetime(r['operate_date'].min()).strftime('%d%m%Y')\n",
    "        ads_error_dict.setdefault(date, []).append(r)\n",
    "       \n",
    "# Merge DataFrames with the same dates\n",
    "merged_dfs = {}\n",
    "for date, review_dfs in ads_review_dict.items():\n",
    "    if date in ads_error_dict:\n",
    "        error_dfs = ads_error_dict[date]\n",
    "        merged_dfs[date] = pd.concat(review_dfs + error_dfs, ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to CSV files\n",
    "for date, df in merged_dfs.items():\n",
    "    try:\n",
    "        start_date = pd.to_datetime(date, format='%d%m%Y')\n",
    "        end_date = start_date + timedelta(days=6)\n",
    "        start_date_str = start_date.strftime('%d%m%Y')\n",
    "        end_date_str = end_date.strftime('%d%m%Y')\n",
    "        filename = f'AdsReview_RawData-{start_date_str}_{end_date_str}.csv'\n",
    "        df.to_csv(Ads_Review_destination_EMEA + \"\\\\\" + filename, index=False)\n",
    "        df.to_csv(ADS_Review_onedrive + \"\\\\\" + filename, index=False)\n",
    "        \n",
    "        log_df = log_df.append({'Filename': filename, 'Date': date, 'Status': 'Success', 'Message': 'File saved successfully'}, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        log_df = log_df.append({'Filename': filename, 'Date': date, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n",
    "\n",
    "# Display the log DataFrame\n",
    "print(log_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f83f54",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e4df1",
   "metadata": {},
   "source": [
    "## Labelling Project Details - Single Mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26412c58",
   "metadata": {},
   "source": [
    "link:https://datapower-va.bytelemon.com/bi/visit/6982209149375414277?immersive=1\n",
    "\n",
    "- Go to \"[Ops] Project Details\" tab of the following dashboard: Select the tab __Single Moderation__\t\t\t\t\n",
    "\n",
    "- 1 file per month: Before day 15 extract previous month and current one.\n",
    "\n",
    "\n",
    "- Filters:(select the following filters - which is also shown on the print attached)\n",
    "    - __Top Filter__\n",
    "        - __1 st Round resolve:__ >> Date Fixed ( select the full month that you need to extract)\n",
    "        - __Project ID :__ unselect \" Sellect all \"\n",
    "    - __Table__\n",
    "        - __Date scale:__ resolve_date\t\t\t\t\n",
    "        - __Dynamic dimension:__ (remove \"product_meego\")\tClick on Empty\t\t\t\n",
    "        - __Moderator info:__ Moderator (now with 审核员 )\t\t\t\t\n",
    "        - __\"The second\"-Dynamic dimension:__ department\t\t\t\t\n",
    "        - __Dynamic dimension:__ mode_name\t\t\t\t\n",
    "        - __Dynamic dimension:__ labeling_method\t\n",
    "        \n",
    "Download using the three dots on the right side\n",
    "\n",
    "    CSV UTF - 8\n",
    "    Include a 0 on number of rows\n",
    "                \n",
    "!!AFTER DOWNLOAD RUN THE BELOW CODE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c69618d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\u001b[1;31mMi Labeling\u001b[0m', the most recent file is: '\u001b[1;31m_Labelling Project Details - Single Mod - 01092024_30092024.csv\u001b[0m'\n"
     ]
    }
   ],
   "source": [
    "mi_folder = {'Mi Labeling': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling\\Single Moderation\"}\n",
    "\n",
    "def format_text(text, color_code, bold=False):\n",
    "    return f\"\\033[1;{color_code}m{text}\\033[0m\" if bold else f\"\\033[{color_code}m{text}\\033[0m\"\n",
    "\n",
    "for short_name, folder_path in mi_folder.items():\n",
    "    if os.path.isdir(folder_path) and (files := os.listdir(folder_path)):\n",
    "        date_pattern = re.compile(r'_(\\d{8})_(\\d{8})')\n",
    "\n",
    "        max_date_file = max(\n",
    "            (file for file in files if (date_matches := date_pattern.search(file)) or (date_matches := re.search(r'(\\d{8})_(\\d{8})', file))),\n",
    "            key=lambda file: datetime.strptime(date_matches.groups()[1], '%d%m%Y') if date_matches else datetime.min,\n",
    "            default=None)\n",
    "\n",
    "        if max_date_file:\n",
    "            max_date_format = format_text(max_date_file, 31, bold=True)\n",
    "            short_name_format = format_text(short_name, 31, bold=True)\n",
    "            print(f\"'{short_name_format}', the most recent file is: '{max_date_format}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4caefe",
   "metadata": {},
   "source": [
    "# THIS CODE WILL READ THE LAST FILE ON THE mi LABELING FOLDER\n",
    "mi_folder={'Mi Labeling': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling\\Single Moderation\"}\n",
    "def format_text(text, color_code, bold=False):\n",
    "    return f\"\\033[1;{color_code}m{text}\\033[0m\" if bold else f\"\\033[{color_code}m{text}\\033[0m\"\n",
    "\n",
    "for short_name, folder_path in mi_folder.items():\n",
    "    if os.path.isdir(folder_path) and (files := os.listdir(folder_path)):\n",
    "        date_pattern = re.compile(r'_(\\d{8})_(\\d{8})')\n",
    "\n",
    "        max_date_file = max(\n",
    "            (file for file in files if (date_matches := date_pattern.search(file)) or (date_matches := re.search(r'(\\d{8})_(\\d{8})', file))),\n",
    "            key=lambda file: datetime.strptime(date_matches.groups()[1], '%d%m%Y') if date_matches else datetime.min,\n",
    "            default=None)\n",
    "\n",
    "        if max_date_file:\n",
    "            max_date_format = format_text(max_date_file, 31, bold=True)\n",
    "            short_name_format = format_text(short_name, 31, bold=True)\n",
    "            print(f\"'{short_name_format}', the most recent file is: '{max_date_format}'\")\n",
    "\n",
    "# THIS CODE WILL DO ALL THE TRANFORMATION TO THE LABELING DATA\n",
    "\n",
    "#WHERE TO SEND:\n",
    "Labelling_project_destination_datadrops = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\MI_Labeling\"\n",
    "Labelling_project_destination_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling\\Single Moderation\"\n",
    "\n",
    "#Sharepoitn\n",
    "Mi_Labeling_onedrive =fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Single Moderation\"\n",
    "\n",
    "\n",
    "#WILL SHOW THE NAME OF THE FILE SO YOU CAN HAVE SURE WE ARE TAKING THE RIGHT FILE:\n",
    "Labelling_project_files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                         filename.startswith(\"_Labelling Project\")]\n",
    "print(Labelling_project_files)\n",
    "\n",
    "\n",
    "for w in Labelling_project_files:\n",
    " try:\n",
    "    x = pd.read_csv(os.path.join(downloads_folder_path, w))\n",
    "    x = x.rename(columns={\"Moderator.1\": \"Moderator\"})\n",
    "\n",
    "\n",
    "     # Validação das colunas\n",
    "    required_columns = ['resolve_date', 'department', 'mode_name', 'labeling_method']\n",
    "    forbidden_columns = ['product_meego', 'department.1']\n",
    "\n",
    "    # Verificar se todas as colunas necessárias estão presentes\n",
    "    if not all(col in x.columns for col in required_columns):\n",
    "        raise Exception(\"PS: The Validation Failed, Something is wrong - Required columns missing\")\n",
    "\n",
    "    # Verificar se as colunas proibidas estão presentes\n",
    "    if any(col in x.columns for col in forbidden_columns):\n",
    "        raise Exception(\"PS: The Validation Failed, Something is wrong - Forbidden columns present\")\n",
    "\n",
    "    \n",
    "    start_date = pd.to_datetime(x.iloc[1:]['resolve_date'].min())\n",
    "    start_date_year = start_date.year\n",
    "    start_date_month = start_date.month\n",
    "    \n",
    "    final_day = calendar.monthrange(start_date_year, start_date_month)[1]\n",
    "    len_start_date_month = str(start_date_month)\n",
    "    if len(len_start_date_month) < 2:\n",
    "        len_start_date_month = '0' + len_start_date_month    \n",
    "\n",
    "    start_date_str = start_date.strftime('%d%m%Y')\n",
    "    end_date_str = \"%s%s%s\" % (final_day, len_start_date_month, start_date_year)\n",
    "\n",
    "    if len(start_date_str) < 8:\n",
    "        start_date_str = '0' + start_date_str\n",
    "    \n",
    "    # Criação do nome do arquivo:\n",
    "    new_filename = '_Labelling Project Details - Single Mod - %s_%s.csv' % (start_date_str, end_date_str)\n",
    "     # Salvando o arquivo processado nas pastas especificadas\n",
    "    datadrops_file_path = os.path.join(Labelling_project_destination_datadrops, new_filename)\n",
    "    emea_file_path = os.path.join(Labelling_project_destination_EMEA, new_filename)\n",
    "    ondrive_file_path = os.path.join(Mi_Labeling_onedrive, new_filename)\n",
    "\n",
    "    x.to_csv(datadrops_file_path, index=False, encoding='utf_8_sig')\n",
    "    x.to_csv(emea_file_path, index=False, encoding='utf_8_sig')\n",
    "    x.to_csv(ondrive_file_path, index=False, encoding='utf_8_sig')\n",
    "\n",
    "\n",
    "    # Atualizar o DataFrame de log em caso de sucesso\n",
    "    log_df = log_df.append({'Filename': w, 'Date': start_date_str, 'Status': 'Success', 'Message': 'File saved successfully'}, ignore_index=True)\n",
    " except Exception as e:\n",
    "        # Atualizar o DataFrame de log em caso de erro\n",
    "    log_df = log_df.append({'Filename': w, 'Date': start_date_str, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273903a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_Labelling Project Details - Single Mod - 2024-09-10 08-10-16.csv', '_Labelling Project Details - Single Mod - 2024-09-10 08-10-54.csv']\n"
     ]
    }
   ],
   "source": [
    "# THIS CODE WILL DO ALL THE TRANSFORMATION TO THE LABELING DATA\n",
    "\n",
    "# WHERE TO SEND:\n",
    "Labelling_project_destination_datadrops = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\MI_Labeling\"\n",
    "Labelling_project_destination_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling\\Single Moderation\"\n",
    "\n",
    "# Sharepoint\n",
    "Mi_Labeling_onedrive = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Single Moderation\"\n",
    "\n",
    "# WILL SHOW THE NAME OF THE FILE SO YOU CAN HAVE SURE WE ARE TAKING THE RIGHT FILE:\n",
    "Labelling_project_files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                         filename.startswith(\"_Labelling Project\")]\n",
    "print(Labelling_project_files)\n",
    "\n",
    "for w in Labelling_project_files:\n",
    "    try:\n",
    "        x = pd.read_csv(os.path.join(downloads_folder_path, w))\n",
    "        x = x.rename(columns={\"Moderator.1\": \"Moderator\"})\n",
    "\n",
    "        # Validação das colunas\n",
    "        required_columns = ['resolve_date', 'department', 'mode_name', 'labeling_method']\n",
    "        forbidden_columns = ['product_meego', 'department.1']\n",
    "\n",
    "        # Verificar se todas as colunas necessárias estão presentes\n",
    "        if not all(col in x.columns for col in required_columns):\n",
    "            raise Exception(\"PS: The Validation Failed, Something is wrong - Required columns missing\")\n",
    "\n",
    "        # Verificar se as colunas proibidas estão presentes\n",
    "        if any(col in x.columns for col in forbidden_columns):\n",
    "            raise Exception(\"PS: The Validation Failed, Something is wrong - Forbidden columns present\")\n",
    "\n",
    "        # Limpar espaços em branco nas células da coluna \"department\"\n",
    "        x['department'] = x['department'].str.strip()\n",
    "\n",
    "        # Validação do conteúdo da coluna \"department\"\n",
    "        invalid_departments = x.loc[~x['department'].isin(['LIS-TP', '']), 'department'].unique()\n",
    "        if len(invalid_departments) > 0:\n",
    "            raise Exception(f\"PS: The Validation Failed, Something is wrong - Invalid department values found: {invalid_departments}\")\n",
    "\n",
    "        start_date = pd.to_datetime(x.iloc[1:]['resolve_date'].min())\n",
    "        start_date_year = start_date.year\n",
    "        start_date_month = start_date.month\n",
    "\n",
    "        final_day = calendar.monthrange(start_date_year, start_date_month)[1]\n",
    "        len_start_date_month = str(start_date_month)\n",
    "        if len(len_start_date_month) < 2:\n",
    "            len_start_date_month = '0' + len_start_date_month    \n",
    "\n",
    "        start_date_str = start_date.strftime('%d%m%Y')\n",
    "        end_date_str = \"%s%s%s\" % (final_day, len_start_date_month, start_date_year)\n",
    "\n",
    "        if len(start_date_str) < 8:\n",
    "            start_date_str = '0' + start_date_str\n",
    "\n",
    "        # Criação do nome do arquivo:\n",
    "        new_filename = '_Labelling Project Details - Single Mod - %s_%s.csv' % (start_date_str, end_date_str)\n",
    "        \n",
    "        # Salvando o arquivo processado nas pastas especificadas\n",
    "        datadrops_file_path = os.path.join(Labelling_project_destination_datadrops, new_filename)\n",
    "        emea_file_path = os.path.join(Labelling_project_destination_EMEA, new_filename)\n",
    "        ondrive_file_path = os.path.join(Mi_Labeling_onedrive, new_filename)\n",
    "\n",
    "        x.to_csv(datadrops_file_path, index=False, encoding='utf_8_sig')\n",
    "        x.to_csv(emea_file_path, index=False, encoding='utf_8_sig')\n",
    "        x.to_csv(ondrive_file_path, index=False, encoding='utf_8_sig')\n",
    "\n",
    "        # Atualizar o DataFrame de log em caso de sucesso\n",
    "        log_df = log_df.append({'Filename': w, 'Date': start_date_str, 'Status': 'Success', 'Message': 'File saved successfully'}, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        # Atualizar o DataFrame de log em caso de erro\n",
    "        log_df = log_df.append({'Filename': w, 'Date': start_date_str, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f3a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=datetime.today().strftime('%d-%m-%Y %H-%M-%S')\n",
    "Log_file=log_df.to_excel(fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\Logs_etl_part3_{date}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adf3e5d",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b7313",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3b38e",
   "metadata": {},
   "source": [
    "## MI QA Appeals( MI ACCOUNT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6d479",
   "metadata": {},
   "source": [
    "link: https://bytedance.sg.larkoffice.com/base/GHoXboG1YaWmigsDvKLlO8KzgFe?table=tblPELwHGA3cbJe5&view=veweOl34mL\n",
    "\n",
    "__IMP:__  Use MI Account\n",
    "\n",
    "__Tab:__ TAXONOMY: BS SR Qs performance\n",
    "\n",
    "Simply click on the three dots, Export, CSV, Should say SR Lis, than click download and run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b0db28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File moved successfully!\n"
     ]
    }
   ],
   "source": [
    "# New functionality to copy the file\n",
    "filename = \"TP-LIS - RCA - 2nd Round QA_BS SR Qs Performance_SR LIS.csv\"\n",
    "source_path = os.path.join(downloads_folder_path, filename)\n",
    "destination_folder = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling QAs Appeals\"\n",
    "destination_path = os.path.join(destination_folder, filename)\n",
    "#Sharepoitn\n",
    "Labeling_QAS_Appeals_onedrive = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Labeling QAs Appeals\"\n",
    "\n",
    "\n",
    "# Ensure the destination folder exists\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# Copy the file and print success message\n",
    "try:\n",
    "    shutil.copy(source_path, destination_path)\n",
    "    shutil.copy(source_path, Labeling_QAS_Appeals_onedrive)\n",
    "    \n",
    "    print('File moved successfully!')\n",
    "except FileNotFoundError:\n",
    "    print(f'Error: File {filename} not found in {downloads_folder_path}')\n",
    "except PermissionError:\n",
    "    print('Error: Permission denied. Could not move the file.')\n",
    "except Exception as e:\n",
    "    print(f'Error: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08b5aa",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d04f5",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f526472",
   "metadata": {},
   "source": [
    "## MI- ADS QA MODE ( MI ACCOUNT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8eff8b",
   "metadata": {},
   "source": [
    "link: https://datapower-va.bytelemon.com/bi/visit/6953507931698642950?immersive=1\n",
    "\n",
    "__IMP:__ You need to login to Datapower with the MI account, you need to logout to your main account and login again and selec the MI account.\n",
    "\n",
    "__Tab:__  Dashboards __>__ All Dashboardsm __>__ Site Moderation Report __>__ QA from new dataset __>__Raw Data Export\n",
    "\n",
    "__Table :__ QA Mode - RAW data - task level\n",
    "\n",
    "__Filter:__ \n",
    "\n",
    "1: QA1 Date : Extract 2 files ( Current month + previous )\n",
    "\n",
    "!!AFTER DOWNLOAD RUN THE BELOW CODES!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0a8acfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\u001b[1;31mMi ADS\u001b[0m', the most recent file is: '\u001b[1;31m_QA Mode - RAW data - task level_20240901_20240930.csv\u001b[0m'\n"
     ]
    }
   ],
   "source": [
    "#THIS CODE WILL READ THE LAST MI ADS QA MODE DATA FROM THE EMEA FOLDER\n",
    "mi_ads_folder={'Mi ADS':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Ads_Quality_QA_Mode\"}\n",
    "\n",
    "\n",
    "def format_text(text, color_code, bold=False):\n",
    "    return f\"\\033[1;{color_code}m{text}\\033[0m\" if bold else f\"\\033[{color_code}m{text}\\033[0m\"\n",
    "\n",
    "for short_name, folder_path in mi_ads_folder.items():\n",
    "    if os.path.isdir(folder_path) and (files := os.listdir(folder_path)):\n",
    "        date_pattern = re.compile(r'_(\\d{8})_(\\d{8})')\n",
    "\n",
    "        max_date_file = max(\n",
    "            (file for file in files if (date_matches := date_pattern.search(file)) or (date_matches := re.search(r'(\\d{8})_(\\d{8})', file))),\n",
    "            key=lambda file: datetime.strptime(date_matches.groups()[1], '%Y%m%d') if date_matches else datetime.min,\n",
    "            default=None)\n",
    "\n",
    "        if max_date_file:\n",
    "            max_date_format = format_text(max_date_file, 31, bold=True)\n",
    "            short_name_format = format_text(short_name, 31, bold=True)\n",
    "            print(f\"'{short_name_format}', the most recent file is: '{max_date_format}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "819991ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mAll good\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#THIS CODE WILL VERIFY THE STRUTURE OF THE FILE AND VALIDATE IF THE ALL NEEDED COLUMNS ARE PRESENT ON THE DATA\n",
    "\n",
    "#Folders destination:\n",
    "MI_Ads_QA1_destination_EMEA =r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Ads_Quality_QA_Mode\"\n",
    "\n",
    "#Sharepoitn\n",
    "Mi_QA_Mode_onedrive = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Ads_Quality_QA_Mode\"\n",
    "\n",
    "#Location the file name base on a conditon:\n",
    "MI_Ads_QA1_Files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                    filename.startswith(\"_QA Mode - RAW data - task level -\")]\n",
    "MI_Ads_QA1_Files\n",
    "\n",
    "expected_columns = ['QA1 Date', 'QA1 Date.1', 'qa1_inspect_id', 'moderator_name',\n",
    "       'moderator_email', 'moderator_site', 'qa1_name', 'qa1_email',\n",
    "       'qa1_site', 'task_mode', 'business_line', 'task_name',\n",
    "       'moderation_stage', 'qa_country', 'QA1 task Link',\n",
    "       'merged_task_inspect_status_pre_appeal', 'QA1 RCA Link',\n",
    "       'qa1_appeal_result_task_level', 'QA1 Auditor Appeal Reason',\n",
    "       'QA1 Appeal Response', 'QA1 Task Level RCA', 'qa1_rca_rca_result',\n",
    "       'qa1_rca_rca_error', 'qa1_rca_rca_desc', 'qa1_result_type',\n",
    "       'merged_task_inspect_status_post_appeal', 'row_reason_code_name',\n",
    "       'QA2 Date', 'qa2_inspect_id', 'qa2_inspect_status_en',\n",
    "       'qa2_rca_type_task_level', 'qa2_rca_rca_result', 'qa2_rca_rca_desc',\n",
    "       'qa2_task_link', 'qa2_appeal_rca_task_link', 'qa2_name', 'qa2_email',\n",
    "       'qa2_site', 'qa2_appeal_appeal_result',\n",
    "       'qa2_appeal_original_appeal_reason'] \n",
    "\n",
    "validation_results = {}\n",
    "def validate_csv_columns(csv_files, expected_columns):\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(csv_file)\n",
    "\n",
    "            # Get the column names from the CSV file\n",
    "            actual_columns = df.columns.tolist()\n",
    "\n",
    "            # Check if all expected columns are present in the CSV file\n",
    "            if set(expected_columns) == set(actual_columns):\n",
    "                validation_results[csv_file] = True\n",
    "            else:\n",
    "                validation_results[csv_file] = False\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: CSV file '{csv_file}' not found.\")\n",
    "            validation_results[csv_file] = False\n",
    "    return validation_results\n",
    "\n",
    "# Check if all files passed the validation\n",
    "if all(validation_results.values()):\n",
    "    # Print \"all good\" in bold and green\n",
    "    print(\"\\033[1;32mAll good\\033[0m\")  # ANSI escape codes for bold (1) and green color (32)\n",
    "else:\n",
    "    # Print the validation results\n",
    "    print(\"Validation results:\")\n",
    "    for csv_file, is_valid in validation_results.items():\n",
    "        print(f\"{csv_file}: {is_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fc0ccb",
   "metadata": {},
   "source": [
    "__If above is ALL GOOD run below cell, if error do not run the below cell. Ask MI DA about the error, to find to correct fix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "168b2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For loop to read all files:\n",
    "for i in MI_Ads_QA1_Files:\n",
    " try:\n",
    "    #Read the files on the download folder:\n",
    "    j = pd.read_csv(downloads_folder_path + '\\\\'+ i)\n",
    "    j= j.rename(columns ={\"QA1 Date.1\":\"QA1 Date\"})\n",
    "    #Date conditon to use on the filename:\n",
    "    start_date = pd.to_datetime(j.iloc[:, 1].min())\n",
    "    start_date_year = start_date.year\n",
    "    start_date_month = start_date.month\n",
    "    \n",
    "    final_day = calendar.monthrange(start_date_year, start_date_month)[1]\n",
    "    len_start_date_month = str(start_date_month)\n",
    "    if len(len_start_date_month) < 2:\n",
    "        len_start_date_month = '0'+ len_start_date_month    \n",
    "\n",
    "    start_date = start_date.strftime('%Y%m%d')\n",
    "    end_date3 = \"%s%s%s\" %(start_date_year, len_start_date_month, final_day)\n",
    "\n",
    "    if len(start_date) < 8:\n",
    "        start_date = '0'+ start_date     \n",
    "    #Name creation base on a string +date above created\n",
    "    k = '_QA Mode - RAW data - task level_%s_%s.csv' %(start_date, end_date3) \n",
    "    #Move the file with the new name:\n",
    "    j.to_csv(MI_Ads_QA1_destination_EMEA + \"\\\\\" + k, index = False)\n",
    "    j.to_csv(Mi_QA_Mode_onedrive + \"\\\\\" + k, index = False)\n",
    "\n",
    "     # Atualizar o DataFrame de log em caso de sucesso\n",
    "    log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File saved successfully'}, ignore_index=True)\n",
    " except Exception as e:\n",
    "        # Atualizar o DataFrame de log em caso de erro\n",
    "    log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n",
    "\n",
    " \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb705ea",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d838f20",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca5a7c",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b59b4",
   "metadata": {},
   "source": [
    "# MI_Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08313fca",
   "metadata": {},
   "source": [
    "link: https://datapower-va.bytelemon.com/bi/visit/6953507931698642950\n",
    "\n",
    "__TAB:__  Produtivity\n",
    "\n",
    "__TAB:__  Raw DAta\n",
    "\n",
    "__Tabela:__Statuses Raw Data\n",
    "\n",
    "\n",
    "__Data time:__  3 files, current week + the last 2 weeks ( files de Monday to Sunday)\n",
    "\n",
    "!!AFTER DOWNLOAD RUN THE BELOW CODE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c41fedbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\u001b[1;31mMi Statuses\u001b[0m', the most recent file is: '\u001b[1;31mStatuses Raw Data_02092024_08092024.csv\u001b[0m'\n"
     ]
    }
   ],
   "source": [
    "#THIS CODE WILL VALIDATE THE LAST MI STATUS FILE FROM THE FOLDER:\n",
    "\n",
    "mi_Statuses={'Mi Statuses':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Ads_status\"}\n",
    "def format_text(text, color_code, bold=False):\n",
    "    return f\"\\033[1;{color_code}m{text}\\033[0m\" if bold else f\"\\033[{color_code}m{text}\\033[0m\"\n",
    "\n",
    "for short_name, folder_path in mi_Statuses.items():\n",
    "    if os.path.isdir(folder_path) and (files := os.listdir(folder_path)):\n",
    "        date_pattern = re.compile(r'_(\\d{8})_(\\d{8})')\n",
    "\n",
    "        max_date_file = max(\n",
    "            (file for file in files if (date_matches := date_pattern.search(file)) or (date_matches := re.search(r'(\\d{8})_(\\d{8})', file))),\n",
    "            key=lambda file: datetime.strptime(date_matches.groups()[1], '%d%m%Y') if date_matches else datetime.min,\n",
    "            default=None)\n",
    "\n",
    "        if max_date_file:\n",
    "            max_date_format = format_text(max_date_file, 31, bold=True)\n",
    "            short_name_format = format_text(short_name, 31, bold=True)\n",
    "            print(f\"'{short_name_format}', the most recent file is: '{max_date_format}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e916a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Statuses Raw Data - 2024-09-10 08-14-40.csv',\n",
       " '_Statuses Raw Data - 2024-09-10 08-14-57.csv',\n",
       " '_Statuses Raw Data - 2024-09-10 08-15-13.csv']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code explain on MI- ADS QA1 code:\n",
    "Statuses_destination_datadrops = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\MI_Ads_status\"\n",
    "Statuses_destination_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Ads_status\"\n",
    "\n",
    "#Sharepoitn\n",
    "Mi_Ads_status_onedrive = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Ads_status\"\n",
    "\n",
    "Statuses_files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                  filename.startswith(\"_Statuses Raw\")]\n",
    "Statuses_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8704c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE BELOW CODE WILL TRANSFOR ALL THE FILE OF MI STATUS AND SENT THEM TO THE FOLDER:\n",
    "\n",
    "for jj in Statuses_files:\n",
    " try:\n",
    "    u = pd.read_csv(downloads_folder_path + '\\\\'+ jj)\n",
    "    u.drop(['language_skill_main'],axis=1, inplace=True)\n",
    "    u = u.rename(columns ={\"mod_role_extract\":\"mod_role\",\"work_duty_extract\":\"work_duty\"})\n",
    " \n",
    "    start_date = pd.to_datetime(u['duty_date'].min())\n",
    "    end_date = start_date + timedelta(days = 6)\n",
    "\n",
    "    start_date = start_date.strftime('%d%m%Y')\n",
    "    end_date = end_date.strftime('%d%m%Y')    \n",
    "    \n",
    "    if len(start_date) < 8:\n",
    "        start_date = '0'+ start_date\n",
    "    if len(end_date) < 8:\n",
    "        end_date = '0'+ end_date        \n",
    "       \n",
    "    ff = 'Statuses Raw Data_%s_%s.csv' %(start_date, end_date)    \n",
    "    \n",
    "    u.to_csv(Statuses_destination_datadrops + '\\\\'+ ff, index = False)\n",
    "    u.to_csv(Statuses_destination_EMEA + '\\\\'+ ff, index = False)\n",
    "    u.to_csv(Mi_Ads_status_onedrive + '\\\\'+ ff, index = False)\n",
    "\n",
    "    \n",
    " # Atualizar o DataFrame de log em caso de sucesso\n",
    "    log_df = log_df.append({'Filename': jj, 'Date': start_date, 'Status': 'Success', 'Message': 'File saved successfully'}, ignore_index=True)\n",
    " except Exception as e:\n",
    "        # Atualizar o DataFrame de log em caso de erro\n",
    "    log_df = log_df.append({'Filename': jj, 'Date': start_date, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d344db91",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861acaf",
   "metadata": {},
   "source": [
    "# MI_Produtivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f7194",
   "metadata": {},
   "source": [
    "link: https://datapower-va.bytelemon.com/bi/visit/6953507931698642950?immersive=1\n",
    "\n",
    "__TAB:__ Produtivity TAB /// Raw Data /// \n",
    "\n",
    "__Table:__ Productivity Raw Data // 2 files- Current +last week ( files de Monday to Sunday)\n",
    "\n",
    "\n",
    "!!AFTER DOWNLOAD RUN THE BELOW CODE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "962704db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\u001b[1;31mMi Produtivity\u001b[0m', the most recent file is: '\u001b[1;31mProductivity Raw Data_20240902_20240908.csv\u001b[0m'\n"
     ]
    }
   ],
   "source": [
    "#THIS CODE WILL CHECK THE LAST FILE PRESENT ON THE EMEA FOLDER:\n",
    "\n",
    "mi_Productivity={'Mi Produtivity':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Ads_productivity_v2\"}\n",
    "def format_text(text, color_code, bold=False):\n",
    "    return f\"\\033[1;{color_code}m{text}\\033[0m\" if bold else f\"\\033[{color_code}m{text}\\033[0m\"\n",
    "\n",
    "for short_name, folder_path in mi_Productivity.items():\n",
    "    if os.path.isdir(folder_path) and (files := os.listdir(folder_path)):\n",
    "        date_pattern = re.compile(r'_(\\d{8})_(\\d{8})')\n",
    "\n",
    "        max_date_file = max(\n",
    "            (file for file in files if (date_matches := date_pattern.search(file)) or (date_matches := re.search(r'(\\d{8})_(\\d{8})', file))),\n",
    "            key=lambda file: datetime.strptime(date_matches.groups()[1], '%Y%m%d') if date_matches else datetime.min,\n",
    "            default=None)\n",
    "\n",
    "        if max_date_file:\n",
    "            max_date_format = format_text(max_date_file, 31, bold=True)\n",
    "            short_name_format = format_text(short_name, 31, bold=True)\n",
    "            print(f\"'{short_name_format}', the most recent file is: '{max_date_format}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45b440fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Productivity Raw Data - 2024-09-10 08-16-08.csv',\n",
       " '_Productivity Raw Data - 2024-09-10 08-17-21.csv',\n",
       " '_Productivity Raw Data - 2024-09-10 08-17-39.csv']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code explain on MI- ADS QA1 code:\n",
    "Productivity_destination_EMEA = r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Ads_productivity_v2'\n",
    "\n",
    "#Sharepoitn\n",
    "Ads_productivity_onedrive = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Ads_productivity_v2\"\n",
    "\n",
    "Productivity_files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                  filename.startswith(\"_Productivity Raw Data\")]\n",
    "Productivity_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81c7fc9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#THE BELOW CODE WILL TRANSFOR ALL THE FILE OF MI PRODUTIVITY AND SENT THEM TO THE FOLDER:\n",
    "\n",
    "for c in Productivity_files:\n",
    " try:\n",
    "                      \n",
    "    df = pd.read_csv(downloads_folder_path + '\\\\'+ c)[['p_date','auditor_staff_id','auditor_staff_name','mod_role_extract',\n",
    "                                                      'delivery_country','task_type_en','queue_type','queue_name','platform_source',\n",
    "                                                       'handling_time_min']]\n",
    "    \n",
    "    df = df.rename(columns ={\"mod_role_extract\":\"mod_role\",\"queue_type\":\"Queue Type\",\"queue_name\":\"queue_name_en\"})\n",
    "    \n",
    " \n",
    "    df['task_count']=1\n",
    "    #Dates for the file name\n",
    "    start_date = pd.to_datetime(df['p_date'].min())\n",
    "    end_date = start_date + timedelta(days = 6)\n",
    "    start_date = start_date.strftime('%Y%m%d')\n",
    "    end_date = end_date.strftime('%Y%m%d')\n",
    "    if len(start_date) < 8:\n",
    "        start_date = '0'+ start_date\n",
    "    if len(end_date) < 8:\n",
    "        end_date = '0'+ end_date \n",
    "    #Groupby:\n",
    "    df =df.groupby(['p_date','auditor_staff_id','auditor_staff_name','mod_role',\n",
    "                                                      'delivery_country','task_type_en','Queue Type','queue_name_en','platform_source'],dropna=False).agg({\n",
    "    'task_count':'sum',\n",
    "    'handling_time_min':'sum'}).reset_index()\n",
    "    dd = 'Productivity Raw Data_%s_%s.csv' %(start_date, end_date)    \n",
    "    df.to_csv(Productivity_destination_EMEA + '\\\\'+ dd, index = False)\n",
    "    df.to_csv(Ads_productivity_onedrive + '\\\\'+ dd, index = False)\n",
    "\n",
    "    \n",
    "\n",
    "     # Atualizar o DataFrame de log em caso de sucesso\n",
    "    log_df = log_df.append({'Filename': c, 'Date': start_date, 'Status': 'Success', 'Message': 'File saved successfully'}, ignore_index=True)\n",
    " except Exception as e:\n",
    "        # Atualizar o DataFrame de log em caso de erro\n",
    "    log_df = log_df.append({'Filename': c, 'Date': start_date, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8904e517",
   "metadata": {},
   "source": [
    "# WELLNESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc72f44",
   "metadata": {},
   "source": [
    " Download the file from the below link and open the file and Refresh all, and run the code: \n",
    "\n",
    "\n",
    "__File Link (2024):__  https://teleperformance-my.sharepoint.com/:x:/p/dijkstra_28_emea/ETb7JZ2sQ7RCsjFK6x6S7U4BVYn0WuL_eNkpaHbdnJ1VEw?email=pedro.esteves%40teleperformance.com&e=4%3AGTDzi5&fromShare=true&at=9&CID=93410ac1-2838-1df0-cac8-9b2ccfaedfb8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8987269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File location:\n",
    "#MANUAL WAY:\n",
    "caminho_origem1 = fr\"C:\\Users\\\\{username}\\\\{folder}\\Wellness Data 2024.xlsx\"\n",
    "caminho_origem1\n",
    "\n",
    "# # location of 3 folders\n",
    "Folder1 = r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\18. Wellness'\n",
    "Folder2 = r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\Wellness_Group'\n",
    "Folder3 = r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\Wellness_Individual'\n",
    "Wellness_Onedrive= fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\18. Wellness\"\n",
    "\n",
    "try:\n",
    "    # Move the files to the folder\n",
    "    shutil.copy(caminho_origem1, Folder1)\n",
    "    log_df = log_df.append({'Filename': caminho_origem1, 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "    shutil.copy(caminho_origem1, Folder2)\n",
    "    log_df = log_df.append({'Filename': caminho_origem1, 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "    shutil.copy(caminho_origem1, Folder3)\n",
    "    log_df = log_df.append({'Filename': caminho_origem1, 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "    shutil.copy(caminho_origem1, Wellness_Onedrive)\n",
    "    log_df = log_df.append({'Filename': caminho_origem1, 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "\n",
    "except Exception as e:\n",
    "    log_df = log_df.append({'Filename': 'N/A', 'Date': date_today, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2166a22",
   "metadata": {},
   "source": [
    "-------------------------------END of this code Group--------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12674f3e",
   "metadata": {},
   "source": [
    "## MMP Data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60ae55",
   "metadata": {},
   "source": [
    "__IMP:__ THIS NEEDS TO BE DONE WITH MI ACCOUNT AS WELL, THE SAME DAYS\n",
    "\n",
    "__Link:__  https://byteworks-va.bytelemon.com/v2/workhour/correct\n",
    "\n",
    "__Change Time Zone for Portugal, if Winter: \"UTC\", if Summers: \"UTC +1\" (+01:00 Paris now)__\t\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "__- Filter:__ By time\n",
    "\n",
    "__- Time range:__ Fixe Date - Day by Day ( Extract the current week and the previous week of data ), from 00:00 to 23:59\t\t\t\t\n",
    "\n",
    "\n",
    "__- Data aggregation method:__ By Employee\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "\n",
    "__- Employee Status:__ Employee Status: Leave it empty, not selected, as the dashboard will pull the data with on and offbard data together.\n",
    "\n",
    "__- Department FOR NOT MI ACCOUNT:__\tTP_LIS \t/ TP_ADSO\t\n",
    "\n",
    "__- Department FOR MI ACCOUNT:__ MI_TP_LIS\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\t\t\n",
    "__1- Select \"Export Statistics\" and download from \\ 2-\"My export\" when the status is \"completed\"__\t\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "__- NOTE:  We need to verify the size of the file, sometimes the file will download empty and without any data or with some data only.__\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9ad0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Destination folder:\n",
    "MMP_destination_datadrops = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\Aux_Codes\"\n",
    "MMP_destination_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\19. MMP Data Calibration - Aux Codes\"\n",
    "MMP_destination_desktop = r\"C:\\Users\\bechara.9\\Downloads\"\n",
    "Onedrive_destination= fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\19. MMP Data Calibration - Aux Codes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "295f242a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data-1725953307_1725953309755.csv',\n",
       " 'data-1725953312_1725953314833.csv',\n",
       " 'data-1725953318_1725953320830.csv',\n",
       " 'data-1725953324_1725953326878.csv',\n",
       " 'data-1725953330_1725953332667.csv',\n",
       " 'data-1725953334_1725953337000.csv',\n",
       " 'data-1725953338_1725953341370.csv',\n",
       " 'data-1725953343_1725953346597.csv',\n",
       " 'data-1725953349_1725953352101.csv',\n",
       " 'data-1725953476_1725953488205.csv',\n",
       " 'data-1725953484_1725953506203.csv',\n",
       " 'data-1725953490_1725953508230.csv',\n",
       " 'data-1725953497_1725953515600.csv',\n",
       " 'data-1725953503_1725953526100.csv',\n",
       " 'data-1725953512_1725953536855.csv',\n",
       " 'data-1725953521_1725953546295.csv',\n",
       " 'data-1725953536_1725953558880.csv',\n",
       " 'data-1725953543_1725953565431.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the files and list them:\n",
    "MMP_Data_files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                  filename.startswith(\"data-\")]\n",
    "MMP_Data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58436e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "                                             Filename        Date   Status  \\\n",
      "0             AdsReview_RawData-02092024_08092024.csv    02092024  Success   \n",
      "1             AdsReview_RawData-26082024_01092024.csv    26082024  Success   \n",
      "2   _Labelling Project Details - Single Mod - 2024...    01092024  Success   \n",
      "3   _Labelling Project Details - Single Mod - 2024...    01082024  Success   \n",
      "4   _QA Mode - RAW data - task level - 2024-09-10 ...    20240901  Success   \n",
      "5   _QA Mode - RAW data - task level - 2024-09-10 ...    20240801  Success   \n",
      "6        _Statuses Raw Data - 2024-09-10 08-14-40.csv    02092024  Success   \n",
      "7        _Statuses Raw Data - 2024-09-10 08-14-57.csv         NaN    Error   \n",
      "8        _Statuses Raw Data - 2024-09-10 08-15-13.csv    26082024  Success   \n",
      "9    _Productivity Raw Data - 2024-09-10 08-16-08.csv    20240902  Success   \n",
      "10   _Productivity Raw Data - 2024-09-10 08-17-21.csv         NaN    Error   \n",
      "11   _Productivity Raw Data - 2024-09-10 08-17-39.csv    20240826  Success   \n",
      "12  C:\\Users\\\\bechara.9\\\\Downloads\\Wellness Data 2...  10-09-2024  Success   \n",
      "13  C:\\Users\\\\bechara.9\\\\Downloads\\Wellness Data 2...  10-09-2024  Success   \n",
      "14  C:\\Users\\\\bechara.9\\\\Downloads\\Wellness Data 2...  10-09-2024  Success   \n",
      "15  C:\\Users\\\\bechara.9\\\\Downloads\\Wellness Data 2...  10-09-2024  Success   \n",
      "16                  data-1725953307_1725953309755.csv    10092024  Success   \n",
      "17                  data-1725953312_1725953314833.csv    09092024  Success   \n",
      "18                  data-1725953318_1725953320830.csv    08092024  Success   \n",
      "19                  data-1725953324_1725953326878.csv    07092024  Success   \n",
      "20                  data-1725953330_1725953332667.csv    06092024  Success   \n",
      "21                  data-1725953334_1725953337000.csv    05092024  Success   \n",
      "22                  data-1725953338_1725953341370.csv    04092024  Success   \n",
      "23                  data-1725953343_1725953346597.csv    03092024  Success   \n",
      "24                  data-1725953349_1725953352101.csv    02092024  Success   \n",
      "25                  data-1725953476_1725953488205.csv    10092024  Success   \n",
      "26                  data-1725953484_1725953506203.csv    09092024  Success   \n",
      "27                  data-1725953490_1725953508230.csv    08092024  Success   \n",
      "28                  data-1725953497_1725953515600.csv    07092024  Success   \n",
      "29                  data-1725953503_1725953526100.csv    06092024  Success   \n",
      "30                  data-1725953512_1725953536855.csv    05092024  Success   \n",
      "31                  data-1725953521_1725953546295.csv    04092024  Success   \n",
      "32                  data-1725953536_1725953558880.csv    03092024  Success   \n",
      "33                  data-1725953543_1725953565431.csv    02092024  Success   \n",
      "\n",
      "                                         Message  \n",
      "0                        File saved successfully  \n",
      "1                        File saved successfully  \n",
      "2                        File saved successfully  \n",
      "3                        File saved successfully  \n",
      "4                        File saved successfully  \n",
      "5                        File saved successfully  \n",
      "6                        File saved successfully  \n",
      "7              NaTType does not support strftime  \n",
      "8                        File saved successfully  \n",
      "9                        File saved successfully  \n",
      "10             NaTType does not support strftime  \n",
      "11                       File saved successfully  \n",
      "12                      File copied successfully  \n",
      "13                      File copied successfully  \n",
      "14                      File copied successfully  \n",
      "15                      File copied successfully  \n",
      "16  File copied successfully to all destinations  \n",
      "17  File copied successfully to all destinations  \n",
      "18  File copied successfully to all destinations  \n",
      "19  File copied successfully to all destinations  \n",
      "20  File copied successfully to all destinations  \n",
      "21  File copied successfully to all destinations  \n",
      "22  File copied successfully to all destinations  \n",
      "23  File copied successfully to all destinations  \n",
      "24  File copied successfully to all destinations  \n",
      "25  File copied successfully to all destinations  \n",
      "26  File copied successfully to all destinations  \n",
      "27  File copied successfully to all destinations  \n",
      "28  File copied successfully to all destinations  \n",
      "29  File copied successfully to all destinations  \n",
      "30  File copied successfully to all destinations  \n",
      "31  File copied successfully to all destinations  \n",
      "32  File copied successfully to all destinations  \n",
      "33  File copied successfully to all destinations  \n"
     ]
    }
   ],
   "source": [
    "# Process each file\n",
    "for i in MMP_Data_files:\n",
    "    try:\n",
    "        # Initialize flag\n",
    "        ff = 0\n",
    "\n",
    "        # Read the file and skip the first row\n",
    "        dd = pd.read_csv(os.path.join(downloads_folder_path, i), skiprows=1)\n",
    "\n",
    "        # Change the date to string\n",
    "        file_date = str(pd.to_datetime(dd['日期/Date']).dt.strftime('%d%m%Y').astype(int).min())\n",
    "        if len(file_date) < 8:\n",
    "            file_date = '0' + file_date\n",
    "\n",
    "        # Check if the department column contains any value starting with \"MI_\"\n",
    "        if dd['部门/Department'].str.startswith('MI_').any():\n",
    "            prefix = \"MI_\"\n",
    "        else:\n",
    "            prefix = \"\"\n",
    "\n",
    "        # Create the filename with or without the MI_ prefix\n",
    "        ff = prefix + 'MMP_Data_calibration_' + file_date + '.csv'\n",
    "\n",
    "        # Save the file to the destination folder on the desktop\n",
    "        new_file_path = os.path.join(MMP_destination_desktop, ff)\n",
    "        dd.to_csv(new_file_path, index=False, encoding='utf_8_sig')\n",
    "\n",
    "        # Copy the file to the other destinations\n",
    "        shutil.copy(new_file_path, MMP_destination_datadrops)\n",
    "        shutil.copy(new_file_path, MMP_destination_EMEA)\n",
    "        shutil.copy(new_file_path, Onedrive_destination)\n",
    "        \n",
    "\n",
    "        # Log success\n",
    "        log_df = log_df.append({'Filename': i, 'Date': file_date, 'Status': 'Success', 'Message': 'File copied successfully to all destinations'}, ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error\n",
    "        log_df = log_df.append({'Filename': i, 'Date': file_date, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n",
    "\n",
    "# Optional: Remove the files from your download folder\n",
    "#[os.remove(os.path.join(downloads_folder_path, filename)) for filename in os.listdir(downloads_folder_path) if filename.startswith(\"data-\")]\n",
    "\n",
    "# Display the log\n",
    "print(\"---------------------------------------------\")\n",
    "print(log_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45578a1a",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c110e68",
   "metadata": {},
   "source": [
    "# Coaching Survey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef3a48",
   "metadata": {},
   "source": [
    "\n",
    "Just download the file:\n",
    "\n",
    "link: https://teleperformance.larksuite.com/sheets/shtusIaUugKkg91iq4IA1YP2wgd\n",
    "\n",
    "Format:  EXCEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98f26c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File location:\n",
    "Coaching_Survey = fr\"C:\\Users\\\\{username}\\\\{folder}\\TikTok Internal Coaching Survey - Weekly(datarecord).xlsx\"\n",
    "#Destination:\n",
    "Coaching_Survey_EMEA =r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\34. CoachingSurvey_Raw\\CoachingSurvey_Raw.xlsx\"\n",
    "Coaching_Survey_DROPS=r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\Coaching_Survey\\CoachingSurvey_Raw.xlsx\"\n",
    "Coaching_destination= fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\34. CoachingSurvey_Raw\\CoachingSurvey_Raw.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Move the files to the EMEA folder\n",
    "    shutil.copy(Coaching_Survey, Coaching_Survey_EMEA)\n",
    "    log_df = log_df.append({'Filename': f'EMEA:{Coaching_Survey}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    " \n",
    "  \n",
    "    # Move the files to the DROPS folder\n",
    "    shutil.copy(Coaching_Survey, Coaching_Survey_DROPS)\n",
    "    log_df = log_df.append({'Filename': f'Drops:{Coaching_Survey}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    " \n",
    "\n",
    " # Move the files to the sharepoint folder\n",
    "    shutil.copy(Coaching_Survey, Coaching_destination)\n",
    "    log_df = log_df.append({'Filename': f'Drops:{Coaching_Survey}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    " \n",
    "except Exception as e:\n",
    "    log_df = log_df.append({'Filename': Coaching_Survey, 'Date': date_today, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896113e",
   "metadata": {},
   "source": [
    "-------------------------------END of this code Group--------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b5df2",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6d4887",
   "metadata": {},
   "source": [
    "\n",
    "# QA DATA ( ADSO : Ecolabeling Accuracy Individual) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c8e23",
   "metadata": {},
   "source": [
    "Go to the below link and go to the folders: ( ADSO), download the files  ( Ecolabeling Accuracy Individual ), if you need the file name, look on below paths location, there you have each file name.\n",
    "\n",
    "\n",
    "link:https://teleperformance.sharepoint.com/:f:/r/sites/TikTok212/Shared%20Documents/Data%20Analytics/1.%20Quality%20files%20for%20DA%20Team%20PBI%20reports?csf=1&web=1&e=Pe8zSi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f6b9df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RS_ADSO = fr\"C:\\Users\\\\{username}\\\\{folder}\\Ecolabeling Accuracy Individual.xlsx\"\n",
    "\n",
    "\n",
    "#EMEA\n",
    "\n",
    "RS_ADSO_EMEA =r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\ADSO ECO Random Sampling\"\n",
    "\n",
    "#Data Drops\n",
    "RS_ADSO_DROPS= r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\Random_sampling_adso_post_appeal\"\n",
    "\n",
    "#Sharepoitn\n",
    "RS_ADSO_onedrive = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\ADSO ECO Random Sampling\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905234b0",
   "metadata": {},
   "source": [
    "# Struture Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2863a09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sheet 'Sheet1' of file 'C:\\Users\\\\bechara.9\\\\Downloads\\Ecolabeling Accuracy Individual.xlsx' contains all required columns.\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "\n",
    "def verify_excel_files(files_info):\n",
    "    for file_info in files_info:\n",
    "        # Extract file path, sheet name, and required columns from file_info dictionary\n",
    "        file_path = file_info[\"file_path\"]\n",
    "        sheet_name = file_info[\"sheet_name\"]\n",
    "        required_columns = file_info[\"required_columns\"]\n",
    "        \n",
    "        try:\n",
    "            # Load workbook in read-only mode\n",
    "            workbook = openpyxl.load_workbook(file_path, read_only=True)\n",
    "            # Check if the specified sheet exists in the workbook\n",
    "            if sheet_name not in workbook.sheetnames:\n",
    "                print(f\"The sheet '{sheet_name}' is not present in the file '{file_path}'.\")\n",
    "                continue\n",
    "            # Access the specified sheet\n",
    "            sheet = workbook[sheet_name]\n",
    "            # Retrieve header row and extract unique header values\n",
    "            header_row = next(sheet.rows)\n",
    "            header_values = {cell.value for cell in header_row}\n",
    "            # Find missing columns in the header\n",
    "            missing_columns = required_columns - header_values\n",
    "            # Print appropriate message based on missing columns\n",
    "            if missing_columns:\n",
    "                print(f\"The following columns are missing in the sheet '{sheet_name}' of file '{file_path}': {', '.join(missing_columns)}.\")\n",
    "            else:\n",
    "                print(f\"The sheet '{sheet_name}' of file '{file_path}' contains all required columns.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File '{file_path}' not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing file '{file_path}': {e}\")\n",
    "\n",
    "# Files to be read:\n",
    "files_info = [\n",
    "     \n",
    "     {\n",
    "        \"file_path\": fr\"C:\\Users\\\\{username}\\\\{folder}\\Ecolabeling Accuracy Individual.xlsx\",\n",
    "        \"sheet_name\": \"Sheet1\",\n",
    "        \"required_columns\": {'LOB','Week','Queue','Label','Labeler','TRUE_1','FALSE_1','Total Sampled','Accuracy','Date'}   \n",
    "    }\n",
    "]\n",
    "# Call the function with the list of file information\n",
    "verify_excel_files(files_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2115e0",
   "metadata": {},
   "source": [
    "# Will move the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa9793eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Move the files to the EMEA folder\n",
    "    shutil.copy(RS_ADSO, RS_ADSO_EMEA)\n",
    "    log_df = log_df.append({'Filename': f'EMEA:{RS_ADSO}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    " \n",
    "    \n",
    "    # Move the files to the Drops folder\n",
    "    shutil.copy(RS_ADSO, RS_ADSO_DROPS)\n",
    "    log_df = log_df.append({'Filename': f'Drops:{RS_ADSO}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "    \n",
    "    # Move the files to the sharepoint folder\n",
    "    shutil.copy(RS_ADSO, RS_ADSO_onedrive)\n",
    "    log_df = log_df.append({'Filename': f'Drops:{RS_ADSO}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    " \n",
    "\n",
    "\n",
    " \n",
    "except Exception as e:\n",
    "    log_df = log_df.append({'Filename': RS_ADSO, 'Date': date_today, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ebfcd1",
   "metadata": {},
   "source": [
    "---IF YOU DOWNLOAD LIVE R1 QUIZZ DATA RUN THE NEXT CODE----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f650ff",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727f305",
   "metadata": {},
   "source": [
    "# Live R1 Quizzes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a399ac",
   "metadata": {},
   "source": [
    "Go to the link and download the file Live R1 R3 - Quiz\n",
    "Link:  https://teleperformance.sharepoint.com/sites/TikTok212/Shared%20Documents/Forms/AllItems.aspx?csf=1&web=1&e=Pe8zSi&cid=ccd55fb2%2D82d4%2D404d%2Db167%2Dc77620ab68a6&FolderCTID=0x012000B98B5AF80497F14392E44394E54AA19B&id=%2Fsites%2FTikTok212%2FShared%20Documents%2FData%20Analytics%2F1%2E%20Quality%20files%20for%20DA%20Team%20PBI%20reports%2FLIVE%20R1%2F2%2E%20LIVE%20R1%20quizzes&viewid=4d19ea0e%2D0774%2D4a0a%2D8fc4%2D204e3ee1184f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e60051b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE LOCATION:\n",
    "xls = pd.ExcelFile(fr\"C:\\Users\\\\{username}\\\\{folder}\\Live R1 R3 - Quiz.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61d6cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each sheet:\n",
    "df1 = pd.read_excel(xls, 'Live R1 FR')\n",
    "df2 = pd.read_excel(xls, 'Live R1 SE')\n",
    "df3 = pd.read_excel(xls, 'Live R1 IT')\n",
    "df4 = pd.read_excel(xls, 'Live R1 NL')\n",
    "df5 = pd.read_excel(xls, 'Live R1 HE')\n",
    "df6 = pd.read_excel(xls, 'Live R2 IL')\n",
    "df7 = pd.read_excel(xls, 'Live R2 SE')\n",
    "df8 = pd.read_excel(xls, 'Gaming FR')\n",
    "df9 = pd.read_excel(xls, 'Live R1 ES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62cbf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concact all the sheets on a single file:\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9])\n",
    "#drop row vazias na coluna \"Date\"\n",
    "df =df[~df.Date.isnull()]\n",
    "# save the file on emea:\n",
    "df.to_excel(r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\Quizzes\\LIVE R1 & R3\\Live R1 R3 - Quiz.xlsx', index=False)\n",
    "#Drops folder location:\n",
    "EMEA_location = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\Quizzes\\LIVE R1 & R3\\Live R1 R3 - Quiz.xlsx\"\n",
    "Data_Drops= r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\Quizzes_live _r1_r3\"\n",
    "\n",
    "#Sharepoitn\n",
    "Quizzes_LIVE_onedrive = fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Quizzes_LIVE R1 & R3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80b31b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#move the file to Drops:\n",
    "try:\n",
    "    # Move the files from EMEA folder to DROPS folder \n",
    "    shutil.copy(EMEA_location, Data_Drops)\n",
    "    log_df = log_df.append({'Filename': EMEA_location, 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "    # Move the files from EMEA folder to Sharepoint folder \n",
    "    shutil.copy(EMEA_location, Quizzes_LIVE_onedrive)\n",
    "    log_df = log_df.append({'Filename': EMEA_location, 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    " \n",
    "except Exception as e:\n",
    "    log_df = log_df.append({'Filename': EMEA_location, 'Date': date_today, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb313084",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a008a2",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd2c5c",
   "metadata": {},
   "source": [
    "# STD_DIM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03670798",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use the link to download the file and them run the code \n",
    "\n",
    "\n",
    "Link: https://teleperformance.sharepoint.com/:x:/r/sites/S.DAF.Operations_Data_Analytics/_layouts/15/Doc.aspx?sourcedoc=%7B239593F6-9215-440A-A196-777F73C4952C%7D&file=STD_DIMS_TTOK%20new%20(Teams%20Edit).xlsx&action=default&mobileredirect=true&cid=232a65da-64a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f16ca7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\\\\\bechara.9\\\\\\\\Downloads\\\\STD_DIMS_TTOK new (Teams Edit).xlsx'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#File location:\n",
    "STD_DIM=fr\"C:\\Users\\\\{username}\\\\{folder}\\STD_DIMS_TTOK new (Teams Edit).xlsx\"\n",
    "\n",
    "#Folder destination:\n",
    "EMEA_STD_DIM= r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\STD_DIMS_TTOK new.xlsx'\n",
    "Drop_DIM_LOB= r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\Dim_LOB\\STD_DIMS_TTOK new.xlsx'\n",
    "Drop_Target= r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\Dim_Target\\STD_DIMS_TTOK new.xlsx'\n",
    "Onedrive_STD_DIM=fr\"C:\\Users\\{os.getenv('USERNAME')}\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\15. DIM files\\STD_DIMS_TTOK new.xlsx\"\n",
    "STD_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dd473f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Result_lob: \u001b[1;32mTRUE\u001b[0m\n",
      "New Values: []\n"
     ]
    }
   ],
   "source": [
    "#THIS CODE WILL VALIDATE IS THE DOWNLOAD FILES HAVE ALL THE DATA THAT WE NEED \n",
    "# List for validation: \n",
    "STD_DIMv =pd.read_excel(fr\"C:\\Users\\\\{username}\\\\{folder}\\STD_DIMS_TTOK new (Teams Edit).xlsx\", sheet_name=\"DIM_LOB\")\n",
    " \n",
    "LobList = ['?', 'MI', 'ADSO', 'ALL LOB', 'AVIA', 'BFS', 'TTR1', 'CSAM', 'DSA',\n",
    "       'LIVE R1', 'LIVE R2', 'LIVE R3', 'RESSO', 'STAFF', 'TTR2']\n",
    " \n",
    " \n",
    "# Function that will check is the name of the above list is on the files:\n",
    "def verifica_valores_na_coluna(coluna, LobList):\n",
    "    new_values = []\n",
    "    for valor_coluna in coluna:\n",
    "        if valor_coluna.lower() not in [v.lower() for v in LobList]:\n",
    "                 new_values.append(valor_coluna)\n",
    "    if new_values:\n",
    "            return \"\\033[1;31m!!!!!!FALSE!!!!!!\\033[0m\", new_values\n",
    "    return \"\\033[1;32mTRUE\\033[0m\", []\n",
    " \n",
    "# APPLY THIS CODE TO THE COLUMN OF OUR DATASET:\n",
    "resultado_lob1, novos_valores = verifica_valores_na_coluna(STD_DIMv['LOB_L1'], LobList)\n",
    " \n",
    "print(\"Validation Result_lob:\", resultado_lob1)  # Accessing the first element of the tuple\n",
    "print(\"New Values:\", novos_valores)\n",
    " \n",
    " \n",
    "#!!!!TRUE (ALL GOOD)\n",
    "#!!!!FLASE(NOT GOOD=MISSING DATA ON OUR DOWNLOADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "773ef69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Result: \u001b[1;31m!!!!!!FALSE!!!!!!\u001b[0m\n",
      "New Values: ['MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'PHOTO LLM', 'STP', 'PHOTO LLM', 'PHOTO LLM', 'STP', 'STP', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MARS', 'MASK Labeling', 'MASK Labeling']\n"
     ]
    }
   ],
   "source": [
    "STD_DIMv2 =pd.read_excel(fr\"C:\\Users\\\\{username}\\\\{folder}\\STD_DIMS_TTOK new (Teams Edit).xlsx\", sheet_name=\"DIM_LOB\")\n",
    " \n",
    "sub_lob = ['?', 'ADS', 'ALL LOB', 'ASR', 'AVIA', 'BFS', 'CA', 'CF',\n",
    "       'CLICKBAIT', 'COMMERCIAL CONTENT', 'CREATOR', 'CREATOR FUND',\n",
    "       'CSAM', 'DSA', 'EDF', 'ENT', 'FIZZO', 'GAMING', 'GCL', 'HASH LAB',\n",
    "       'HPCL', 'INSTRUCTIVE', 'LABELING', 'LCL', 'LEMON8',\n",
    "       'LIVE LABELING', 'LIVE R1', 'LIVE R2', 'LIVE R3',\n",
    "       'MENTAL HEALTH NARRATIVE', 'MI', 'NACL', 'OCR', 'ORIG', 'PML',\n",
    "       'RESSO', 'SBP', 'SPS', 'SSM', 'STAFF', 'TAL', 'TRECO', 'TTR1',\n",
    "       'TTR2', 'URL', 'VARL', 'VC', 'VDL', 'RVL', 'LTLM', 'CSL','HVC', 'CAPCUT','MCM','LVL','Issue Labeling','TCOR','VDL5.0', 'MULTI','SDS','COMMENTS QUALITY',\n",
    "       'COMMENTS TAXONOMY','MARS', 'PHOTO LLM', 'STP', 'MASK Labeling']\n",
    " \n",
    " \n",
    "# Function that will check is the name of the above list is on the files:\n",
    "def verifica_valores_na_coluna1(coluna, sub_lob):\n",
    "    new_values1 = []\n",
    "    for valor_coluna in coluna:\n",
    "        if valor_coluna.lower() not in [v.lower() for v in sub_lob]:\n",
    "                 new_values1.append(valor_coluna)\n",
    "    if new_values1:\n",
    "            return \"\\033[1;31m!!!!!!FALSE!!!!!!\\033[0m\", new_values1\n",
    "    return \"\\033[1;32mTRUE\\033[0m\", []\n",
    "\n",
    "\n",
    "# APPLY THIS CODE TO THE COLUMN OF OUR DATASET:\n",
    "resultado1, novos_valores1 = verifica_valores_na_coluna1(STD_DIMv2['LOB_L2'], sub_lob)\n",
    " \n",
    " \n",
    "print(\"Validation Result:\", resultado1)  # Accessing the first element of the tuple\n",
    "print(\"New Values:\", novos_valores1)\n",
    "#!!!!TRUE (ALL GOOD)\n",
    "#!!!!FLASE(NOT GOOD=MISSING DATA ON OUR DOWNLOADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "658d5028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Move the file:\n",
    "try:\n",
    "    # Move the files from EMEA folder to DROPS folder \n",
    "    shutil.copy(STD_DIM, EMEA_STD_DIM)\n",
    "    log_df = log_df.append({'Filename': f'Emea:{STD_DIM}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "    shutil.copy(STD_DIM, Drop_DIM_LOB)\n",
    "    log_df = log_df.append({'Filename': f'Drop:{STD_DIM}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "    shutil.copy(STD_DIM, Drop_Target)\n",
    "    log_df = log_df.append({'Filename': f'Drop:{STD_DIM}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "    shutil.copy(STD_DIM, Onedrive_STD_DIM)\n",
    "    log_df = log_df.append({'Filename': f'Drop:{STD_DIM}', 'Date': date_today, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "except Exception as e:\n",
    "    log_df = log_df.append({'Filename': STD_DIM, 'Date': date_today, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a223a820",
   "metadata": {},
   "source": [
    "# Log file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb8041aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=datetime.today().strftime('%d-%m-%Y %H-%M-%S')\n",
    "Log_file=log_df.to_excel(fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\Logs_etl_part3_{date}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63501e9",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
