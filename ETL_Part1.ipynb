{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4150ad40",
   "metadata": {},
   "source": [
    "# ETL script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e018b5",
   "metadata": {},
   "source": [
    "ETL Manual: https://teleperformance.sharepoint.com/:x:/r/sites/S.DAF.Operations_Data_Analytics/Shared%20Documents/BAU%20Tasks%20and%20Procedures/P.TTOK.CONT/PROCEDURE%201_ETL.xlsx?d=w89a05f7c5bcf43158ff39cfa7d141eaf&csf=1&web=1&e=cVHTOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5930b6d",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d846f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import calendar\n",
    "import re\n",
    "import win32com.client\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime, timedelta\n",
    "import win32com.client as win32\n",
    "import subprocess\n",
    "import requests\n",
    "import warnings\n",
    "import win32com.client as w3c\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac398a0a",
   "metadata": {},
   "source": [
    "# !!! IMP NOTE ( DONT FORGET TO CHANGE THE TIMEZONE WHEN THE HOUR CHANGE)!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19b2ca4",
   "metadata": {},
   "source": [
    "Change Time Zone\t\t\tfor Portugal, if Winter: \"UTC+0\", if Summers: \"UTC +1\" (+01:00 Paris now)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc8b32",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12ec8d",
   "metadata": {},
   "source": [
    "## !!!CHANGE THE  USER NAME, AND DOWNLOAD PATH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39a91df0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username: sequeira.81\n",
      "Path:C:\\\\Users\\\\sequeira.81\\\\Downloads\n",
      "---------------------------------------------\n",
      "Empty DataFrame\n",
      "Columns: [Filename, Date, Status, Message]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#THE CODE SHOULD SHOW YOUR NAME:\n",
    "username = os.getlogin()\n",
    "username\n",
    "print(f\"Username: {username}\")\n",
    "#IF YOU DONT USE YOUR DOWNLOAD FOLDER CHANGE THE BELOW STRING TO YOUR FOLDER:\n",
    "folder=\"Downloads\"\n",
    "downloads_folder_path = fr\"C:\\\\Users\\\\{username}\\\\{folder}\"\n",
    "print(f\"Path:{downloads_folder_path}\")\n",
    "\n",
    "date_today= datetime.today().strftime('%d-%m-%Y')\n",
    "log_df = pd.DataFrame(columns=['Filename', 'Date', 'Status', 'Message'])\n",
    "log_df.drop(log_df.index, inplace=True)\n",
    "print(\"---------------------------------------------\")\n",
    "print(log_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18869f",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9ce66c",
   "metadata": {},
   "source": [
    "# HC tracker ( Friday: Current week / Monday: Previous week)\n",
    "\n",
    "__Link:__ https://byteworks-va.bytelemon.com/hc/fte-tracker/bpo\n",
    "\n",
    "__Dashboard:__ WORKFORCE\n",
    "\n",
    "__TAB:__ FTE ENTRY - BPO Available FTE\n",
    "\n",
    "__Filter:__ BPO Site: If friday extract the data from the current week, if Monday extract the data from last week,\n",
    "On the left we have the month group, and then the week. \n",
    "\n",
    "__Filter( Advanced Filters // Category ):__  Select all the Categories and Apply\n",
    "\n",
    "__Download:__ Click on on the 3 dots and then Export, a new page will open with an excel file and then we can export the file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3de8354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['available_fte_1725606337014916332.xlsx']\n",
      "Reading file: C:\\\\Users\\\\sequeira.81\\\\Downloads\\available_fte_1725606337014916332.xlsx\n",
      "4\n",
      "Year week selected: 2024 W36\n",
      "site_management_2024 W36.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Hc_Tracker_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\40. FTE Tracker\"\n",
    "\n",
    "Hc_Tracker_onedrive= fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\40. FTE Tracker\"\n",
    "\n",
    "Hc_Tracker_files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                  filename.startswith(\"available_fte_\")]\n",
    "print(Hc_Tracker_files)\n",
    "\n",
    "\n",
    "for jj in Hc_Tracker_files:\n",
    "    try:\n",
    "        file_path = os.path.join(downloads_folder_path, jj)\n",
    "        print(f\"Reading file: {file_path}\")\n",
    "        u = pd.read_excel(file_path)\n",
    "        # Get today's date\n",
    "        today = datetime.today()\n",
    "        # Get the weekday (0 is Monday, 6 is Sunday)\n",
    "        weekday = today.weekday()\n",
    "        print(weekday)\n",
    "        # Calculate the ISO week\n",
    "        year, week, _ = today.isocalendar()\n",
    "        # If today is Friday, use the current week\n",
    "        if weekday == 4:\n",
    "            week_str = f\"{year} W{week:02d}\"\n",
    "        # If today is Monday, use the previous week\n",
    "        elif weekday == 0:\n",
    "            # Subtract 7 days from today to get a date in the previous week\n",
    "            last_week = today - timedelta(days=7)\n",
    "            # Calculate the ISO week for the date in the previous week\n",
    "            year, week, _ = last_week.isocalendar()\n",
    "            week_str = f\"{year} W{week:02d}\"\n",
    "      \n",
    "        else:\n",
    "            week_str = f\"{year} W{week:02d}\"\n",
    "\n",
    "        print(f\"Year week selected: {week_str}\")\n",
    "                    \n",
    "        #Will verify how many columns has like \"Target FTE\", If more than 1 will keep the last columns and drop the rest.\n",
    "        fte_cols = [col for col in u.columns if col.startswith('Target FTE')]\n",
    "        if len(fte_cols) >1:\n",
    "            fte_cols.sort()\n",
    "            cols_drop = fte_cols[:-1]\n",
    "            u=u.drop(columns=cols_drop)\n",
    "        # Rename columna names like 'Unnamed' to blank value:    \n",
    "        for col in u.columns:\n",
    "            if col.startswith('Unnamed:'):\n",
    "                u.rename(columns={col: ''}, inplace=True)\n",
    "            # rename the column to Target FTE     \n",
    "            elif col.startswith('Target FTE'):\n",
    "                u.rename(columns={col:'Target FTE Final'},inplace= True)\n",
    "\n",
    "\n",
    "        ff = f'site_management_{week_str}.xlsx'\n",
    "        print(ff)\n",
    "        \n",
    "        u.to_excel(f\"{Hc_Tracker_EMEA}/{ff}\", index=False)\n",
    "        u.to_excel(f\"{Hc_Tracker_onedrive}/{ff}\", index=False)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Update the log DataFrame in case of success\n",
    "        log_df = log_df.append({'Filename': jj, 'Date': date_today, 'Status': 'Success', 'Message': 'File saved successfully'}, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        # Update the log DataFrame in case of error\n",
    "        log_df = log_df.append({'Filename': jj, 'Date': date_today, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce228f",
   "metadata": {},
   "source": [
    "# TEAM FILE BK\n",
    "\n",
    "____Note:__ Run the below code, this code will move the teamfile used on the last refresh and move it the backup folder, so we will able to have files to rollback in case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f05d1483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moveed with success.\n"
     ]
    }
   ],
   "source": [
    "#1 GROUP\n",
    "#this code will move the team file from dim folder to a backup folder:\n",
    "Date= datetime.now() - timedelta(1)\n",
    "Date=Date.strftime('%Y-%m-%d')\n",
    "TeamfileOnUse = r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\TikTok Lisbon - Teamfile New - Teamfile Lisbon.csv'\n",
    "FolderBk=fr'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\TeamFile BK\\TikTok Lisbon - Teamfile New - Teamfile Lisbon-{Date}.csv'\n",
    "\n",
    "try:\n",
    "    # Move the files to the folder\n",
    "    shutil.copy(TeamfileOnUse, FolderBk)\n",
    "    \n",
    "\n",
    "    print(f\"Files moveed with success.\")\n",
    "except Exception as e:\n",
    "    print(\"Fails\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9611a",
   "metadata": {},
   "source": [
    "# TEAM FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da120d",
   "metadata": {},
   "source": [
    "__Step 1:__ Download the Team file from the below link:\n",
    "https://teleperformance.sharepoint.com/:x:/r/sites/P.TTOK.CONTTEAMDATAGROUP/Shared%20Documents/General/Teamfile/teamfile_management.xlsx?d=wea7a73936c5d4b9590422a33d71c4b4d&csf=1&web=1&e=dfITMi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7749e",
   "metadata": {},
   "source": [
    " __Step 2:__ Run the below code, code will verify the file that you download and validate if all columns that should be bool are bool, in case is not will print a text sayig it, and if all well will move a file to emea to the folder 15 DIM, with the name ( TikTok Lisbon - Teamfile New - Teamfile Lisbon (validate).csv).\n",
    " \n",
    " If the file have problems on these columns, if is empty the columns should not be a issue if is with #ref will crach the model. If have # ref please use the step 6 and inform the BO team, using the email script.\n",
    " \n",
    " __If all good go to the next step__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91263f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas as colunas são do tipo booleano.\n"
     ]
    }
   ],
   "source": [
    "#2 GROUP\n",
    "#The below code will take the file from sharepoint,verify is all the columns from the \"Columns_name\" varieable are bool\n",
    "#if not the code will show the column with the wrong type and not move the file. \n",
    "\n",
    "df = pd.read_excel (fr'C:\\\\Users\\\\{username}\\\\{folder}\\teamfile_management.xlsx', sheet_name = 'Teamfile')\n",
    "df.columns=df.columns.str.title()\n",
    "\n",
    "class NonBooleanColumnError(Exception):\n",
    "    pass\n",
    "\n",
    "def check_columns_for_bool(df, column_names):\n",
    "    non_bool_columns = []\n",
    "\n",
    "    for column_name in column_names:\n",
    "        dtype = df[column_name].dtype\n",
    "        if dtype != bool:\n",
    "            non_bool_columns.append(column_name)\n",
    "\n",
    "    if non_bool_columns:\n",
    "        raise NonBooleanColumnError(f\"Colunas não booleanas encontradas: {non_bool_columns},Tipo encontrado: {dtype}\")\n",
    "\n",
    "try:\n",
    "\n",
    "    Columns_name=['Lark_Account_Provisioning','Lark_Account_Deprovisioning','Is_Active',\n",
    "                    'Locker_Need_Termination','Last_Record']\n",
    "\n",
    "    check_columns_for_bool(df, Columns_name)\n",
    "    print(\"Todas as colunas são do tipo booleano.\")\n",
    "    df.to_csv (r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\TikTok Lisbon - Teamfile New - Teamfile Lisbon (validate).csv',\n",
    "           index =False)\n",
    "    \n",
    "except NonBooleanColumnError as e:\n",
    "    print(f\"Erro: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f66da1",
   "metadata": {},
   "source": [
    "__Just in case you need to change the excel file that validades the teamfile,the one in EMEA.__\n",
    "\n",
    "__IMP: All changes on  the EMEA file, please change it on cloud too!!!!!__\n",
    "https://teleperformance.sharepoint.com/sites/S.DAF.Operations_Data_Analytics/Shared%20Documents/Forms/AllItems.aspx?RootFolder=%2Fsites%2FS%2EDAF%2EOperations%5FData%5FAnalytics%2FShared%20Documents%2FP%2ETTOK%2ECONT%2F10%2E%20Data%20%26%20PBI%20management%2F1%2E%20Raw%2Ddata%20validation%20files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c57a247",
   "metadata": {},
   "source": [
    " __Step 3:__ Run the below code, this code will excute a refresh on the EMEA excel file that validates the Teamfile.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be16122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the process....................\n",
      "...1\n",
      "...2\n",
      "...3\n",
      "Refreshing new data..............\n",
      "<COMObject Open> File opened.....\n",
      "<COMObject Open> File refreshed.....\n",
      "<COMObject Open> File saved.....\n",
      "Refresh new data.... Done\n"
     ]
    }
   ],
   "source": [
    "#3 GROUP\n",
    "gen_py_path = fr\"C:\\Users\\\\{username}\\\\AppData\\Local\\Temp\\gen_py\"\n",
    "shutil.rmtree(gen_py_path, ignore_errors=True)\n",
    "print('Starting the process....................')\n",
    "xl = w3c.Dispatch(\"Excel.Application\")\n",
    "print('...1')\n",
    "xl.Visible = False\n",
    "print('...2')\n",
    "xl.DisplayAlerts = False\n",
    "print('...3')\n",
    "print('Refreshing new data..............')\n",
    "wb = xl.Workbooks.Open(r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\Validate Team File - new.xlsx\")\n",
    "print(wb, 'File opened.....')\n",
    "wb.RefreshAll()\n",
    "xl.CalculateUntilAsyncQueriesDone()\n",
    "print(wb,'File refreshed.....')\n",
    "wb.Save()\n",
    "print(wb,'File saved.....')\n",
    "wb.Close(True)  \n",
    "xl.Quit()\n",
    "print('Refresh new data.... Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecda61d",
   "metadata": {},
   "source": [
    "__Step 4:__ If the output on the previous code is \"Refresh new data.... Done\" you can  run the below code.\n",
    "\n",
    "The code will read the \"Validate Team File - new.xlsx\", and show you as output the first sheet name. \n",
    "\n",
    "If you have the __NOK__ columns with all 0 jump to the __step 7__, if not go to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ace834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page</th>\n",
       "      <th>NOK</th>\n",
       "      <th>TO DO if Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEAM FILE</td>\n",
       "      <td>0</td>\n",
       "      <td>(1-TEAM FILE): The Logins returned are Logins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEAM FILE</td>\n",
       "      <td>0</td>\n",
       "      <td>(2-End Date Validation):More than 1 end date b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEAM FILE</td>\n",
       "      <td>0</td>\n",
       "      <td>(3-Date Validation): Start date is smaller tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEAM FILE</td>\n",
       "      <td>0</td>\n",
       "      <td>(4-Start Date Validation): Start date is empty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEAM FILE</td>\n",
       "      <td>0</td>\n",
       "      <td>(5-Go-live Date Validation): Go-live date  nee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TEAM FILE</td>\n",
       "      <td>0</td>\n",
       "      <td>(6-Email duplicated Validation): Check if the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Page  NOK                                     TO DO if Error\n",
       "0  TEAM FILE    0  (1-TEAM FILE): The Logins returned are Logins ...\n",
       "1  TEAM FILE    0  (2-End Date Validation):More than 1 end date b...\n",
       "2  TEAM FILE    0  (3-Date Validation): Start date is smaller tha...\n",
       "3  TEAM FILE    0    (4-Start Date Validation): Start date is empty \n",
       "4  TEAM FILE    0  (5-Go-live Date Validation): Go-live date  nee...\n",
       "5  TEAM FILE    0  (6-Email duplicated Validation): Check if the ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4 GROUP\n",
    "#THIS WILL READ THE VALIDATION FILE AFTER THE REFRESH, SO WE CAN SEE THE ERROS:\n",
    "validation_teamfile =r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\Validate Team File - new.xlsx\"\n",
    "\n",
    "pd.read_excel(validation_teamfile,skiprows=1,usecols=[1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f5b43",
   "metadata": {},
   "source": [
    " __Step 5:__ If the above code gives you a __NOK__ you can use the below code to open the validate file an then check the Sheet name so you can indentify the error, so you can use the __step 6__ to email the BO team.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your excel file\n",
    "file_path = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\Validate Team File - new.xlsx\"\n",
    "\n",
    "# Open the file with the default application\n",
    "if os.name == 'nt':  # For Windows\n",
    "    os.startfile(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e5cd76",
   "metadata": {},
   "source": [
    "__Step 6:__ Once you identify what are the errors on the Teamfile, you can use the below code to send an email to BO team.\n",
    "\n",
    "Change the variable \"newmail.CC\" in case you want to add your email, and you can change the variable newmail.Body to write a different mail.\n",
    "\n",
    "__IMP: Do not remove the '''\\ at the beginning and the last ''' as they ensure the formatting of the mail body remains intact.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 GROUP\n",
    "import win32com.client\n",
    "ol=win32com.client.Dispatch(\"outlook.application\")\n",
    "olmailitem=0x0 #size of the new email\n",
    "newmail=ol.CreateItem(olmailitem)\n",
    "newmail.Subject= 'Team File data issue'\n",
    "newmail.To='TikTok-Backoffice-Lisbon@pt.teleperformance.com'\n",
    "newmail.CC='Data-Analytics-TikTok-Lisbon@pt.teleperformance.com'\n",
    "newmail.Body= '''\\\n",
    "Hello Team,\n",
    "\n",
    "Could you please verify the following shortlogin \"NDZA\", has 2 end dates empty.\n",
    "\n",
    "\n",
    "Thank you.\n",
    "\n",
    "'''\n",
    "\n",
    "# attach='C:\\\\Users\\\\admin\\\\Desktop\\\\Python\\\\Sample.xlsx'\n",
    "# newmail.Attachments.Add(attach)\n",
    "\n",
    "# To display the mail before sending it\n",
    "#newmail.Display()\n",
    "\n",
    "newmail.Send()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360fb406",
   "metadata": {},
   "source": [
    "__IMP:__ Once the BO team fixe all the issues with the team file please do again the step ( 1, 2,3,4), if no noks, jump to the step 7.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d1ee14",
   "metadata": {},
   "source": [
    "__Step 7:__ If all good with the teamfile, run the 1 code group, the code will move the file with the right name for powerbi. \n",
    "\n",
    "Once this first group will show (\"Files moveed with success.\"), run the 2 group that will move the file to sql folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796dcac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moveed with success.\n"
     ]
    }
   ],
   "source": [
    "#ETL 7 GROUP\n",
    "#1 group:\n",
    "df.to_csv (r'\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\TikTok Lisbon - Teamfile New - Teamfile Lisbon.csv',\n",
    "           index =False)\n",
    "Teamfile_EMEA=r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\15. DIM files\\TikTok Lisbon - Teamfile New - Teamfile Lisbon.csv\"\n",
    "Teamfile_datadrops= r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\Dim_Agent\"\n",
    "Teamfile_Onedrive= fr\"C:\\Users\\{os.getenv('USERNAME')}\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\15. DIM files\"\n",
    "print(\"Files moveed with success.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800b9657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files moveed with success.\n"
     ]
    }
   ],
   "source": [
    "#ETL 8 GROUP\n",
    "#2 group\n",
    "try:\n",
    "    # Move the files to the folder\n",
    "    shutil.copy(Teamfile_EMEA, Teamfile_datadrops)\n",
    "    shutil.copy(Teamfile_EMEA, Teamfile_Onedrive)\n",
    "\n",
    "    print(\"Files moveed with success.\")\n",
    "except Exception as e:\n",
    "    print(\"Fails\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d4e66",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0b045",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93a87b",
   "metadata": {},
   "source": [
    "## UTC Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35cddf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1º GROUP\n",
    "#This code will read the folder paths:\n",
    "\n",
    "#EMEA Folders:\n",
    "BPO_PEOPLE_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\"\n",
    "BPO_PEOPLE_MI_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE_MI\"\n",
    "BPO_QUEUE_HOUR_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE HOUR\"\n",
    "BPO_QUEUE_DAILY_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_DAILY\"\n",
    "BPO_QUEUE_VMS_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_VMS\"\n",
    "INTEGRITY_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\INTEGRITY\"\n",
    "MODERATION_STATS_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\MODERATION_STATS\"\n",
    "MODERATION_STATS_HOUR_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\MODERATION_STATS_HOUR\"\n",
    "BPO_QUEUE_HOUR_RR2_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_HOUR_RR2.0\"\n",
    "BPO_QUEUE_DAILY_RR2_EMEA = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_DAILY_RR2.0\"\n",
    "\n",
    "#Data Drops (New paths) ) DE Team  SQL folders\n",
    "BPO_PEOPLE_DROP = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\BPOPeopleWorkhour_new\"\n",
    "BPO_QUEUE_HOUR_DROP =r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\BPO_Queue_new\"\n",
    "BPO_QUEUE_VMS_DROP = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\BPO_Queue_VMS\"\n",
    "INTEGRITY_DROP = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\BPO_Queue_integrity\"\n",
    "MODERATION_STATS_HOUR_DROP =r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\08 - data engineering\\Data Drops\\P.TTOK.CONT\\BPO_hour_global_moderation_stat\"\n",
    "\n",
    "#Databricks\n",
    "BPO_PEOPLE_DATABRICKS= r\"\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\bpopeopleworkhour\"\n",
    "\n",
    "#Data Sharepoint:\n",
    "BPO_Onedrive= fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\BPO_PEOPLE\"\n",
    "INTEGRITY_Ondrive= fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\INTEGRITY\"\n",
    "\n",
    "#Validacao:\n",
    "BPO_PEOPLE_Validacao: r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\BPO_People\"\n",
    "BPO_PEOPLE_Validacao_ADSO:r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\BPO_People_ADSO\"\n",
    "BPO_PEOPLE_Validacao_Integrity:r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\Integrity\"\n",
    "BPO_PEOPLE_Validacao_Integrity_MI:r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\Integrity_MI\"\n",
    "    \n",
    "#Folders for validation code( The lines have a short_name)\n",
    "folders2={\n",
    "    'BPO_PEOPLE(3 em 3)download last 2 weeks data)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\BPO_People\",\n",
    "    'BPO_PEOPLE_ADSO(3 em 3)download last 2 weeks data)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\BPO_People_ADSO\",\n",
    "    'BPO_QUEUE_HOUR_RR1.0 (4 em 4)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE HOUR\",\n",
    "    'BPO_QUEUE_HOUR_RR2.0(4 em 4)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_HOUR_RR2.0\",\n",
    "    'BPO_QUEUE_VMS(8 em 8)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_VMS\",\n",
    "    'MODERATION_STATS_HOUR_EMEA(4 em 4)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\MODERATION_STATS_HOUR\",\n",
    "    'BPO_QUEUE_DAILY_RR1.0(8 em 8)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_DAILY\",\n",
    "    'BPO_QUEUE_DAILY_RR2.0(8 em 8)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_DAILY_RR2.0\",\n",
    "    'MODERATION_STATS_EMEA(15 em 15)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\MODERATION_STATS\",\n",
    "    'INTEGRITY_EMEA(15 em 15)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\Integrity\",\n",
    "    'INTEGRITY_MI(15 em 15)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\Integrity_MI\"}\n",
    "\n",
    "\n",
    "#Folders for validation code( The lines have a short_name)\n",
    "folders={\n",
    "    'BPO_PEOPLE(3 em 3)download last 2 weeks data)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\",\n",
    "    'BPO_PEOPLE_MI(3 em 3)download last 2 weeks data)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE_MI\",\n",
    "    'BPO_QUEUE_HOUR_RR1.0 (4 em 4)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE HOUR\",\n",
    "    'BPO_QUEUE_HOUR_RR2.0(4 em 4)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_HOUR_RR2.0\",\n",
    "    'BPO_QUEUE_VMS(8 em 8)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_VMS\",\n",
    "    'MODERATION_STATS_HOUR_EMEA(4 em 4)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\MODERATION_STATS_HOUR\",\n",
    "    'BPO_QUEUE_DAILY_RR1.0(8 em 8)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_DAILY\",\n",
    "    'BPO_QUEUE_DAILY_RR2.0(8 em 8)': r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_DAILY_RR2.0\",\n",
    "    'MODERATION_STATS_EMEA(15 em 15)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\MODERATION_STATS\",\n",
    "    'INTEGRITY_EMEA(15 em 15)':r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\INTEGRITY\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b7c1c",
   "metadata": {},
   "source": [
    "__This code will show the last file on the above folder, base on the file name.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb39271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\u001b[1;31mBPO_PEOPLE(3 em 3)download last 2 weeks data)\u001b[0m', the most recent file is: '\u001b[1;31mWorkHour Indicators 03092024_05092024.csv\u001b[0m'\n",
      "'\u001b[1;31mBPO_PEOPLE_MI(3 em 3)download last 2 weeks data)\u001b[0m', the most recent file is: '\u001b[1;31mWorkHour Indicators 03092024_05092024.csv\u001b[0m'\n",
      "'\u001b[1;31mBPO_QUEUE_HOUR_RR1.0 (4 em 4)\u001b[0m', the most recent file is: '\u001b[1;31mLatSLA_Hourly_03092024_06092024.csv\u001b[0m'\n",
      "'\u001b[1;31mBPO_QUEUE_HOUR_RR2.0(4 em 4)\u001b[0m', the most recent file is: '\u001b[1;31mLatSLA_Hourly_03092024_06092024.csv\u001b[0m'\n",
      "'\u001b[1;31mBPO_QUEUE_VMS(8 em 8)\u001b[0m', the most recent file is: '\u001b[1;31mLatSLA_VMS_30082024_06092024.csv\u001b[0m'\n",
      "'\u001b[1;31mMODERATION_STATS_HOUR_EMEA(4 em 4)\u001b[0m', the most recent file is: '\u001b[1;31mhour_global_moderation_stat_03092024_06092024.csv\u001b[0m'\n",
      "'\u001b[1;31mBPO_QUEUE_DAILY_RR1.0(8 em 8)\u001b[0m', the most recent file is: '\u001b[1;31mLatSLA_30082024_06092024.csv\u001b[0m'\n",
      "'\u001b[1;31mBPO_QUEUE_DAILY_RR2.0(8 em 8)\u001b[0m', the most recent file is: '\u001b[1;31mLatSLA_30082024_06092024.csv\u001b[0m'\n",
      "'\u001b[1;31mMODERATION_STATS_EMEA(15 em 15)\u001b[0m', the most recent file is: '\u001b[1;31mglobal_moderation_stat_01092024_15092024.csv\u001b[0m'\n",
      "'\u001b[1;31mINTEGRITY_EMEA(15 em 15)\u001b[0m', the most recent file is: '\u001b[1;31mIntegrity_01092024_15092024.csv\u001b[0m'\n"
     ]
    }
   ],
   "source": [
    "#2º GROUP\n",
    "#THIS CODE WILL VERIFY FROM ABOVE FOLDERM WHAT IS THE LAST FILES ON EACH FOLDER:\n",
    "def format_text(text, color_code, bold=False):\n",
    "    if bold:\n",
    "        return f\"\\033[1;{color_code}m{text}\\033[0m\"\n",
    "    else:\n",
    "        return f\"\\033[{color_code}m{text}\\033[0m\"\n",
    "for short_name, folder_path in folders.items():\n",
    "    # Checking if the path is a folder\n",
    "    if os.path.isdir(folder_path):\n",
    "        # List all the files\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        if files:\n",
    "            # Find the most resent date base on the file name\n",
    "            max_date = None\n",
    "            max_date_file = None\n",
    "            date_pattern = re.compile(r'_(\\d{8})_(\\d{8})')  # Expression of the date\n",
    "\n",
    "            for file in files:\n",
    "                date_matches = date_pattern.search(file)\n",
    "                if date_matches:\n",
    "                    start_date_str, end_date_str = date_matches.groups()\n",
    "                    start_date = datetime.strptime(start_date_str, '%d%m%Y')\n",
    "                    end_date = datetime.strptime(end_date_str, '%d%m%Y')\n",
    "\n",
    "                    # Use the final date (end_date) as a criterion for the most recent date\n",
    "                    if max_date is None or end_date > max_date:\n",
    "                        max_date = end_date\n",
    "                        max_date_file = file\n",
    "                else:\n",
    "                    # Handles the format \"01082023_04082023\" without the prefix before the dates ( BPO PEOPLE case)\n",
    "                    date_matches = re.search(r'(\\d{8})_(\\d{8})', file)\n",
    "                    if date_matches:\n",
    "                        start_date_str, end_date_str = date_matches.groups()\n",
    "                        start_date = datetime.strptime(start_date_str, '%d%m%Y')\n",
    "                        end_date = datetime.strptime(end_date_str, '%d%m%Y')\n",
    "\n",
    "                        if max_date is None or end_date > max_date:\n",
    "                            max_date = end_date\n",
    "                            \n",
    "                            max_date_file = file\n",
    "\n",
    "            if max_date_file:\n",
    "                max_date_format = format_text(max_date_file, 31, bold=True)\n",
    "                short_name_format = format_text(short_name, 31, bold=True)\n",
    "                print(f\"'{short_name_format}', the most recent file is: '{max_date_format}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f511939",
   "metadata": {},
   "source": [
    "__DATAPOWER LINK:___ https://datapower-va.bytelemon.com/operating/bpo_site_dashboard\n",
    " -----------------------------------------------------------------------------------------------------     \n",
    " __On the Bot BPO_PEOPLE:__\n",
    " \n",
    " __IMP: We need to use the GCP accountand select all the departems and not the \"MI\" department__\n",
    "  \n",
    " 1 file need to contain 3 days of data // Hourly data// Exclude the MI department from the department filter on the top // Everyday download all current week and previous  week/\n",
    "\n",
    "__Last Table:__ (  Moderation Raking  )\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\n",
    "        a. Filter( Column present: Workhours Indicators )\n",
    "\t\tc. Export Data:  Export Custom Subnit Data ( here you select the date range) \t\t\t\t\n",
    "\t\tD. File type : CSV\t\t\t\t\n",
    " -----------------------------------------------------------------------------------------------------     \t\t\t\t\n",
    " \n",
    "  __On the Bot BPO_PEOPLE_MI:__\n",
    " \n",
    " __IMP: We need to use the MI account and select only MI departmente:__\n",
    " \n",
    " 1 file need to contain 3 days of data // Hourly data// Everyday download all current week and previous  week/\n",
    "\n",
    "__Last Table:__ (  Moderation Raking  )\t\t\t\t\t\t\t\t\t\t\n",
    "\t\t\n",
    "        a. Filter( Column present: Workhours Indicators )\n",
    "\t\tc. Export Data:  Export Custom Subnit Data ( here you select the date range) \t\t\t\t\n",
    "\t\tD. File type : CSV\t\t\t\t\n",
    " -----------------------------------------------------------------------------------------------------     \t\t\t\t\t\t\n",
    " \n",
    " __BPO_QUEUE HOUR_RR1.0:__\n",
    " \n",
    " 1 file 4 days//Hourly data\n",
    "\n",
    "__First table:__  (Queue Statistics Table )\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\n",
    "\t\ta. Filter( Column present: basics Indicators )\t\t\t\t\n",
    "\t\tb.Queue type RR1.0\t\t\t\t\n",
    "\t\tc. Export Data:  Export Custom Subnit Data ( here you select the date range) \t\t\t\t\n",
    "\t\tD. File type : CSV\n",
    " -----------------------------------------------------------------------------------------------------       \n",
    "  __BPO_QUEUE HOUR_RR2.0:__\n",
    " \n",
    " 1 file 4 days//Hourly data\n",
    " \n",
    " __First table:__ (Queue Statistics Table )\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\n",
    "\t\ta. Filter( Column present: basics Indicators )\t\t\t\t\n",
    "\t\tb.Queue type RR2.0\t\t\t\t\n",
    "\t\tc. Export Data:  Export Custom Subnit Data ( here you select the date range) \t\t\t\t\n",
    "\t\tD. File type : CSV\n",
    "        \n",
    "-----------------------------------------------------------------------------------------------------                  \n",
    " __BPO_QUEUE_VMS:__\n",
    " \n",
    " 1 file 8 days//Hour data  \n",
    " \n",
    " __First table:__  (Queue Statistics Table )\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\n",
    "\t\ta. Filter( Column present: basics Indicators )\t\t\t\t\t\n",
    "\t\tb.Queue type VMS\t\t\t\t\t\n",
    "\t\tc. Export Data:  Export Custom Subnit Data ( here you select the date range) \t\t\t\t\t\n",
    "\t\tD. File type : CSV\n",
    "        \n",
    " -----------------------------------------------------------------------------------------------------     \n",
    " __MODERATION_STATS_HOUR:__\n",
    " \n",
    " 1  File 4 days // Hour data \n",
    "\n",
    "__Last Table:__ (  Moderation Raking  )\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\t\n",
    "\t\ta. Filter( Column present:  Basic Indicators ) /// Select Queue Data \t\t\t\t\t\n",
    "\t\tb. Select Data + Column preset: Basic  + Check the queue  box\t\t\t\t\t\n",
    "\t\tc. Export Data:  Export Custom Subnit Data ( here you select the date range) \t\t\t\t\t\n",
    "\t\tD. File type : CSV\t\t\n",
    "\n",
    " -----------------------------------------------------------------------------------------------------     \n",
    " __BPO_QUEUE_DAILY_RR1.0:__\n",
    " \n",
    " 1 file 8 days//Daily data\n",
    " \n",
    " __First table:__  (Queue Statistics Table )\t\t\t\t\t\t\n",
    "\n",
    "\t\ta. Filter( Column present: basics Indicators )\t\t\t\t\n",
    "\t\tb.Queue type RR1.0\t\t\t\t\n",
    "\t\tc. Export Data: Export Page Unit Data  \t\t\n",
    "\t\tD. File type : CSV\t\n",
    "        \n",
    " -----------------------------------------------------------------------------------------------------            \n",
    "  __BPO_QUEUE_DAILY_RR2.0:__\n",
    " \n",
    " 1 file 8 days//Daily data\n",
    " \n",
    " __First table:__  (Queue Statistics Table )\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\n",
    "\t\ta. Filter( Column present: basics Indicators )\t\t\t\t\n",
    "\t\tb.Queue type RR2.0\t\t\t\t\n",
    "\t\tc. Export Data: Export Page Unit Data  \t\t\n",
    "\t\tD. File type : CSV\t\n",
    "        \n",
    " \n",
    "        \n",
    " -----------------------------------------------------------------------------------------------------     \n",
    "\n",
    "\n",
    "  __MODERATION_STATS:__\n",
    "  \n",
    "  1 File (15 em 15) // Daily data      \n",
    " \n",
    " __Last Table:__ (  Moderation Raking  )\t\t\t\t\t\t\n",
    "\n",
    "\t\ta. Filter( Column present:  Basic Indicators ) /// Select Queue Data \t\t\t\t\n",
    "\t\tb. Select Data + Column preset: Basic  + Check the queue  box\t\t\t\t\n",
    "\t\tc. Export Data:  Export page unit Data\t\t\t\t\n",
    "\t\tD. File type : CSV\t\t\t\t\n",
    "\t\t\t\t\t\t\n",
    " -----------------------------------------------------------------------------------------------------    \t\t\t\n",
    "__INTEGRITY:__\n",
    " \n",
    " 1 File (15 em 15)// Daily data\n",
    " \n",
    " __Last Table:__ (  Moderation Raking  )\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t\n",
    "\t\ta. Filter( Column present: Integrity ) /// Select Queue Data \t\t\t\t\n",
    "\t\tb. Select Data + Column preset: Basic  + Check the queue  box\t\t\t\t\n",
    "\t\tc. Export Data:  Export page unit Data\t\t\t\t\n",
    "\t\tD. File type : CSV\t\t\t\t\t\t\t\n",
    "\n",
    " -----------------------------------------------------------------------------------------------------   \n",
    "__INTEGRITY ( MI Account) :__\n",
    "\n",
    "1 File (15 em 15)// Daily data\n",
    "\n",
    "Last Table: ( Moderation Raking )\n",
    "\n",
    "    a. Filter( Column present: Integrity ) /// Select Queue Data                 \n",
    "    b. Select Data + Column preset: Basic  + Check the queue  box                \n",
    "    c. Export Data:  Export page unit Data                \n",
    "    D. File type : CSV       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d84bd72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UTC+1：20240906080902.csv',\n",
       " 'UTC+1：20240906080908.csv',\n",
       " 'UTC+1：20240906081113.csv',\n",
       " 'UTC+1：20240906081131.csv',\n",
       " 'UTC+1：20240906081145.csv',\n",
       " 'UTC+1：20240906081154.csv',\n",
       " 'UTC+1：20240906081240.csv',\n",
       " 'UTC+1：20240906081254.csv',\n",
       " 'UTC+1：20240906081302.csv',\n",
       " 'UTC+1：20240906081315.csv',\n",
       " 'UTC+1：20240906081407.csv',\n",
       " 'UTC+1：20240906081415.csv',\n",
       " 'UTC+1：20240906081418.csv',\n",
       " 'UTC+1：20240906081507.csv',\n",
       " 'UTC+1：20240906081509.csv',\n",
       " 'UTC+1：20240906081521.csv',\n",
       " 'UTC+1：20240906081542.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3º GROUP\n",
    "#WILL SHOW THE NAME OF FILES THAT ARE ON OUR DOWNLOADS FOLDER BASE ON THE CONDITION OF THE FILE NAME\"UTC\"\n",
    "utc_files = [filename for filename in os.listdir(downloads_folder_path) if \n",
    "                  filename.startswith(\"UTC\")]\n",
    "utc_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60eef687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4º GROUP\n",
    "#Remove the files from folder BPO_QUEUE_HOUR_RR2_EMEA:\n",
    "[os.remove(os.path.join(BPO_QUEUE_HOUR_RR2_EMEA, filename)) for filename in os.listdir(BPO_QUEUE_HOUR_RR2_EMEA)\n",
    " if filename.startswith(\"LatSLA_Hourly\")]\n",
    "#Remove the files from folder BPO_QUEUE_DAILY_RR2_EMEA:\n",
    "[os.remove(os.path.join(BPO_QUEUE_DAILY_RR2_EMEA, filename)) for filename in os.listdir(BPO_QUEUE_DAILY_RR2_EMEA)\n",
    " if filename.startswith(\"LatSLA_\")]\n",
    "#Remove the files from folder BPO_PEOPLE_MI_EMEA:\n",
    "[os.remove(os.path.join(BPO_PEOPLE_MI_EMEA, filename)) for filename in os.listdir(BPO_PEOPLE_MI_EMEA)\n",
    " if filename.startswith(\"WorkHour Indicators\")]\n",
    "#Remove the files from folder Valitaion BPO_PEOPLE_Validacao:\n",
    "#[os.remove(os.path.join(BPO_PEOPLE_Validacao, filename)) for filename in os.listdir(BPO_PEOPLE_Validacao)\n",
    " #if filename.startswith(\"WorkHour Indicators\")]\n",
    "#Remove the files from folder Valitaion BPO_PEOPLE_Validacao_ADSO:\n",
    "#[os.remove(os.path.join(BPO_PEOPLE_Validacao_ADSO, filename)) for filename in os.listdir(BPO_PEOPLE_Validacao_ADSO)\n",
    " #if filename.startswith(\"WorkHour Indicators\")]\n",
    "#Remove the files from folder Valitaion BPO_PEOPLE_Validacao_Integrity:\n",
    "#[os.remove(os.path.join(BPO_PEOPLE_Validacao_Integrity, filename)) for filename in os.listdir(BPO_PEOPLE_Validacao_Integrity)\n",
    " #if filename.startswith(\"WorkHour Indicators\")]\n",
    "#Remove the files from folder Valitaion BPO_PEOPLE_Validacao_Integrity_MI:\n",
    "#[os.remove(os.path.join(BPO_PEOPLE_Validacao_Integrity_MI, filename)) for filename in os.listdir(BPO_PEOPLE_Validacao_Integrity_MI)\n",
    " #if filename.startswith(\"WorkHour Indicators\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95451d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:WorkHour Indicators_MI: UTC+1：20240906080902.csv\n",
      "1:WorkHour Indicators: UTC+1：20240906080908.csv\n",
      "2:WorkHour Indicators_MI: UTC+1：20240906081113.csv\n",
      "2:WorkHour Indicators_MI: UTC+1：20240906081131.csv\n",
      "2:WorkHour Indicators_MI: UTC+1：20240906081145.csv\n",
      "1:WorkHour Indicators: UTC+1：20240906081154.csv\n",
      "3:LatSLA_Hourly: UTC+1：20240906081240.csv\n",
      "4:LatSLA_Hourly: UTC+1：20240906081254.csv\n",
      "1:WorkHour Indicators: UTC+1：20240906081302.csv\n",
      "7:LatSLA_VMS: UTC+1：20240906081315.csv\n",
      "5:LatSLA: UTC+1：20240906081407.csv\n",
      "1:WorkHour Indicators: UTC+1：20240906081415.csv\n",
      "6:LatSLA: UTC+1：20240906081418.csv\n",
      "10:hour_global_moderation_stat: UTC+1：20240906081507.csv\n",
      "9:global_moderation_stat: UTC+1：20240906081509.csv\n",
      "8:Integrity: UTC+1：20240906081521.csv\n"
     ]
    }
   ],
   "source": [
    "#5º GROUP\n",
    "#THIS CODE WILL TRANSFORM THE FILE IF NEED, CHANGE THE NAME,MOVE TO THE RIGHT FOLDER:\n",
    "\n",
    "#Loop to read all the utc_files \n",
    "for i in utc_files:\n",
    " try:\n",
    "    #drop the first row on all columns\n",
    "    g = pd.read_csv(downloads_folder_path + '\\\\' + i, header=1,index_col = False)\n",
    "    #rename all columns as Date, this is because bpo peopledata has other name.\n",
    "    g = g.rename(columns = {g.columns[0]: 'Date'})\n",
    "    #Variable with the columns name\n",
    "    c = g.columns\n",
    "#-----------------------------------Logic to get the start_date e end_date for filename----------------------------------------\n",
    "# Get Start Date for Each File\n",
    "    start_date = pd.to_datetime(g['Date'].min())\n",
    "    \n",
    "# Store the Year + Month of Start Date for End Of Month Calculation\n",
    "# And store the end date for 4 days and 8 days type file\n",
    "    start_date_year = start_date.year\n",
    "    start_date_month = start_date.month\n",
    "    end_date1 = start_date + timedelta(days = 3)\n",
    "    end_date2 = start_date + timedelta(days = 7)\n",
    "    end_date3 = start_date + timedelta(days = 2)\n",
    "\n",
    "# Clean Start Date, End Date 4 Days, End Date 8 Days\n",
    "# Clean End Date 1 Month on file specific step\n",
    "    start_date = start_date.strftime('%d%m%Y')\n",
    "    end_date1 = end_date1.strftime('%d%m%Y')\n",
    "    end_date2 = end_date2.strftime('%d%m%Y')\n",
    "    end_date3 = end_date3.strftime('%d%m%Y')\n",
    "\n",
    "# Normalizes the Start and End Date formats\n",
    "# Normalization of End Date Month on file specific Step\n",
    "    if len(start_date) < 8:\n",
    "        start_date = '0'+ start_date\n",
    "    if len(end_date1) < 8:\n",
    "        end_date1 = '0'+ end_date1\n",
    "    if len(end_date2) < 8:\n",
    "        end_date2 = '0'+ end_date2\n",
    "    if len(end_date3) < 8:\n",
    "        end_date3 = '0'+ end_date3\n",
    "#-----------------------------------END of Logic to get the start_date e end_date for filename----------------------------------------        \n",
    "#BPO_People HOUR (Need to do merge with BPO people MI)\n",
    " \n",
    "    if 'Others Hours(h)' in list(c) and not g['Department Name'].str.contains('MI_TP_LIS').any():\n",
    "        f = \"WorkHour Indicators %s_%s.csv\" %(start_date, end_date3)\n",
    "        g = g.rename(columns = {g.columns[0]: 'Hour'})\n",
    "        g.to_csv(BPO_PEOPLE_EMEA + '\\\\'+ f, index = False)\n",
    "        print(\"1:WorkHour Indicators: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "        \n",
    "#BPO_People HOUR (MI ACCOUNT)\n",
    "\n",
    "    elif 'Others Hours(h)' in list(c) and g['Department Name'].str.contains('MI_TP_LIS').any():\n",
    "        f = \"WorkHour Indicators %s_%s.csv\" %(start_date, end_date3)\n",
    "        g = g.rename(columns = {g.columns[0]: 'Hour'}) \n",
    "        g.to_csv(BPO_PEOPLE_MI_EMEA+ '\\\\'+ f, index = False)\n",
    "        print(\"2:WorkHour Indicators_MI: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)        \n",
    "        \n",
    "           \n",
    " #BPO_QUEUE HOUR_RR1.0\n",
    "\n",
    "    elif len(list(c)) == 23 and 'No. of Input' in list(c):\n",
    "        f = 'LatSLA_Hourly_%s_%s.csv' %(start_date, end_date1)\n",
    "        g.drop(['Ontime Output Completion SLA(%)'],axis=1, inplace=True)\n",
    "        g.to_csv(BPO_QUEUE_HOUR_EMEA+ '\\\\' + f, index = False)\n",
    "        print(\"3:LatSLA_Hourly: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "        \n",
    " #BPO_QUEUE HOUR_RR2.0 \n",
    "\n",
    "    elif len(list(c)) == 19 and 'No. of Output_RR2.0 High Priority' in list(c):\n",
    "        f = 'LatSLA_Hourly_%s_%s.csv' %(start_date, end_date1)\n",
    "        g.drop(['No.of High-pri. Incoming','No. of Output_RR2.0 High Priority','Ontime Output Completion SLA(%)'],axis=1, inplace= True)\n",
    "        g.to_csv(BPO_QUEUE_HOUR_RR2_EMEA+ '\\\\' + f, index = False)\n",
    "        print(\"4:LatSLA_Hourly: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "       \n",
    "\n",
    " #BPO_QUEUE_DAILY_RR1.0\n",
    "    \n",
    "    elif len(list(c)) == 28 and 'No. of Input' in list(c):\n",
    "        f = 'LatSLA_%s_%s.csv' %(start_date, end_date2)\n",
    "        g.drop(['Ontime Output Completion SLA(%)'],axis=1, inplace=True)\n",
    "        g.to_csv(BPO_QUEUE_DAILY_EMEA + '\\\\'+ f, index = False)\n",
    "        print(\"5:LatSLA: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "        \n",
    "  #BPO_QUEUE_DAILY_RR2.0\n",
    "    \n",
    "    elif len(list(c)) == 24 and 'No. of Output_RR2.0 High Priority' in list(c):\n",
    "        f = 'LatSLA_%s_%s.csv' %(start_date, end_date2)\n",
    "        g.drop(['No.of High-pri. Incoming','No. of Output_RR2.0 High Priority','Ontime Output Completion SLA(%)'],axis=1,inplace=True)\n",
    "        g.to_csv(BPO_QUEUE_DAILY_RR2_EMEA + '\\\\'+ f, index = False)\n",
    "        print(\"6:LatSLA: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "        \n",
    " #BPO_QUEUE_VMS\n",
    "\n",
    "    elif 'No. of Achievable Output_VMS3' in list(c):\n",
    "        f = 'LatSLA_VMS_%s_%s.csv' %(start_date, end_date2)\n",
    "        g = g.rename(columns ={\"Ontime Output\":\"Ontime Output_VMS3.0\"})\n",
    "        g.to_csv(BPO_QUEUE_VMS_EMEA + '\\\\'+ f, index = False)\n",
    "        g.to_csv(BPO_QUEUE_VMS_DROP + '\\\\'+ f, index = False)\n",
    "        print(\"7:LatSLA_VMS: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "        \n",
    " #INTEGRITY\n",
    "\n",
    "    elif len(list(c)) == 12 and 'No. of Deferment (Cases)' in list(c)and not g['Department name'].str.contains('MI_TP_LIS').any():\n",
    "        final_day = calendar.monthrange(start_date_year, start_date_month)[1]\n",
    "        len_start_date_month = str(start_date_month)\n",
    "        if len(len_start_date_month) < 2:\n",
    "            len_start_date_month = '0'+ len_start_date_month\n",
    "        \n",
    "        if (start_date[:2] < \"16\"):\n",
    "            end_date3 = \"%s%s%s\" %(\"15\", len_start_date_month, start_date_year)\n",
    "        else:\n",
    "            end_date3 = \"%s%s%s\" %(final_day, len_start_date_month, start_date_year)\n",
    "        \n",
    "        f = 'Integrity_%s_%s.csv' %(start_date, end_date3)\n",
    "        g = g.rename(columns ={\"No tag Rate(%)\":\"No-tag Rate(%)\"})\n",
    "        g.to_csv(INTEGRITY_EMEA + '\\\\'+ f, index = False)\n",
    "        g.to_csv(INTEGRITY_DROP + '\\\\'+ f, index = False)\n",
    "        \n",
    "        print(\"8:Integrity: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "  \n",
    " #INTEGRITY MI\n",
    "\n",
    "    elif len(list(c)) == 12 and 'No. of Deferment (Cases)' in list(c) and g['Department name'].str.contains('MI_TP_LIS').any():\n",
    "        final_day = calendar.monthrange(start_date_year, start_date_month)[1]\n",
    "        len_start_date_month = str(start_date_month)\n",
    "        if len(len_start_date_month) < 2:\n",
    "            len_start_date_month = '0'+ len_start_date_month\n",
    "        \n",
    "        if (start_date[:2] < \"16\"):\n",
    "            end_date3 = \"%s%s%s\" %(\"15\", len_start_date_month, start_date_year)\n",
    "        else:\n",
    "            end_date3 = \"%s%s%s\" %(final_day, len_start_date_month, start_date_year)\n",
    "        \n",
    "        f = 'Integrity_MI_%s_%s.csv' %(start_date, end_date3)\n",
    "        g = g.rename(columns ={\"No tag Rate(%)\":\"No-tag Rate(%)\"})\n",
    "        g.to_csv(INTEGRITY_EMEA + '\\\\'+ f, index = False)\n",
    "        g.to_csv(INTEGRITY_DROP + '\\\\'+ f, index = False)\n",
    "        g.to_csv(INTEGRITY_Ondrive + '\\\\'+ f, index = False)\n",
    "        #g.to_csv(INTEGRITY_Validacao + '\\\\'+ f, index = False)\n",
    "        print(\"888:Integrity_MI: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)        \n",
    " #MODERATION_STATS\n",
    "\n",
    "    elif len(list(c)) == 26 and 'Total Productive Hours(h)' in list(c):\n",
    "        final_day = calendar.monthrange(start_date_year, start_date_month)[1]\n",
    "        len_start_date_month = str(start_date_month)\n",
    "        if len(len_start_date_month) < 2:\n",
    "            len_start_date_month = '0'+ len_start_date_month\n",
    "        \n",
    "        if (start_date[:2] < \"16\"):\n",
    "            end_date3 = \"%s%s%s\" %(\"15\", len_start_date_month, start_date_year)\n",
    "        else:\n",
    "            end_date3 = \"%s%s%s\" %(final_day, len_start_date_month, start_date_year)\n",
    "        \n",
    "        f = 'global_moderation_stat_%s_%s.csv' %(start_date, end_date3)\n",
    "        g.to_csv(MODERATION_STATS_EMEA + '\\\\'+ f, index = False)\n",
    "        print(\"9:global_moderation_stat: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    "\n",
    " #MODERATION_STATS_HOUR:\n",
    "\n",
    "    elif len(list(c)) == 21 and 'Total Productive Hours(h)' in list(c):\n",
    "        f = 'hour_global_moderation_stat_%s_%s.csv' %(start_date, end_date1)\n",
    "        g.to_csv(MODERATION_STATS_HOUR_EMEA + '\\\\'+ f, index = False)\n",
    "        g.to_csv(MODERATION_STATS_HOUR_DROP + '\\\\'+ f, index = False)\n",
    "        print(\"10:hour_global_moderation_stat: \" + i)\n",
    "        log_df = log_df.append({'Filename': i, 'Date': start_date, 'Status': 'Success', 'Message': 'File copied successfully'}, ignore_index=True)\n",
    " \n",
    "# Fail condition, to catch any file not complying with the rules above\n",
    "    else:\n",
    "        print(\"File Not Uploaded: \" + i) \n",
    " except Exception as e:\n",
    "    log_df = log_df.append({'Filename': i, 'Date': date_today, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfddfd6",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfdcba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Group\n",
    "#THIS CODE WILL CONCAT THE FILES FROM THE BELOW FOLDER TOGETHER TO MERGE THE RR1.0 WITH RR2.0\n",
    "\n",
    "pasta_A = BPO_QUEUE_DAILY_EMEA\n",
    "pasta_B = BPO_QUEUE_DAILY_RR2_EMEA\n",
    "#---------------------------------\n",
    "pasta_C = BPO_QUEUE_HOUR_EMEA\n",
    "pasta_D = BPO_QUEUE_HOUR_RR2_EMEA\n",
    "#---------------------------------\n",
    "pasta_E = BPO_PEOPLE_EMEA\n",
    "pasta_F = BPO_PEOPLE_MI_EMEA\n",
    "#----------------------------------\n",
    "\n",
    "# Lista todos os arquivos em cada pasta\n",
    "arquivos_A = os.listdir(pasta_A)\n",
    "arquivos_B = os.listdir(pasta_B)\n",
    "#________________________________\n",
    "arquivos_C = os.listdir(pasta_C)\n",
    "arquivos_D = os.listdir(pasta_D)\n",
    "#________________________________\n",
    "arquivos_E = os.listdir(pasta_E)\n",
    "arquivos_F = os.listdir(pasta_F)\n",
    "\n",
    "#-----------BPO QUEUE Daily( merge rr1.0 with rr2.0)----------------------\n",
    "# Itera sobre os arquivos com o mesmo nome em ambas as pastas\n",
    "for arquivo1 in set(arquivos_A) & set(arquivos_B):\n",
    "    caminho_A = os.path.join(pasta_A, arquivo1)\n",
    "    caminho_B = os.path.join(pasta_B, arquivo1)\n",
    "# Lê os dataframes de ambos os arquivos\n",
    "    df_A = pd.read_csv(caminho_A)\n",
    "    df_B = pd.read_csv(caminho_B)\n",
    "    df_concatenado = pd.concat([df_A, df_B], axis=0)\n",
    "    \n",
    "    caminho_saida = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE_DAILY\\{}\".format(arquivo1)\n",
    "    df_concatenado.to_csv(caminho_saida, index=False)\n",
    "    \n",
    "#-------------BPO QUEUE Hour( merge rr1.0 with rr2.0)---------------------------------------------------    \n",
    "for arquivo2 in set(arquivos_C) & set(arquivos_D):\n",
    "    caminho_C = os.path.join(pasta_C, arquivo2)\n",
    "    caminho_D = os.path.join(pasta_D, arquivo2)\n",
    "    # Lê os dataframes de ambos os arquivos\n",
    "    df_C = pd.read_csv(caminho_C)\n",
    "    df_D = pd.read_csv(caminho_D)\n",
    "    df_concatenado1 = pd.concat([df_C, df_D], axis=0)\n",
    "    \n",
    "    caminho_saida = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_QUEUE HOUR\\{}\".format(arquivo2)\n",
    "    df_concatenado1.to_csv(caminho_saida, index=False)\n",
    "    \n",
    "#----------------BPO PEOPLE ( merge Gcp data with MI data) --------------------------------------------------------------\n",
    "for arquivo3 in set(arquivos_E) & set(arquivos_F):\n",
    "    caminho_E = os.path.join(pasta_E, arquivo3)\n",
    "    caminho_F = os.path.join(pasta_F, arquivo3)\n",
    "    \n",
    "    df_E = pd.read_csv(caminho_E)\n",
    "    df_F = pd.read_csv(caminho_F)\n",
    "    df_concatenado = pd.concat([df_E, df_F], axis=0)\n",
    "    \n",
    "    caminho_saida3 = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\\{}\".format(arquivo3)\n",
    "    df_concatenado.to_csv(caminho_saida3, index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849876e7",
   "metadata": {},
   "source": [
    "##__ADSO BPO UR% Data__\n",
    "\n",
    "\n",
    "__Link:__ https://datapower-va.bytelemon.com/bi/visit/7284236472998969349\n",
    "\n",
    "__Dashboard:__ TP-LIS ADSEO Performance Data\n",
    "\n",
    "__Left Tab:__ UR\n",
    "\n",
    "__Table:__ Last table (UR Projet/Site Breakdown)\n",
    "\n",
    "__Filtros:__ Dynamic dimension (Day time) // Dynamic dimensio (hour time) // Time ( __Same range of the dates same as the bot__)\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565ac701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A guardar o merge do file de ADSO.\n",
      "A guardar o file de BPO_ADSO.\n",
      "Successfully read 'BPO_ADSO': C:\\\\Users\\\\sequeira.81\\\\Downloads\\\\BPO_ADSO.csv\n",
      "---------------------------------------------------------------------------------------\n",
      "Date range in data file: 25082024 to 05092024\n",
      "File WorkHour Indicators 03092024_05092024.csv matches the date range.\n",
      "Foi feita o drop de linhas com 0 na columna \"TalentX\". \n",
      "---------------------------------------------------------------------------------------\n",
      "Successfully processed and saved file: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\\WorkHour Indicators 03092024_05092024.csv\n",
      "---------------------------------------------------------------------------------------\n",
      "----------------------NEXT FILE OR END-----------------------------------------------------------------\n",
      "File WorkHour Indicators 25082024_27082024.csv matches the date range.\n",
      "Foi feita o drop de linhas com 0 na columna \"TalentX\". \n",
      "---------------------------------------------------------------------------------------\n",
      "Successfully processed and saved file: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\\WorkHour Indicators 25082024_27082024.csv\n",
      "---------------------------------------------------------------------------------------\n",
      "----------------------NEXT FILE OR END-----------------------------------------------------------------\n",
      "File WorkHour Indicators 28082024_30082024.csv matches the date range.\n",
      "Foi feita o drop de linhas com 0 na columna \"TalentX\". \n",
      "---------------------------------------------------------------------------------------\n",
      "Successfully processed and saved file: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\\WorkHour Indicators 28082024_30082024.csv\n",
      "---------------------------------------------------------------------------------------\n",
      "----------------------NEXT FILE OR END-----------------------------------------------------------------\n",
      "File WorkHour Indicators 31082024_02092024.csv matches the date range.\n",
      "Foi feita o drop de linhas com 0 na columna \"TalentX\". \n",
      "---------------------------------------------------------------------------------------\n",
      "Successfully processed and saved file: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\\WorkHour Indicators 31082024_02092024.csv\n",
      "---------------------------------------------------------------------------------------\n",
      "----------------------NEXT FILE OR END-----------------------------------------------------------------\n",
      "File WorkHour Indicators ADSO 03092024_05092024.csv matches the date range.\n",
      "Foi feita o drop de linhas com 0 na columna \"TalentX\". \n",
      "---------------------------------------------------------------------------------------\n",
      "Successfully processed and saved file: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\\WorkHour Indicators ADSO 03092024_05092024.csv\n",
      "---------------------------------------------------------------------------------------\n",
      "----------------------NEXT FILE OR END-----------------------------------------------------------------\n",
      "File WorkHour Indicators ADSO 25082024_27082024.csv matches the date range.\n",
      "Foi feita o drop de linhas com 0 na columna \"TalentX\". \n",
      "---------------------------------------------------------------------------------------\n",
      "Successfully processed and saved file: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\\WorkHour Indicators ADSO 25082024_27082024.csv\n",
      "---------------------------------------------------------------------------------------\n",
      "----------------------NEXT FILE OR END-----------------------------------------------------------------\n",
      "File WorkHour Indicators ADSO 28082024_30082024.csv matches the date range.\n",
      "Foi feita o drop de linhas com 0 na columna \"TalentX\". \n",
      "---------------------------------------------------------------------------------------\n",
      "Successfully processed and saved file: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\\WorkHour Indicators ADSO 28082024_30082024.csv\n",
      "---------------------------------------------------------------------------------------\n",
      "----------------------NEXT FILE OR END-----------------------------------------------------------------\n",
      "File WorkHour Indicators ADSO 31082024_02092024.csv matches the date range.\n",
      "Foi feita o drop de linhas com 0 na columna \"TalentX\". \n",
      "---------------------------------------------------------------------------------------\n",
      "Successfully processed and saved file: \\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\\WorkHour Indicators ADSO 31082024_02092024.csv\n",
      "---------------------------------------------------------------------------------------\n",
      "----------------------NEXT FILE OR END-----------------------------------------------------------------\n",
      "A guardar os files originais na pasta EMEA.\n",
      "A guardar os files originais na pasta EMEA.\n",
      "A guardar os files originais na pasta EMEA.\n",
      "A guardar os files originais na pasta EMEA.\n"
     ]
    }
   ],
   "source": [
    "#This code part will read the UR file from the new source of ADSO:\n",
    "#Look fo the file on the downlaod folder\n",
    "UR_ADSO = [filename for filename in os.listdir(downloads_folder_path) if\n",
    "                  filename.startswith(\"_UR Project_Site Breakdown -\")]\n",
    " \n",
    "# Lista para armazenar DataFrames\n",
    "dataframes = []\n",
    " \n",
    "for file in UR_ADSO:\n",
    "    filepath = os.path.join(downloads_folder_path, file)\n",
    "    df = pd.read_csv(filepath)  # Ajuste para read_excel(filepath) se forem arquivos .xlsx\n",
    "    dataframes.append(df)\n",
    " \n",
    "# Concatenar todos os DataFrames, eliminando colunas duplicadas\n",
    "combined_df = pd.concat(dataframes, axis=0, ignore_index=True)\n",
    "# Remover colunas duplicadas\n",
    "combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
    "# Salvar o DataFrame combinado em um novo arquivo\n",
    "output_filepath = os.path.join(downloads_folder_path, \"combined_file.csv\")  # Ajuste para .xlsx se necessário\n",
    "print('A guardar o merge do file de ADSO.')\n",
    "combined_df.to_csv(fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\BPO_ADSO_merged.csv\", index=False)\n",
    " \n",
    " \n",
    "UR_ADSO_merge = [filename for filename in os.listdir(downloads_folder_path) if\n",
    "                  filename.startswith(\"BPO_ADSO_merged\")]\n",
    "#Loop to read the file and transform the data:\n",
    "for jj in UR_ADSO_merge:\n",
    "    file_path = os.path.join(downloads_folder_path, jj)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.rename(columns={'Time.1': 'dt'}, inplace=True)\n",
    "    df['Formatted_Hour'] = pd.to_datetime(df['dt'], format='%H').dt.strftime('%H:%M:%S')\n",
    "    df['Hour']= df['Time']  + ' ' + df['Formatted_Hour']\n",
    "    df = df[['Hour','Moderator']]\n",
    "    df['Talent_X']=0\n",
    "    print( 'A guardar o file de BPO_ADSO.')\n",
    "    df.to_csv(fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\BPO_ADSO.csv\", index=False)\n",
    "#---------------------------------------------------------------------------------------------\n",
    " \n",
    "# Path to the directory containing the multiple files\n",
    "folder_path = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\"\n",
    "folder_path1 = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\BPO_People\" \n",
    "folder_path2 = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\Validation\\BPO_People_ADSO\"\n",
    "\n",
    "# Path to the single file with data\n",
    "data_file = fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\BPO_ADSO.csv\"\n",
    " \n",
    "# Read the single file with data into a DataFrame\n",
    "try:\n",
    "    data_df = pd.read_csv(data_file)\n",
    "    print(f\"Successfully read 'BPO_ADSO': {data_file}\")\n",
    "except FileNotFoundError:\n",
    "     \n",
    "    print(f\"Data file not found: {data_file}\")\n",
    "    raise\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"Data file is empty: {data_file}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the data file: {e}\")\n",
    "    raise\n",
    " \n",
    "# Initialize min_date and max_date\n",
    "min_date = data_df['Hour'].min()\n",
    "max_date = data_df['Hour'].max()\n",
    " \n",
    "# Generate all dates between min_date and max_date\n",
    "date_range = pd.date_range(start=min_date, end=max_date)\n",
    " \n",
    "# Convert all dates in the range to the desired string format\n",
    "date_strings = [date.strftime('%d%m%Y') for date in date_range]\n",
    " \n",
    "print(\"---------------------------------------------------------------------------------------\")\n",
    "print(f\"Date range in data file: {date_strings[0]} to {date_strings[-1]}\")\n",
    " \n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):  # Check if the file is a CSV\n",
    "        # Extract start date and end date from the filename\n",
    "        try:\n",
    "            start_date_str, end_date_str = filename.split(' ')[-1].split('.')[0].split('_')\n",
    "        except ValueError:\n",
    "            continue\n",
    " \n",
    "        # Check if any date in the range matches the start or end date in the filename\n",
    "         # Check if the start date is within the date range\n",
    "        if start_date_str in date_strings:\n",
    "            # Now check if the end date is within the range or just beyond the max date\n",
    "            if end_date_str in date_strings or pd.to_datetime(end_date_str, format='%d%m%Y') > pd.to_datetime(max_date):\n",
    "                print(f\"File {filename} matches the date range.\")\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            file_path2 = os.path.join(folder_path1, filename)\n",
    "            try:\n",
    "                # Read the current file into a DataFrame\n",
    "                file_df = pd.read_csv(file_path)\n",
    "               \n",
    "            except FileNotFoundError:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "                continue\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"File is empty: {file_path}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while reading the file: {e}\")\n",
    "                continue\n",
    "           \n",
    "   # Drop the Talent_X column if it exists\n",
    "            if 'Talent_X' in file_df.columns:\n",
    "                file_df.drop(columns=['Talent_X'], inplace=True)\n",
    " \n",
    "            # Process the file\n",
    "            merged_df = file_df.merge(data_df, on=['Hour', 'Moderator'], how='left')\n",
    "            merged_df['Talent_X'] = merged_df['Talent_X'].fillna(1).astype(int)\n",
    "           # Identify the index of rows where column B is 0\n",
    "            index_to_drop = merged_df[merged_df['Talent_X'] == 0].index\n",
    "            merged_df = merged_df.drop(index_to_drop)\n",
    "            print('Foi feita o drop de linhas com 0 na columna \"TalentX\". ')      \n",
    "                       \n",
    "            # Salvar o DataFrame resultante de volta no arquivo original\n",
    "            merged_df.to_csv(file_path, index=False)\n",
    "            #merged_df.to_csv(file_path1, index=False)\n",
    "            print(\"---------------------------------------------------------------------------------------\")\n",
    "            print(f\"Successfully processed and saved file: {file_path}\")\n",
    "            print(\"---------------------------------------------------------------------------------------\")\n",
    "            print(\"----------------------NEXT FILE OR END-----------------------------------------------------------------\")\n",
    " \n",
    "#---------------------------------------------------------------------------------------------\n",
    "UR_ADSO_Main = [filename for filename in os.listdir(downloads_folder_path) if\n",
    "                  filename.startswith(\"_UR Project_Site Breakdown -\")]\n",
    "for jj in UR_ADSO_Main:\n",
    "    file_path = os.path.join(downloads_folder_path, jj)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.rename(columns={'Time.1': 'dt'}, inplace=True)\n",
    "    df['Formatted_Hour'] = pd.to_datetime(df['dt'], format='%H').dt.strftime('%H:%M:%S')\n",
    "    df['Hour']= df['Time']  + ' ' + df['Formatted_Hour']\n",
    "    df['Offline Hours(h)'] =0\n",
    "    df['Others Hours(h)'] =0\n",
    "    df = df.rename(columns ={\"Utilization Rate of Effective Prod Hours\":\"Utilization Rate of Effective Prod Hours(%)\",\"Occupancy rate\":\"Occupancy rate(%)\",\"Shrinkage rate\":\"Shrinkage rate(%)\",\n",
    "    \"Avg TCS EPH\":\"Effective Prod Hours(h)\",\"Avg Mod. Task Hour\":\"Mod. Task Hours(h)\",\"Avg Non-Mod Task Hour\":\"Non-mod. Task Hours(h)\",\"Avg Meeting Hour\":\"Meeting Hours(h)\",\"Avg Training Hour\":\"Training Hours(h)\",\n",
    "    \"Avg Wellness Hours\":\"Wellness Hour(h)\",\"Avg Rest Hour\":\"Rest Hours(h)\",\"Avg Idle Hour\":\"Idle Hours(h)\"})\n",
    "    agents = df[['Hour','Moderator','Department Name','Effective Prod Hours(h)','Utilization Rate of Effective Prod Hours(%)','Shrinkage rate(%)','Occupancy rate(%)','Mod. Task Hours(h)','Non-mod. Task Hours(h)',\n",
    "    'Wellness Hour(h)','Training Hours(h)','Meeting Hours(h)','Rest Hours(h)','Idle Hours(h)','Offline Hours(h)','Others Hours(h)']]\n",
    "    agents['Talent_X']=0\n",
    "    # Get Start Date for Each File\n",
    "    start_date = pd.to_datetime(agents['Hour'].min())\n",
    "    # Store the Year + Month of Start Date for End Of Month Calculation\n",
    "    # And store the end date for 4 days and 8 days type file\n",
    "    start_date_year = start_date.year\n",
    "    start_date_month = start_date.month\n",
    "    end_date1 = start_date + timedelta(days = 2)\n",
    "    start_date = start_date.strftime('%d%m%Y')\n",
    "    end_date1 = end_date1.strftime('%d%m%Y')\n",
    "    # Normalizes the Start and End Date formats\n",
    "    # Normalization of End Date Month on file specific Step\n",
    "    if len(start_date) < 8:\n",
    "        start_date = '0'+ start_date\n",
    "    if len(end_date1) < 8:\n",
    "        end_date1 = '0'+ end_date1\n",
    "#-----------------------------------END of Dates logic----------------------------------------\n",
    "    new_name = \"WorkHour Indicators ADSO %s_%s.csv\" %(start_date, end_date1)\n",
    "    print(\"A guardar os files originais na pasta EMEA.\")\n",
    "    agents.to_csv(folder_path + '\\\\'+ new_name, index = False)\n",
    "    agents.to_csv(folder_path2 + '\\\\'+ new_name, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3837641",
   "metadata": {},
   "source": [
    "# The below code will move the last modifiy files from BPO People to a databricks, and sharepoint, folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31b00933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-Copied'WorkHour Indicators 03092024_05092024.csv' to '\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\CONT\\bpopeopleworkhour'.\n",
      "2-Copied 'WorkHour Indicators 03092024_05092024.csv' to 'C:\\Users\\sequeira.81\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\BPO_PEOPLE'.\n",
      "1-Copied'WorkHour Indicators 25082024_27082024.csv' to '\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\CONT\\bpopeopleworkhour'.\n",
      "2-Copied 'WorkHour Indicators 25082024_27082024.csv' to 'C:\\Users\\sequeira.81\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\BPO_PEOPLE'.\n",
      "1-Copied'WorkHour Indicators 28082024_30082024.csv' to '\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\CONT\\bpopeopleworkhour'.\n",
      "2-Copied 'WorkHour Indicators 28082024_30082024.csv' to 'C:\\Users\\sequeira.81\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\BPO_PEOPLE'.\n",
      "1-Copied'WorkHour Indicators 31082024_02092024.csv' to '\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\CONT\\bpopeopleworkhour'.\n",
      "2-Copied 'WorkHour Indicators 31082024_02092024.csv' to 'C:\\Users\\sequeira.81\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\BPO_PEOPLE'.\n",
      "1-Copied'WorkHour Indicators ADSO 03092024_05092024.csv' to '\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\CONT\\bpopeopleworkhour'.\n",
      "2-Copied 'WorkHour Indicators ADSO 03092024_05092024.csv' to 'C:\\Users\\sequeira.81\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\BPO_PEOPLE'.\n",
      "1-Copied'WorkHour Indicators ADSO 25082024_27082024.csv' to '\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\CONT\\bpopeopleworkhour'.\n",
      "2-Copied 'WorkHour Indicators ADSO 25082024_27082024.csv' to 'C:\\Users\\sequeira.81\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\BPO_PEOPLE'.\n",
      "1-Copied'WorkHour Indicators ADSO 28082024_30082024.csv' to '\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\CONT\\bpopeopleworkhour'.\n",
      "2-Copied 'WorkHour Indicators ADSO 28082024_30082024.csv' to 'C:\\Users\\sequeira.81\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\BPO_PEOPLE'.\n",
      "1-Copied'WorkHour Indicators ADSO 31082024_02092024.csv' to '\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\CONT\\bpopeopleworkhour'.\n",
      "2-Copied 'WorkHour Indicators ADSO 31082024_02092024.csv' to 'C:\\Users\\sequeira.81\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\BPO_PEOPLE'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def move_files_modified_today(BPO_folder, databricks_folder, BPO_Onedrive):\n",
    "    # Get today's date\n",
    "    today = datetime.today().date()\n",
    "\n",
    "    # Get list of all files in the source directory\n",
    "    files = [f for f in os.listdir(BPO_folder) if os.path.isfile(os.path.join(BPO_folder, f))]\n",
    "\n",
    "    # Filter files modified today\n",
    "    files_modified_today = []\n",
    "    for file in files:\n",
    "        file_path = os.path.join(BPO_folder, file)\n",
    "        file_mod_time = datetime.fromtimestamp(os.path.getmtime(file_path)).date()\n",
    "\n",
    "        if file_mod_time == today:\n",
    "            files_modified_today.append(file)\n",
    "\n",
    "    if not files_modified_today:\n",
    "        print(\"No files modified today found in the source folder.\")\n",
    "        return\n",
    "    \n",
    "    # Move the files modified today\n",
    "    for file in files_modified_today:\n",
    "        src_file = os.path.join(BPO_folder, file)\n",
    "        dst_file_databricks = os.path.join(databricks_folder, file)\n",
    "        dst_file_onedrive = os.path.join(BPO_Onedrive, file)\n",
    "        \n",
    "        # Move to databricks_folder\n",
    "        shutil.copy(src_file, dst_file_databricks)\n",
    "        print(f\"1-Copied'{file}' to '{databricks_folder}'.\")\n",
    "\n",
    "        # Copy to BPO_Onedrive\n",
    "        shutil.copy(src_file, dst_file_onedrive)\n",
    "        print(f\"2-Copied '{file}' to '{BPO_Onedrive}'.\")\n",
    "    \n",
    "# Example usage\n",
    "BPO_folder = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\37. New DP Data\\BPO_PEOPLE\"\n",
    "databricks_folder = r\"\\\\10.235.133.90\\ptdoufs01Departments$\\ITDEV\\DataManagement\\TTOK\\CONT\\bpopeopleworkhour\"\n",
    "BPO_Onedrive = fr\"C:\\Users\\{os.getenv('USERNAME')}\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\BPO_PEOPLE\"\n",
    "\n",
    "move_files_modified_today(BPO_folder, databricks_folder, BPO_Onedrive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0ac02e",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a74b05",
   "metadata": {},
   "source": [
    "__The below code will verify the folders of the UTC files and valiudate if exist some duplication of data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d42544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In folder BPO_PEOPLE(3 em 3)download last 2 weeks data), no duplicate dates were found among the files.\n",
      "In folder BPO_PEOPLE_ADSO(3 em 3)download last 2 weeks data), no duplicate dates were found among the files.\n",
      "In folder BPO_QUEUE_HOUR_RR1.0 (4 em 4), no duplicate dates were found among the files.\n",
      "In folder BPO_QUEUE_HOUR_RR2.0(4 em 4), no duplicate dates were found among the files.\n",
      "In folder BPO_QUEUE_VMS(8 em 8), no duplicate dates were found among the files.\n",
      "In folder MODERATION_STATS_HOUR_EMEA(4 em 4), no duplicate dates were found among the files.\n",
      "In folder BPO_QUEUE_DAILY_RR1.0(8 em 8), no duplicate dates were found among the files.\n",
      "In folder BPO_QUEUE_DAILY_RR2.0(8 em 8), no duplicate dates were found among the files.\n",
      "In folder MODERATION_STATS_EMEA(15 em 15), no duplicate dates were found among the files.\n",
      "In folder INTEGRITY_EMEA(15 em 15), no duplicate dates were found among the files.\n",
      "In folder INTEGRITY_MI(15 em 15), no duplicate dates were found among the files.\n"
     ]
    }
   ],
   "source": [
    "#7 Group\n",
    "#THIS CODE WILL VERIFY EACH FOLDER AND EACH FILE AND GRAB AL THE DATES AND DO A UNIQUE DATE,\n",
    "#IN CASE 2 FILE WILL HAVE THE SAME DATE THAT FILE AND DATE WILL BE SHOW ON THE CODE RESULT.\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def check_duplicate_dates_in_folder(folder_path):\n",
    "    # List all files in the folder and sort them by modification time\n",
    "    files = sorted(os.listdir(folder_path), key=lambda x: os.path.getmtime(os.path.join(folder_path, x)), reverse=True)[:5]\n",
    "    # Create a dictionary to store unique dates from each file\n",
    "    unique_dates_dict = {}\n",
    "    # Iterate over the files\n",
    "    for file in files:\n",
    "        # Check if the file is a regular file (not a directory)\n",
    "        if os.path.isfile(os.path.join(folder_path, file)):\n",
    "            # Read the file content into a pandas DataFrame\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)  # Assuming files are in CSV format, adjust the format as needed\n",
    "                # Check if the 'Date' or 'Hour' column exists in the DataFrame\n",
    "                date_column = None\n",
    "                if 'Date' in df.columns:\n",
    "                    date_column = df['Date']\n",
    "                elif 'Hour' in df.columns:\n",
    "                    date_column = df['Hour']\n",
    "                else:\n",
    "                    raise ValueError(f\"No 'Date' or 'Hour' found in the file: {file_path}\")\n",
    "                # Convert the date column to datetime\n",
    "                df['Date'] = pd.to_datetime(date_column, errors='coerce')\n",
    "                # Remove rows with invalid dates\n",
    "                df = df.dropna(subset=['Date'])\n",
    "                # Add unique dates to the dictionary\n",
    "                unique_dates_dict[file] = set(df['Date'].unique())\n",
    "            except pd.errors.EmptyDataError:\n",
    "                pass  # Ignore empty files\n",
    "            \n",
    "    # Compare unique dates across all files to find duplicate dates\n",
    "    duplicate_dates = set()\n",
    "    for file1, dates1 in unique_dates_dict.items():\n",
    "        for file2, dates2 in unique_dates_dict.items():\n",
    "            if file1 != file2 and dates1 & dates2:\n",
    "                duplicate_dates.update(dates1 & dates2)\n",
    "\n",
    "    return duplicate_dates\n",
    "\n",
    "for short_name, folder_path in folders2.items():\n",
    "    duplicate_dates = check_duplicate_dates_in_folder(folder_path)\n",
    "    if duplicate_dates:\n",
    "        print(f'In folder {short_name}, the following dates are duplicated in at least two files: {duplicate_dates}.')\n",
    "    else:\n",
    "        print(f'In folder {short_name}, no duplicate dates were found among the files.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7865fe",
   "metadata": {},
   "source": [
    "----------------------------------------END of this code Group--------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69caa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=datetime.today().strftime('%d-%m-%Y %H-%M-%S')\n",
    "Log_file=log_df.to_excel(fr\"C:\\\\Users\\\\{username}\\\\{folder}\\\\Logs_etl_part1_{date}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ba368",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a31685",
   "metadata": {},
   "source": [
    "# Shift data MMP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2dec8",
   "metadata": {},
   "source": [
    "Link: https://byteworks-va.bytelemon.com/v2/workhour/correct\n",
    "\n",
    "__Filters:__ By shift period\n",
    "\n",
    "__Date range of the shift start date:__ 1 file per month, until day 15 of current month take the previous and the current month. \n",
    "\n",
    "__Data aggregattion method:__ By department\n",
    "\n",
    "__Department:__ All withou VLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9b10260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data-1725607099_1725607167284.csv' moved and renamed to 'Shift_data_MMP-01092024_30092024.csv' in '\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\39. MMP Shift Hours'\n",
      "File 'data-1725607099_1725607167284.csv' moved and renamed to 'Shift_data_MMP-01092024_30092024.csv' in 'C:\\Users\\\\sequeira.81\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\39. MMP Shift Hours'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_emea = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\39. MMP Shift Hours\"\n",
    "shift_Onedrive= fr\"C:\\Users\\\\{username}\\\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\39. MMP Shift Hours\"\n",
    "# Look for the file starting with \"Project Omega RCA\" in the source directory\n",
    "source_dir =fr\"C:\\\\Users\\\\{username}\\\\{folder}\"\n",
    "file_prefix = \"data-\"\n",
    "\n",
    "file_to_move = next((file for file in os.listdir(source_dir) if file.startswith(file_prefix)), None)\n",
    "file_path = os.path.join(source_dir, file_to_move)\n",
    "\n",
    "u = pd.read_csv(file_path,skiprows = 1)\n",
    "    #Change the date to string:\n",
    "  \n",
    "start_date = pd.to_datetime(u['日期/Date'].min())\n",
    "start_date_year = start_date.year\n",
    "start_date_month = start_date.month\n",
    "\n",
    "final_day = calendar.monthrange(start_date_year, start_date_month)[1]\n",
    "len_start_date_month = str(start_date_month)\n",
    "if len(len_start_date_month) < 2:\n",
    "    len_start_date_month = '0' + len_start_date_month    \n",
    "\n",
    "start_date_str = start_date.strftime('%d%m%Y')\n",
    "end_date_str = \"%s%s%s\" % (final_day, len_start_date_month, start_date_year)\n",
    "\n",
    "if len(start_date_str) < 8:\n",
    "    start_date_str = '0' + start_date_str \n",
    "\n",
    "\n",
    "if file_to_move:\n",
    "    try:\n",
    "      \n",
    "        # Define the new file name\n",
    "        new_file_name = 'Shift_data_MMP-%s_%s.csv' % (start_date_str, end_date_str)  # Replace with the desired new file name and extension\n",
    "        \n",
    "        # Move and rename the file\n",
    "        shutil.copy(os.path.join(source_dir, file_to_move), os.path.join(shift_emea, new_file_name))\n",
    "        print(f\"File '{file_to_move}' moved and renamed to '{new_file_name}' in '{shift_emea}'\")\n",
    "\n",
    "        # Move  and rename the file in sharepoint\n",
    "        shutil.copy(os.path.join(source_dir, file_to_move), os.path.join(shift_Onedrive, new_file_name))\n",
    "        print(f\"File '{file_to_move}' moved and renamed to '{new_file_name}' in '{shift_Onedrive}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while moving the file: {e}\")\n",
    "else:\n",
    "    print(f\"No file starting with '{file_prefix}' found in '{source_dir}'\")\n",
    "\n",
    "#This line will remove the files from your download files:\n",
    "[os.remove(os.path.join(downloads_folder_path, filename)) for filename in os.listdir(downloads_folder_path)\n",
    " if filename.startswith(\"data-\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f5c31",
   "metadata": {},
   "source": [
    "# OMEGA Client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55278b59",
   "metadata": {},
   "source": [
    "Link: https://datapower-va.bytelemon.com/bi/visit/7325384900452941829?immersive=1\n",
    "\n",
    "Downloads :On Monday download previous week,after that download current week. ( EVERYDAY )\n",
    "\n",
    "__Tab:__ Project Omega\n",
    "\n",
    "__Batch Date:__ Select the week from Monday to Sunday\n",
    "\n",
    "__Moderator Base:__ TP-LIS\n",
    "\n",
    "Click on 3 dots from the table, select download\n",
    "\n",
    "UTF-8 encoded CSV, another 0 on \"Number of rows\" and Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "909b6289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '_Detailed Mod Level Occurence Table   - 2024-09-06 08-19-36.csv' moved and renamed to '_Detailed Mod Level Occurence Table   -26082024_01092024.csv' in '\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\SV OMEGA Project\\Client_Data'\n"
     ]
    }
   ],
   "source": [
    "shift_emea = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\SV OMEGA Project\\Client_Data\"\n",
    "# Look for the file starting with \"Project Omega RCA\" in the source directory\n",
    "source_dir =fr\"C:\\\\Users\\\\{username}\\\\{folder}\"\n",
    "file_prefix = \"_Detailed Mod Level Occurence Table   -\"\n",
    "\n",
    "file_to_move = next((file for file in os.listdir(source_dir) if file.startswith(file_prefix)), None)\n",
    "file_path = os.path.join(source_dir, file_to_move)\n",
    "\n",
    "u = pd.read_csv(file_path)\n",
    "# Define the source and destination directories\n",
    "\n",
    "start_date = pd.to_datetime(u['Resolve Time'].min())\n",
    "end_date = start_date + timedelta(days = 6)\n",
    "\n",
    "start_date = start_date.strftime('%d%m%Y')\n",
    "end_date = end_date.strftime('%d%m%Y')   \n",
    "\n",
    "\n",
    "\n",
    "if file_to_move:\n",
    "    try:\n",
    "      \n",
    "        # Define the new file name\n",
    "        new_file_name = '_Detailed Mod Level Occurence Table   -%s_%s.csv' % (start_date, end_date)  # Replace with the desired new file name and extension\n",
    "        \n",
    "        # Move and rename the file\n",
    "        shutil.copy(os.path.join(source_dir, file_to_move), os.path.join(shift_emea, new_file_name))\n",
    "        print(f\"File '{file_to_move}' moved and renamed to '{new_file_name}' in '{shift_emea}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while moving the file: {e}\")\n",
    "else:\n",
    "    print(f\"No file starting with '{file_prefix}' found in '{source_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9b050b",
   "metadata": {},
   "source": [
    "# OMEGA RCAs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf1980",
   "metadata": {},
   "source": [
    "Link: https://teleperformance.larksuite.com/sheets/Hkhis4qirhViP6t3dZguQPZzsyc?sheet=tYlF1U\n",
    "\n",
    "Downloads : Monday, previous week,after current week. ( EVERYDAY )\n",
    "\n",
    "__STEPS:__ 3 Dots, Download As, CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7fa9c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'Project Omega RCA - 2024 - W35.csv' moved  in '\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\SV OMEGA Project\\Support_File'\n"
     ]
    }
   ],
   "source": [
    "shift_emea = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\9. Quality Data\\SV OMEGA Project\\Support_File\"\n",
    "# Look for the file starting with \"Project Omega RCA\" in the source directory\n",
    "source_dir =fr\"C:\\\\Users\\\\{username}\\\\{folder}\"\n",
    "file_prefix = \"Project Omega RCA - 2024 - \"\n",
    "\n",
    "file_to_move = next((file for file in os.listdir(source_dir) if file.startswith(file_prefix)), None)\n",
    "file_path = os.path.join(source_dir, file_to_move)\n",
    "\n",
    "u = pd.read_csv(file_path)\n",
    "\n",
    "if file_to_move:\n",
    "    try:\n",
    "      \n",
    "     \n",
    "        \n",
    "        # Move and rename the file\n",
    "        shutil.copy(os.path.join(source_dir, file_to_move), os.path.join(shift_emea))\n",
    "        print(f\"File '{file_to_move}' moved  in '{shift_emea}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while moving the file: {e}\")\n",
    "else:\n",
    "    print(f\"No file starting with '{file_prefix}' found in '{source_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084302f8",
   "metadata": {},
   "source": [
    "# OMEGA Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7d52208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1: CompletedProcess(args=['python', '\\\\\\\\emea.tpg.ads\\\\portugal\\\\Departments\\\\ITDEV\\\\PowerBI\\\\accounting\\\\business analysts\\\\01 - ba\\\\02 - projects\\\\tiktok\\\\10. Python scripts\\\\NEW ETL\\\\Aux Scripts\\\\Omega Project\\\\omega_script.py'], returncode=0)\n"
     ]
    }
   ],
   "source": [
    "Script_path = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\10. Python scripts\\NEW ETL\\Aux Scripts\\Omega Project\\omega_script.py\"\n",
    "\n",
    "Result1 = subprocess.run(['python', Script_path], text=True)\n",
    "\n",
    "print(\"Result 1:\", Result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d04f5",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c3d355",
   "metadata": {},
   "source": [
    "# MI RockAppeal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86597e7b",
   "metadata": {},
   "source": [
    "__Link:__ https://rock-va.bytelemon.com/appeal_center/list/claimantStage/unhandled?case_merge=&verifier_type=0&case_state=0&case_content=&search_value_enum=0&origin_audit_time=&new_audit_time=&order_field=remain_time&order=0&origin_sample_index=%5b%5d&new_sample_index=%5b%5d&origin_verifier=&appeal_mode=&task_id=&object_id=&start=0&end=100&role=4\n",
    "\n",
    "__TAB:__ Claimant\n",
    "\n",
    "__TAB:__ __Pending Appeal__ No need to select dates as the file is small and can contain all the data. \n",
    "\n",
    "__TAB:__ __Appeal__  On the filter Moderation time we need to extract (__curent month and previous month__), then click on the left box with a up arrow // Online Export //\n",
    "Whether separate sheets based on the queues ( __select: NO__) // Apply // name as __2 and 3__ a number for each month downloaded.\n",
    "\n",
    "__TAB:__ __Not Appeal__  On the filter Moderation time we need to extract (__curent month and previous month__), then click on the left box with a up arrow // Online Export //\n",
    "Whether separate sheets based on the queues ( __select: NO__) // Apply // name as __4 and 5__ a number for each month downloaded.\n",
    "\n",
    "__TAB:__ __Expired__  On the filter Moderation time we need to extract (__curent month and previous month__), then click on the left box with a up arrow // Online Export //\n",
    "Whether separate sheets based on the queues ( __select: NO__) // Apply // name as __6 and 7__ a number for each month downloaded.\n",
    "\n",
    "After all that we need to go to the left box with a up arrow // My Exports// For each file will pop-up a new lark window and then you need to download the 7 files in .CSV\n",
    "\n",
    "All the files downloaded, then we can run the bellow code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25ab1384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as Rock_Appeal_Labeling_PendingAppeals.csv\n",
      "File saved as Rock_Appeal_Labeling_Appealed_20240901_20240930.csv\n",
      "File saved as Rock_Appeal_Labeling_Appealed_20240801_20240831.csv\n",
      "File saved as Rock_Appeal_Labeling_NotAppeal_20240901_20240930.csv\n",
      "File saved as Rock_Appeal_Labeling_NotAppeal_20240801_20240831.csv\n",
      "File saved as Rock_Appeal_Labeling_Expired_20240901_20240930.csv\n",
      "File saved as Rock_Appeal_Labeling_Expired_20240801_20240831.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Destination Folder:\n",
    "MI_Pending_Appealed= r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling_Rock_Appeal\\Pending Appeals\"\n",
    "MI_Not_Appeal= r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling_Rock_Appeal\\Not Appeal\"\n",
    "MI_Expired= r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling_Rock_Appeal\\Expired\"\n",
    "MI_Appealed= r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling_Rock_Appeal\\Appealed\"\n",
    "MI_Pending_Appealed_OneDrive= fr\"C:\\Users\\{os.getenv('USERNAME')}\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Labeling_Rock_Appeal\\Pending Appeals\"\n",
    "MI_Not_Appeal_OneDrive= fr\"C:\\Users\\{os.getenv('USERNAME')}\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Labeling_Rock_Appeal\\Not Appeal\"\n",
    "MI_Expired_OneDrive= fr\"C:\\Users\\{os.getenv('USERNAME')}\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Labeling_Rock_Appeal\\Expired\"\n",
    "MI_Appealed_OneDrive= fr\"C:\\Users\\{os.getenv('USERNAME')}\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Labeling_Rock_Appeal\\Appealed\"\n",
    "\n",
    "\n",
    "# Caminho para a pasta de downloads\n",
    "folder_path = fr\"C:\\\\Users\\\\{username}\\\\{folder}\"\n",
    "\n",
    "# Função para renomear os arquivos\n",
    "\n",
    "    # Lista todos os arquivos na pasta de downloads\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "for filename in files:\n",
    "        # Verifica se o arquivo é um Excel\n",
    "        if filename.endswith('- sheet.csv'):\n",
    "            # Lê a data mínima da coluna '日期/Date'\n",
    "            df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "            start_date = pd.to_datetime(df['Moderation Time'].min())\n",
    "            start_date_year = start_date.year\n",
    "            start_date_month = start_date.month\n",
    "\n",
    "            # Calcula o último dia do mês\n",
    "            final_day = calendar.monthrange(start_date_year, start_date_month)[1]\n",
    "            len_start_date_month = str(start_date_month).zfill(2)\n",
    "\n",
    "            # Formata as datas de início e fim\n",
    "            start_date_str = start_date.strftime('%Y%m%d')\n",
    "            end_date_str = f\"{start_date_year}{len_start_date_month}{final_day}\"\n",
    "\n",
    "            # Adiciona um zero se necessário\n",
    "            if len(start_date_str) < 8:\n",
    "                start_date_str = '0' + start_date_str\n",
    "\n",
    "            # Renomeia o arquivo com base na lógica fornecida\n",
    "           # Rename the file based on the provided logic\n",
    "        if filename == '1.xlsx - sheet.csv':\n",
    "            f = 'Rock_Appeal_Labeling_PendingAppeals.csv'\n",
    "            df.to_csv(os.path.join(MI_Pending_Appealed, f), index=False)\n",
    "            df.to_csv(os.path.join(MI_Pending_Appealed_OneDrive, f), index=False)\n",
    "            print(f\"File saved as {f}\")\n",
    "\n",
    "        elif filename in ['2.xlsx - sheet.csv', '3.xlsx - sheet.csv']:\n",
    "            f = f\"Rock_Appeal_Labeling_Appealed_{start_date_str}_{end_date_str}.csv\"\n",
    "            df.to_csv(os.path.join(MI_Appealed, f), index=False)\n",
    "            df.to_csv(os.path.join(MI_Appealed_OneDrive, f), index=False)\n",
    "            print(f\"File saved as {f}\")\n",
    "\n",
    "        elif filename in ['4.xlsx - sheet.csv', '5.xlsx - sheet.csv']:\n",
    "            f = f\"Rock_Appeal_Labeling_NotAppeal_{start_date_str}_{end_date_str}.csv\"\n",
    "            df.to_csv(os.path.join(MI_Not_Appeal, f), index=False)\n",
    "            df.to_csv(os.path.join(MI_Not_Appeal_OneDrive, f), index=False)\n",
    "            print(f\"File saved as {f}\")\n",
    "\n",
    "        elif filename in ['6.xlsx - sheet.csv', '7.xlsx - sheet.csv']:\n",
    "            f = f\"Rock_Appeal_Labeling_Expired_{start_date_str}_{end_date_str}.csv\"\n",
    "            df.to_csv(os.path.join(MI_Expired, f), index=False)\n",
    "            df.to_csv(os.path.join(MI_Expired_OneDrive, f), index=False)\n",
    "            print(f\"File saved as {f}\")\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045fcaab",
   "metadata": {},
   "source": [
    "# <del>MI Labeling LP Platform</del>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca19883",
   "metadata": {},
   "source": [
    "__Link:__ https://label.bytedance.com/operation/data-dashboards?device_id=81889&org=DCC\n",
    "        \n",
    "__TAB:__ Queue List\n",
    "\n",
    "__TAB:__ Data Dashboard\n",
    "\n",
    "__Table:__ Labeller Project Mapping\n",
    "\n",
    "__Filter:__ Resolve date ( The week you want from Monday to Sunday)\n",
    "\n",
    "If Monday download previous week, starting Tuesday forward current week (EVERYDAY)\n",
    "\n",
    "Click on 3 dots from the table, select download \n",
    "\n",
    "UTF-8 encoded CSV, anotehr 0 on \"Number of rows\" and Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5bcd672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: _Labeller Project Mapping - 2024-08-23 14-19-17.csv to _Labeller Project Mapping_20240819_20240825.csv\n",
      "Replaced: _Labeller Project Mapping_20240819_20240825.csv\n",
      "Moved: _Labeller Project Mapping_20240819_20240825.csv to C:\\Users\\Sequeira.81\\Desktop\\New folder\n"
     ]
    }
   ],
   "source": [
    "# Define the source folder (downloads) and target folder\n",
    "source_folder = downloads_folder_path\n",
    "target_folder = fr\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling_LP_Platform\\Weekly_data\" \n",
    "\n",
    "# Determine the current day and calculate the date range\n",
    "today = datetime.today()\n",
    "weekday = today.weekday()\n",
    "\n",
    "if weekday == 0:  # Monday\n",
    "    # Use previous week's range\n",
    "    start_date = today - timedelta(days=7)\n",
    "else:\n",
    "    # Use current week's range\n",
    "    start_date = today - timedelta(days=weekday)\n",
    "\n",
    "end_date = start_date + timedelta(days=6)\n",
    "\n",
    "# Format the dates as YYYYMMDD\n",
    "start_date_str = start_date.strftime('%Y%m%d')\n",
    "end_date_str = end_date.strftime('%Y%m%d')\n",
    "\n",
    "# Construct the new file name\n",
    "new_file_base = f\"_Labeller Project Mapping_{start_date_str}_{end_date_str}.csv\"\n",
    "\n",
    "# Find all files matching the pattern in the source folder\n",
    "file_pattern = \"_Labeller Project Mapping*.csv\"  # Adjust extension if different\n",
    "matching_files = glob.glob(os.path.join(source_folder, file_pattern))\n",
    "\n",
    "if matching_files:\n",
    "    for latest_file in matching_files:\n",
    "        try:\n",
    "            # Rename the file with the new date range\n",
    "            new_file_name = new_file_base\n",
    "            new_file_path = os.path.join(source_folder, new_file_name)\n",
    "\n",
    "            # Rename the file\n",
    "            os.rename(latest_file, new_file_path)\n",
    "            print(f\"Renamed: {os.path.basename(latest_file)} to {new_file_name}\")\n",
    "\n",
    "            # Check if a file with the same date range exists in the target folder\n",
    "            existing_file = None\n",
    "            for file in os.listdir(target_folder):\n",
    "                if new_file_base in file:\n",
    "                    existing_file = file\n",
    "                    break\n",
    "\n",
    "            if existing_file:\n",
    "                existing_file_path = os.path.join(target_folder, existing_file)\n",
    "                os.remove(existing_file_path)\n",
    "                log_df = log_df.append({'Filename': existing_file, 'Date': date_today, 'Status': 'Replaced', 'Message': 'Existing file replaced'}, ignore_index=True)\n",
    "                print(f\"Replaced: {existing_file}\")\n",
    "            \n",
    "            # Move the renamed file to the target folder\n",
    "            shutil.copy(new_file_path, os.path.join(target_folder, new_file_name))\n",
    "            log_df = log_df.append({'Filename': new_file_name, 'Date': date_today, 'Status': 'Moved', 'Message': 'File moved successfully'}, ignore_index=True)\n",
    "            print(f\"Moved: {new_file_name} to {target_folder}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log_df = log_df.append({'Filename': os.path.basename(latest_file), 'Date': date_today, 'Status': 'Error', 'Message': str(e)}, ignore_index=True)\n",
    "            print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"No files found matching the pattern.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfb3803",
   "metadata": {},
   "source": [
    "# MI Labeling LP Platform Daily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf40f3",
   "metadata": {},
   "source": [
    "__Link:__ https://teleperformance.sharepoint.com/sites/TikTok212/Shared%20Documents/Forms/AllItems.aspx?FolderCTID=0x012000B98B5AF80497F14392E44394E54AA19B&isAscending=true&id=%2Fsites%2FTikTok212%2FShared%20Documents%2FData%20Analytics%2F5%2E%20Extractions%2FLabeling%20LP%20Platform&sortField=LinkFilename&viewid=4d19ea0e%2D0774%2D4a0a%2D8fc4%2D204e3ee1184f\n",
    "__INFO:__ THIS LINK IS IN CASE YOU NEED TO DO IT MANUALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5c00672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "                                 Filename                 Date   Status  \\\n",
      "0  _Labeller Project Mapping_20240904.csv  2024-09-06 12:37:41  Success   \n",
      "1  _Labeller Project Mapping_20240905.csv  2024-09-06 12:37:41  Success   \n",
      "2  _Labeller Project Mapping_20240903.csv  2024-09-06 12:37:41  Success   \n",
      "3  _Labeller Project Mapping_20240902.csv  2024-09-06 12:37:42  Success   \n",
      "4  _Labeller Project Mapping_20240831.csv  2024-09-06 12:37:42  Success   \n",
      "\n",
      "                   Message  \n",
      "0  File moved successfully  \n",
      "1  File moved successfully  \n",
      "2  File moved successfully  \n",
      "3  File moved successfully  \n",
      "4  File moved successfully  \n"
     ]
    }
   ],
   "source": [
    "# Clean Python cache\n",
    "gen_py_path = fr\"C:\\Users\\\\{username}\\\\AppData\\Local\\Temp\\gen_py\"\n",
    "shutil.rmtree(gen_py_path, ignore_errors=True)\n",
    "\n",
    "# Define the source and destination folders\n",
    "source_folder = r\"C:\\Users\\Sequeira.81\\Teleperformance\\P.TTOK.CONT - 5. Extractions\\Labeling LP Platform\"\n",
    "destination_folder = r\"\\\\emea.tpg.ads\\portugal\\Departments\\ITDEV\\PowerBI\\accounting\\business analysts\\01 - ba\\02 - projects\\tiktok\\4. Raw Data and Aux Files\\10. MI\\Labeling_LP_Platform\\Daily_data\"\n",
    "#destination_folder_2 = r\"C:\\Users\\Sequeira.81\\Teleperformance\\pt-dataanalytics-pbi-datadrops - P.TTOK.CONT\\4. Raw Data and Aux Files\\Labeling_LP_Platform\\Daily_data\"\n",
    "\n",
    "# Log DataFrame\n",
    "log_df = pd.DataFrame(columns=['Filename', 'Date', 'Status', 'Message'])\n",
    "\n",
    "# Get the list of files in the source folder\n",
    "files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
    "\n",
    "# Check for non-CSV files\n",
    "non_csv_files = [f for f in files if not f.endswith('.csv')]\n",
    "if non_csv_files:\n",
    "    # Create and send an email notification for non-CSV files\n",
    "    ol = win32com.client.Dispatch(\"outlook.application\")\n",
    "    olmailitem = 0x0\n",
    "    newmail = ol.CreateItem(olmailitem)\n",
    "    newmail.Subject = 'Non-CSV Files Found in Labeling LP Platform Folder'\n",
    "    newmail.To = 'iuliia.kritskaia@pt.teleperformance.com'\n",
    "    newmail.Body = f\"Hello, the following non-CSV files were found in the Labeling LP Platform folder:\\n\\n\" + \"\\n\".join(non_csv_files)\n",
    "    newmail.Send()\n",
    "\n",
    "# Filter files that start with \"_Labeller Project Mapping\" and end with \".csv\"\n",
    "filtered_files = [f for f in files if f.startswith(\"_Labeller Project Mapping\") and f.endswith(\".csv\")]\n",
    "\n",
    "# Sort files by modification time (most recent first)\n",
    "filtered_files.sort(key=lambda f: os.path.getmtime(os.path.join(source_folder, f)), reverse=True)\n",
    "\n",
    "# Get the last 5 files\n",
    "latest_files = filtered_files[:5]\n",
    "\n",
    "# Move the last 5 files to the destination folder\n",
    "for file_name in latest_files:\n",
    "    try:\n",
    "        # Build full file path\n",
    "        source_path = os.path.join(source_folder, file_name)\n",
    "        destination_path = os.path.join(destination_folder, file_name)\n",
    "        #destination_path_2 = os.path.join(destination_folder_2, file_name)\n",
    "        \n",
    "        # Move the file\n",
    "        shutil.copy(source_path, destination_path)\n",
    "        #shutil.copy(source_path, destination_path_2)\n",
    "        \n",
    "        # Log success\n",
    "        log_df = log_df.append({\n",
    "            'Filename': file_name,\n",
    "            'Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'Status': 'Success',\n",
    "            'Message': 'File moved successfully'\n",
    "        }, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        # Log failure\n",
    "        log_df = log_df.append({\n",
    "            'Filename': file_name,\n",
    "            'Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'Status': 'Failure',\n",
    "            'Message': str(e)\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Print log DataFrame\n",
    "print(\"---------------------------------------------\")\n",
    "print(log_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81afdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
